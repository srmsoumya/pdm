{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Feature Engineering for DPF Maintenance Prediction\n",
    "\n",
    "This notebook teaches you how to create **explainable features** from raw sensor data that fleet managers can understand and trust.\n",
    "\n",
    "## 🎯 Why Interpretable Features Matter\n",
    "\n",
    "**Traditional Approach**: Complex features that work but can't be explained\n",
    "- Fourier transforms, wavelet coefficients, deep learning embeddings\n",
    "- Hard to explain why a vehicle needs maintenance\n",
    "- Fleet managers can't take actionable steps\n",
    "\n",
    "**Our Approach**: Simple, domain-driven features that tell a story\n",
    "- \"Engine load has been trending upward 5% per week\"\n",
    "- \"DEF level drops below 75% more than usual\"\n",
    "- \"Recent driving patterns 20% more aggressive than historical\"\n",
    "\n",
    "## 📚 What You'll Learn\n",
    "\n",
    "1. **Trend Analysis**: How to detect meaningful changes in sensor patterns\n",
    "2. **Threshold Engineering**: Creating actionable alert boundaries\n",
    "3. **Pattern Recognition**: Identifying operational behavior changes\n",
    "4. **Domain Knowledge Integration**: Using fleet management expertise in features\n",
    "5. **Feature Validation**: Testing that features actually predict maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ready to build interpretable features!\n",
      "📚 This notebook will teach you to create features that:\n",
      "   • Fleet managers can understand\n",
      "   • Maintenance teams can act upon\n",
      "   • Actually predict DPF issues\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"🚀 Ready to build interpretable features!\")\n",
    "print(\"📚 This notebook will teach you to create features that:\")\n",
    "print(\"   • Fleet managers can understand\")\n",
    "print(\"   • Maintenance teams can act upon\")\n",
    "print(\"   • Actually predict DPF issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Step 1: Understanding Raw Sensor Data\n",
    "\n",
    "Before we engineer features, let's understand what our raw sensor data looks like and what it tells us about DPF health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading DPF datasets...\n",
      "✅ Loaded 82 maintenance events\n",
      "✅ Loaded 4,466,272 sensor readings\n"
     ]
    }
   ],
   "source": [
    "# Load our datasets\n",
    "print(\"📁 Loading DPF datasets...\")\n",
    "\n",
    "try:\n",
    "    maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n",
    "    sensor_df = pd.read_csv('../data/dpf_vehicle_stats.csv')\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n",
    "    sensor_df['time'] = pd.to_datetime(sensor_df['time'])\n",
    "    \n",
    "    print(f\"✅ Loaded {len(maintenance_df)} maintenance events\")\n",
    "    print(f\"✅ Loaded {len(sensor_df):,} sensor readings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Please run the data munging script first: uv run python 01_data_munging.py\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing DPF-Relevant Sensors\n",
      "==================================================\n",
      "\n",
      "📊 Engine Load Percentage (engineLoadPercent)\n",
      "   📈 Data Availability: 36.3% (1,619,223 readings)\n",
      "   🎯 DPF Relevance: High load generates more soot, stressing DPF\n",
      "   📏 Normal Range: (20, 60) %\n",
      "   📊 Current Range: 1.0 - 125.0 %\n",
      "\n",
      "📊 Engine RPM (engineRpm)\n",
      "   📈 Data Availability: 42.3% (1,889,794 readings)\n",
      "   🎯 DPF Relevance: RPM affects DPF regeneration efficiency\n",
      "   📏 Normal Range: (800, 1800) RPM\n",
      "   📊 Current Range: 26.0 - 2618.0 RPM\n",
      "\n",
      "📊 DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent)\n",
      "   📈 Data Availability: 45.9% (2,048,969 readings)\n",
      "   🎯 DPF Relevance: Low DEF prevents proper DPF operation\n",
      "   📏 Normal Range: (60000, 95000) milli-%\n",
      "   📊 Current Range: 400.0 - 100000.0 milli-%\n",
      "\n",
      "📊 Engine Coolant Temperature (engineCoolantTemperatureMilliC)\n",
      "   📈 Data Availability: 33.5% (1,497,692 readings)\n",
      "   🎯 DPF Relevance: Temperature affects DPF regeneration cycles\n",
      "   📏 Normal Range: (75000, 90000) milli-°C\n",
      "   📊 Current Range: 1000.0 - 123000.0 milli-°C\n",
      "\n",
      "📊 Vehicle Speed (ecuSpeedMph)\n",
      "   📈 Data Availability: 69.0% (3,079,837 readings)\n",
      "   🎯 DPF Relevance: Highway speeds help DPF regeneration\n",
      "   📏 Normal Range: (0, 65) mph\n",
      "   📊 Current Range: 0.0 - 88.9 mph\n"
     ]
    }
   ],
   "source": [
    "# Identify DPF-relevant sensors and their characteristics\n",
    "print(\"🔍 Analyzing DPF-Relevant Sensors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define sensors most relevant to DPF health with their meanings\n",
    "dpf_sensors = {\n",
    "    'engineLoadPercent': {\n",
    "        'description': 'Engine Load Percentage',\n",
    "        'dpf_relevance': 'High load generates more soot, stressing DPF',\n",
    "        'normal_range': (20, 60),\n",
    "        'alert_thresholds': {'high': 80, 'low': 10},\n",
    "        'unit': '%'\n",
    "    },\n",
    "    'engineRpm': {\n",
    "        'description': 'Engine RPM',\n",
    "        'dpf_relevance': 'RPM affects DPF regeneration efficiency',\n",
    "        'normal_range': (800, 1800),\n",
    "        'alert_thresholds': {'high': 2000, 'low': 600},\n",
    "        'unit': 'RPM'\n",
    "    },\n",
    "    'defLevelMilliPercent': {\n",
    "        'description': 'DEF (Diesel Exhaust Fluid) Level',\n",
    "        'dpf_relevance': 'Low DEF prevents proper DPF operation',\n",
    "        'normal_range': (60000, 95000),  # 60-95%\n",
    "        'alert_thresholds': {'high': 95000, 'low': 50000},\n",
    "        'unit': 'milli-%'\n",
    "    },\n",
    "    'engineCoolantTemperatureMilliC': {\n",
    "        'description': 'Engine Coolant Temperature',\n",
    "        'dpf_relevance': 'Temperature affects DPF regeneration cycles',\n",
    "        'normal_range': (75000, 90000),  # 75-90°C\n",
    "        'alert_thresholds': {'high': 95000, 'low': 65000},\n",
    "        'unit': 'milli-°C'\n",
    "    },\n",
    "    'ecuSpeedMph': {\n",
    "        'description': 'Vehicle Speed',\n",
    "        'dpf_relevance': 'Highway speeds help DPF regeneration',\n",
    "        'normal_range': (0, 65),\n",
    "        'alert_thresholds': {'high': 80, 'low': 0},\n",
    "        'unit': 'mph'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze data availability for each sensor\n",
    "for sensor, info in dpf_sensors.items():\n",
    "    if sensor in sensor_df.columns:\n",
    "        non_null = sensor_df[sensor].notna().sum()\n",
    "        total = len(sensor_df)\n",
    "        availability = (non_null / total) * 100\n",
    "        \n",
    "        print(f\"\\n📊 {info['description']} ({sensor})\")\n",
    "        print(f\"   📈 Data Availability: {availability:.1f}% ({non_null:,} readings)\")\n",
    "        print(f\"   🎯 DPF Relevance: {info['dpf_relevance']}\")\n",
    "        print(f\"   📏 Normal Range: {info['normal_range']} {info['unit']}\")\n",
    "        \n",
    "        if non_null > 0:\n",
    "            values = sensor_df[sensor].dropna()\n",
    "            print(f\"   📊 Current Range: {values.min():.1f} - {values.max():.1f} {info['unit']}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {info['description']} ({sensor}) - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Step 2: Trend Analysis Features\n",
    "\n",
    "**Trend features** help us understand if a sensor is getting worse over time. These are highly interpretable:\n",
    "- **Positive slope**: \"Engine load is increasing 2% per week\"\n",
    "- **Negative slope**: \"DEF level is dropping 5% per week\"\n",
    "- **Flat trend**: \"Temperature has been stable\"\n",
    "\n",
    "### Why This Works for Fleet Managers:\n",
    "- Easy to visualize and understand\n",
    "- Actionable: know which direction things are heading\n",
    "- Predictive: trends often continue into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Demonstrating Trend Analysis\n",
      "========================================\n",
      "Sample Vehicle: 1XK1D40X1NJ495537\n",
      "Data Points: 269964\n",
      "Date Range: 2024-06-08 00:03:00+00:00 to 2025-05-21 20:40:00+00:00\n",
      "\n",
      "📈 Engine Load Percentage:\n",
      "   Trend: increasing by 0.013 %/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   ✅ Normal: Stable readings\n",
      "\n",
      "📈 Engine RPM:\n",
      "   Trend: increasing by 0.111 RPM/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate increasing trend\n",
      "\n",
      "📈 DEF (Diesel Exhaust Fluid) Level:\n",
      "   Trend: decreasing by 61.047 milli-%/day\n",
      "   Strength: 0.072 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate decreasing trend\n",
      "\n",
      "📈 Engine Coolant Temperature:\n",
      "   Trend: decreasing by 9.565 milli-°C/day\n",
      "   Strength: 0.005 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate decreasing trend\n",
      "\n",
      "📈 Vehicle Speed:\n",
      "   Trend: decreasing by 0.003 mph/day\n",
      "   Strength: 0.000 (0=weak, 1=strong linear trend)\n",
      "   ✅ Normal: Stable readings\n",
      "\n",
      "📊 Total trend features calculated: 30\n"
     ]
    }
   ],
   "source": [
    "def calculate_trend_features(sensor_data, sensor_name, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate interpretable trend features for a sensor.\n",
    "    Returns features that fleet managers can understand and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 3:  # Need minimum points for trend\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 3:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    times = pd.to_numeric(data[time_col]) / 1e9  # Convert to seconds\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. LINEAR TREND (Most Important)\n",
    "    # Calculate slope in original units per day\n",
    "    slope_per_second, intercept = np.polyfit(times, values, 1)\n",
    "    slope_per_day = slope_per_second * 86400  # Convert to per day\n",
    "    \n",
    "    features[f'{sensor_name}_trend_slope_per_day'] = slope_per_day\n",
    "    \n",
    "    # 2. TREND STRENGTH (How reliable is the trend?)\n",
    "    # R-squared shows how well the linear trend fits\n",
    "    correlation = np.corrcoef(times, values)[0, 1]\n",
    "    r_squared = correlation ** 2 if not np.isnan(correlation) else 0\n",
    "    features[f'{sensor_name}_trend_strength'] = r_squared\n",
    "    \n",
    "    # 3. TREND SIGNIFICANCE (Is this trend meaningful?)\n",
    "    # Calculate statistical significance of the trend\n",
    "    if len(values) >= 3:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(times, values)\n",
    "        features[f'{sensor_name}_trend_p_value'] = p_value\n",
    "        features[f'{sensor_name}_trend_significant'] = 1 if p_value < 0.05 else 0\n",
    "    \n",
    "    # 4. RECENT VS HISTORICAL TREND\n",
    "    # Compare recent behavior to historical (last 25% vs first 75%)\n",
    "    split_point = int(len(values) * 0.75)\n",
    "    if split_point > 2 and split_point < len(values) - 2:\n",
    "        historical_mean = np.mean(values[:split_point])\n",
    "        recent_mean = np.mean(values[split_point:])\n",
    "        \n",
    "        if historical_mean != 0:\n",
    "            percent_change = ((recent_mean - historical_mean) / abs(historical_mean)) * 100\n",
    "            features[f'{sensor_name}_recent_vs_historical_pct'] = percent_change\n",
    "    \n",
    "    # 5. VOLATILITY TREND (Is stability changing?)\n",
    "    # Calculate if the sensor is becoming more or less stable\n",
    "    if len(values) >= 6:\n",
    "        mid_point = len(values) // 2\n",
    "        early_std = np.std(values[:mid_point])\n",
    "        late_std = np.std(values[mid_point:])\n",
    "        \n",
    "        if early_std > 0:\n",
    "            volatility_change = (late_std - early_std) / early_std\n",
    "            features[f'{sensor_name}_volatility_change'] = volatility_change\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Demonstrate trend analysis with a sample vehicle\n",
    "print(\"🔍 Demonstrating Trend Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get a vehicle with good data coverage\n",
    "vehicle_counts = sensor_df['vin'].value_counts()\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Sample Vehicle: {sample_vin}\")\n",
    "    print(f\"Data Points: {len(sample_data)}\")\n",
    "    print(f\"Date Range: {sample_data['time'].min()} to {sample_data['time'].max()}\")\n",
    "    \n",
    "    # Calculate trends for each DPF sensor\n",
    "    all_trends = {}\n",
    "    for sensor in dpf_sensors.keys():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                trends = calculate_trend_features(sensor_subset, sensor)\n",
    "                all_trends.update(trends)\n",
    "                \n",
    "                # Show human-readable interpretation\n",
    "                slope_key = f'{sensor}_trend_slope_per_day'\n",
    "                if slope_key in trends:\n",
    "                    slope = trends[slope_key]\n",
    "                    direction = \"increasing\" if slope > 0 else \"decreasing\"\n",
    "                    strength = trends.get(f'{sensor}_trend_strength', 0)\n",
    "                    \n",
    "                    print(f\"\\n📈 {dpf_sensors[sensor]['description']}:\")\n",
    "                    print(f\"   Trend: {direction} by {abs(slope):.3f} {dpf_sensors[sensor]['unit']}/day\")\n",
    "                    print(f\"   Strength: {strength:.3f} (0=weak, 1=strong linear trend)\")\n",
    "                    \n",
    "                    # Interpret for fleet manager\n",
    "                    if abs(slope) > 0.1 and strength > 0.3:\n",
    "                        print(f\"   🚨 Alert: Strong {direction} trend detected!\")\n",
    "                    elif abs(slope) > 0.05:\n",
    "                        print(f\"   ⚠️ Caution: Moderate {direction} trend\")\n",
    "                    else:\n",
    "                        print(f\"   ✅ Normal: Stable readings\")\n",
    "    \n",
    "    print(f\"\\n📊 Total trend features calculated: {len(all_trends)}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for trend analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Step 3: Threshold-Based Features\n",
    "\n",
    "**Threshold features** tell us how often a sensor operates outside normal ranges. These are immediately actionable:\n",
    "- \"Engine runs above 80% load 25% of the time\" → Reduce load\n",
    "- \"DEF level below 60% for 3 days\" → Refill DEF\n",
    "- \"Temperature exceeds 95°C twice this week\" → Check cooling system\n",
    "\n",
    "### Fleet Manager Benefits:\n",
    "- Clear operational boundaries\n",
    "- Direct maintenance actions\n",
    "- Easy to set up monitoring alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Demonstrating Threshold Analysis\n",
      "========================================\n",
      "Analyzing thresholds for Vehicle: 1XK1D40X1NJ495537\n",
      "\n",
      "🎯 Engine Load Percentage (engineLoadPercent):\n",
      "   ⬆️ Time above 80 %: 24.8%\n",
      "      🚨 ALERT: Excessive high readings!\n",
      "   ⬇️ Time below 10 %: 6.4%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 16 consecutive readings\n",
      "\n",
      "🎯 Engine RPM (engineRpm):\n",
      "   ⬆️ Time above 2000 RPM: 0.0%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 600 RPM: 4.8%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 1 consecutive readings\n",
      "\n",
      "🎯 DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent):\n",
      "   ⬆️ Time above 95000 milli-%: 5.4%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 50000 milli-%: 28.5%\n",
      "      🚨 ALERT: Excessive low readings!\n",
      "   📊 Longest high streak: 101 consecutive readings\n",
      "\n",
      "🎯 Engine Coolant Temperature (engineCoolantTemperatureMilliC):\n",
      "   ⬆️ Time above 95000 milli-°C: 17.6%\n",
      "      ⚠️ WARNING: Frequent high readings\n",
      "   ⬇️ Time below 65000 milli-°C: 4.2%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 35 consecutive readings\n",
      "\n",
      "🎯 Vehicle Speed (ecuSpeedMph):\n",
      "   ⬆️ Time above 80 mph: 9.7%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 0 mph: 0.0%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 18 consecutive readings\n",
      "\n",
      "📊 Total threshold features calculated: 44\n"
     ]
    }
   ],
   "source": [
    "def calculate_threshold_features(sensor_data, sensor_name, thresholds, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate threshold-based features that are easy to interpret and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    features = {}\n",
    "    \n",
    "    # 1. PERCENTAGE OF TIME OUTSIDE THRESHOLDS\n",
    "    \n",
    "    # Time above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        high_thresh = thresholds['high']\n",
    "        pct_above_high = (values > high_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_above_high'] = pct_above_high\n",
    "    \n",
    "    # Time below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        low_thresh = thresholds['low']\n",
    "        pct_below_low = (values < low_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_below_low'] = pct_below_low\n",
    "    \n",
    "    # Total time outside normal range\n",
    "    if 'high' in thresholds and 'low' in thresholds:\n",
    "        outside_normal = ((values > thresholds['high']) | (values < thresholds['low'])).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_outside_normal'] = outside_normal\n",
    "    \n",
    "    # 2. CONSECUTIVE VIOLATIONS (Streak Analysis)\n",
    "    \n",
    "    # Longest streak above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        above_high = values > thresholds['high']\n",
    "        max_streak_high = calculate_max_consecutive(above_high)\n",
    "        features[f'{sensor_name}_max_streak_above_high'] = max_streak_high\n",
    "    \n",
    "    # Longest streak below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        below_low = values < thresholds['low']\n",
    "        max_streak_low = calculate_max_consecutive(below_low)\n",
    "        features[f'{sensor_name}_max_streak_below_low'] = max_streak_low\n",
    "    \n",
    "    # 3. FREQUENCY OF VIOLATIONS\n",
    "    \n",
    "    # How many separate incidents of threshold violations?\n",
    "    if 'high' in thresholds:\n",
    "        high_violations = count_violation_episodes(values > thresholds['high'])\n",
    "        features[f'{sensor_name}_high_violation_episodes'] = high_violations\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations = count_violation_episodes(values < thresholds['low'])\n",
    "        features[f'{sensor_name}_low_violation_episodes'] = low_violations\n",
    "    \n",
    "    # 4. SEVERITY OF VIOLATIONS\n",
    "    \n",
    "    # Average severity when violations occur\n",
    "    if 'high' in thresholds:\n",
    "        high_violations_mask = values > thresholds['high']\n",
    "        if high_violations_mask.any():\n",
    "            avg_excess = (values[high_violations_mask] - thresholds['high']).mean()\n",
    "            features[f'{sensor_name}_avg_excess_above_high'] = avg_excess\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations_mask = values < thresholds['low']\n",
    "        if low_violations_mask.any():\n",
    "            avg_deficit = (thresholds['low'] - values[low_violations_mask]).mean()\n",
    "            features[f'{sensor_name}_avg_deficit_below_low'] = avg_deficit\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_max_consecutive(boolean_array):\n",
    "    \"\"\"Calculate maximum consecutive True values in a boolean array.\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    \n",
    "    return max_streak\n",
    "\n",
    "def count_violation_episodes(boolean_array):\n",
    "    \"\"\"Count number of separate violation episodes (groups of consecutive True values).\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    episodes = 0\n",
    "    in_violation = False\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value and not in_violation:\n",
    "            episodes += 1\n",
    "            in_violation = True\n",
    "        elif not value:\n",
    "            in_violation = False\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Demonstrate threshold analysis\n",
    "print(\"🎯 Demonstrating Threshold Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing thresholds for Vehicle: {sample_vin}\")\n",
    "    \n",
    "    # Calculate threshold features for each sensor\n",
    "    all_threshold_features = {}\n",
    "    \n",
    "    for sensor, info in dpf_sensors.items():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                thresholds = info['alert_thresholds']\n",
    "                threshold_features = calculate_threshold_features(sensor_subset, sensor, thresholds)\n",
    "                all_threshold_features.update(threshold_features)\n",
    "                \n",
    "                print(f\"\\n🎯 {info['description']} ({sensor}):\")\n",
    "                \n",
    "                # Show key threshold metrics\n",
    "                pct_high_key = f'{sensor}_pct_time_above_high'\n",
    "                pct_low_key = f'{sensor}_pct_time_below_low'\n",
    "                \n",
    "                if pct_high_key in threshold_features:\n",
    "                    pct_high = threshold_features[pct_high_key]\n",
    "                    print(f\"   ⬆️ Time above {thresholds['high']} {info['unit']}: {pct_high:.1f}%\")\n",
    "                    \n",
    "                    if pct_high > 20:\n",
    "                        print(f\"      🚨 ALERT: Excessive high readings!\")\n",
    "                    elif pct_high > 10:\n",
    "                        print(f\"      ⚠️ WARNING: Frequent high readings\")\n",
    "                    else:\n",
    "                        print(f\"      ✅ Normal high threshold behavior\")\n",
    "                \n",
    "                if pct_low_key in threshold_features:\n",
    "                    pct_low = threshold_features[pct_low_key]\n",
    "                    print(f\"   ⬇️ Time below {thresholds['low']} {info['unit']}: {pct_low:.1f}%\")\n",
    "                    \n",
    "                    if pct_low > 20:\n",
    "                        print(f\"      🚨 ALERT: Excessive low readings!\")\n",
    "                    elif pct_low > 10:\n",
    "                        print(f\"      ⚠️ WARNING: Frequent low readings\")\n",
    "                    else:\n",
    "                        print(f\"      ✅ Normal low threshold behavior\")\n",
    "                \n",
    "                # Show streak information\n",
    "                streak_high_key = f'{sensor}_max_streak_above_high'\n",
    "                if streak_high_key in threshold_features:\n",
    "                    streak = threshold_features[streak_high_key]\n",
    "                    print(f\"   📊 Longest high streak: {streak} consecutive readings\")\n",
    "    \n",
    "    print(f\"\\n📊 Total threshold features calculated: {len(all_threshold_features)}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for threshold analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Step 4: Operational Pattern Features\n",
    "\n",
    "**Pattern features** capture changes in how the vehicle is being operated. These help identify external factors affecting DPF health:\n",
    "\n",
    "- **Duty Cycle Changes**: \"Vehicle now works 30% harder than usual\"\n",
    "- **Drive Profile Shifts**: \"More city driving, less highway (bad for DPF)\"\n",
    "- **Load Distribution**: \"Consistently heavy loads vs mixed operation\"\n",
    "\n",
    "### Why This Matters:\n",
    "- Operational changes often precede maintenance needs\n",
    "- Helps identify root causes (driver behavior, route changes)\n",
    "- Enables proactive operational adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Demonstrating Operational Pattern Analysis\n",
      "==================================================\n",
      "Analyzing operational patterns for Vehicle: 1XK1D40X1NJ495537\n",
      "Data points: 269964\n",
      "\n",
      "📊 Current Operational Profile:\n",
      "   🔧 Average Engine Load: 51.1%\n",
      "      ✅ Moderate duty cycle\n",
      "   🛣️ Highway driving: 73.6% of time\n",
      "   🏙️ City driving: 17.2% of time\n",
      "      ✅ Good highway driving - helps DPF regeneration\n",
      "\n",
      "📈 Operational Changes (Recent vs Historical):\n",
      "   📊 peak_operating_hour: increased by 16.7%\n",
      "      ⚠️ Notable operational change\n",
      "   📊 operating_hour_spread: decreased by 57.4%\n",
      "      🚨 Significant operational change detected!\n",
      "\n",
      "📊 Total operational features calculated: 28\n"
     ]
    }
   ],
   "source": [
    "def calculate_operational_pattern_features(sensor_data, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate operational pattern features that help identify changes in vehicle usage.\n",
    "    These features help fleet managers understand if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 10:  # Need reasonable amount of data for patterns\n",
    "        return {}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. DUTY CYCLE ANALYSIS (How hard is the vehicle working?)\n",
    "    if 'engineLoadPercent' in sensor_data.columns:\n",
    "        load_data = sensor_data['engineLoadPercent'].dropna()\n",
    "        if len(load_data) >= 10:\n",
    "            \n",
    "            # Average duty cycle\n",
    "            avg_load = load_data.mean()\n",
    "            features['avg_engine_load'] = avg_load\n",
    "            \n",
    "            # Load distribution analysis\n",
    "            features['pct_time_high_load'] = (load_data > 70).mean() * 100  # >70% load\n",
    "            features['pct_time_medium_load'] = ((load_data >= 40) & (load_data <= 70)).mean() * 100\n",
    "            features['pct_time_low_load'] = (load_data < 40).mean() * 100  # <40% load\n",
    "            \n",
    "            # Load consistency (coefficient of variation)\n",
    "            if avg_load > 0:\n",
    "                features['load_consistency'] = load_data.std() / avg_load\n",
    "    \n",
    "    # 2. DRIVE PROFILE ANALYSIS (City vs Highway driving)\n",
    "    if 'ecuSpeedMph' in sensor_data.columns:\n",
    "        speed_data = sensor_data['ecuSpeedMph'].dropna()\n",
    "        if len(speed_data) >= 10:\n",
    "            \n",
    "            # Speed distribution (proxy for drive cycle)\n",
    "            features['pct_time_highway_speed'] = (speed_data > 45).mean() * 100  # >45 mph = highway\n",
    "            features['pct_time_city_speed'] = ((speed_data > 0) & (speed_data <= 35)).mean() * 100  # 0-35 mph = city\n",
    "            features['pct_time_stopped'] = (speed_data == 0).mean() * 100  # Stopped\n",
    "            \n",
    "            # Average operating speed\n",
    "            features['avg_operating_speed'] = speed_data[speed_data > 0].mean() if (speed_data > 0).any() else 0\n",
    "            \n",
    "            # Speed variability (smooth vs stop-and-go driving)\n",
    "            features['speed_variability'] = speed_data.std()\n",
    "    \n",
    "    # 3. OPERATIONAL TIMING PATTERNS\n",
    "    if time_col in sensor_data.columns:\n",
    "        # Add hour of day for operational pattern analysis\n",
    "        sensor_data_copy = sensor_data.copy()\n",
    "        sensor_data_copy['hour'] = pd.to_datetime(sensor_data_copy[time_col]).dt.hour\n",
    "        \n",
    "        # Operating hours distribution\n",
    "        operating_hours = sensor_data_copy['hour'].value_counts().sort_index()\n",
    "        \n",
    "        # Peak operating hours\n",
    "        if len(operating_hours) > 0:\n",
    "            features['peak_operating_hour'] = operating_hours.idxmax()\n",
    "            features['operating_hour_spread'] = operating_hours.max() - operating_hours.min()\n",
    "    \n",
    "    # 4. MULTI-SENSOR OPERATIONAL PATTERNS\n",
    "    # Correlation between load and speed (operational efficiency)\n",
    "    if 'engineLoadPercent' in sensor_data.columns and 'ecuSpeedMph' in sensor_data.columns:\n",
    "        load_speed_data = sensor_data[['engineLoadPercent', 'ecuSpeedMph']].dropna()\n",
    "        if len(load_speed_data) >= 10:\n",
    "            correlation = load_speed_data['engineLoadPercent'].corr(load_speed_data['ecuSpeedMph'])\n",
    "            features['load_speed_correlation'] = correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    # Engine efficiency patterns (RPM vs Load)\n",
    "    if 'engineRpm' in sensor_data.columns and 'engineLoadPercent' in sensor_data.columns:\n",
    "        rpm_load_data = sensor_data[['engineRpm', 'engineLoadPercent']].dropna()\n",
    "        if len(rpm_load_data) >= 10:\n",
    "            # Calculate efficiency proxy (load per RPM)\n",
    "            efficiency_data = rpm_load_data[rpm_load_data['engineRpm'] > 0].copy()\n",
    "            if len(efficiency_data) > 0:\n",
    "                efficiency_data['load_per_rpm'] = efficiency_data['engineLoadPercent'] / efficiency_data['engineRpm'] * 1000\n",
    "                features['avg_load_per_rpm'] = efficiency_data['load_per_rpm'].mean()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compare_operational_periods(sensor_data, split_ratio=0.7, time_col='time'):\n",
    "    \"\"\"\n",
    "    Compare operational patterns between historical (early) and recent periods.\n",
    "    This helps identify if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 20:  # Need enough data to split meaningfully\n",
    "        return {}\n",
    "    \n",
    "    # Sort by time and split\n",
    "    sorted_data = sensor_data.sort_values(time_col)\n",
    "    split_point = int(len(sorted_data) * split_ratio)\n",
    "    \n",
    "    historical_data = sorted_data.iloc[:split_point]\n",
    "    recent_data = sorted_data.iloc[split_point:]\n",
    "    \n",
    "    # Calculate operational features for both periods\n",
    "    historical_features = calculate_operational_pattern_features(historical_data, time_col)\n",
    "    recent_features = calculate_operational_pattern_features(recent_data, time_col)\n",
    "    \n",
    "    # Calculate changes\n",
    "    changes = {}\n",
    "    for feature in historical_features:\n",
    "        if feature in recent_features:\n",
    "            historical_value = historical_features[feature]\n",
    "            recent_value = recent_features[feature]\n",
    "            \n",
    "            if historical_value != 0:\n",
    "                percent_change = ((recent_value - historical_value) / abs(historical_value)) * 100\n",
    "                changes[f'{feature}_change_pct'] = percent_change\n",
    "    \n",
    "    return changes\n",
    "\n",
    "# Demonstrate operational pattern analysis\n",
    "print(\"🔄 Demonstrating Operational Pattern Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing operational patterns for Vehicle: {sample_vin}\")\n",
    "    print(f\"Data points: {len(sample_data)}\")\n",
    "    \n",
    "    # Calculate current operational features\n",
    "    operational_features = calculate_operational_pattern_features(sample_data)\n",
    "    \n",
    "    if operational_features:\n",
    "        print(f\"\\n📊 Current Operational Profile:\")\n",
    "        \n",
    "        # Duty cycle analysis\n",
    "        if 'avg_engine_load' in operational_features:\n",
    "            avg_load = operational_features['avg_engine_load']\n",
    "            print(f\"   🔧 Average Engine Load: {avg_load:.1f}%\")\n",
    "            \n",
    "            if avg_load > 60:\n",
    "                print(f\"      🚨 High duty cycle - may stress DPF\")\n",
    "            elif avg_load < 30:\n",
    "                print(f\"      ⚠️ Low duty cycle - may not regenerate DPF properly\")\n",
    "            else:\n",
    "                print(f\"      ✅ Moderate duty cycle\")\n",
    "        \n",
    "        # Drive profile analysis\n",
    "        if 'pct_time_highway_speed' in operational_features:\n",
    "            highway_pct = operational_features['pct_time_highway_speed']\n",
    "            city_pct = operational_features.get('pct_time_city_speed', 0)\n",
    "            \n",
    "            print(f\"   🛣️ Highway driving: {highway_pct:.1f}% of time\")\n",
    "            print(f\"   🏙️ City driving: {city_pct:.1f}% of time\")\n",
    "            \n",
    "            if highway_pct < 20:\n",
    "                print(f\"      🚨 Mostly city driving - poor for DPF regeneration\")\n",
    "            elif highway_pct > 60:\n",
    "                print(f\"      ✅ Good highway driving - helps DPF regeneration\")\n",
    "            else:\n",
    "                print(f\"      ⚠️ Mixed driving profile\")\n",
    "    \n",
    "    # Compare historical vs recent patterns\n",
    "    pattern_changes = compare_operational_periods(sample_data)\n",
    "    \n",
    "    if pattern_changes:\n",
    "        print(f\"\\n📈 Operational Changes (Recent vs Historical):\")\n",
    "        \n",
    "        significant_changes = {k: v for k, v in pattern_changes.items() if abs(v) > 10}\n",
    "        \n",
    "        if significant_changes:\n",
    "            for change_feature, percent_change in significant_changes.items():\n",
    "                base_feature = change_feature.replace('_change_pct', '')\n",
    "                direction = \"increased\" if percent_change > 0 else \"decreased\"\n",
    "                print(f\"   📊 {base_feature}: {direction} by {abs(percent_change):.1f}%\")\n",
    "                \n",
    "                # Interpret the change\n",
    "                if abs(percent_change) > 25:\n",
    "                    print(f\"      🚨 Significant operational change detected!\")\n",
    "                elif abs(percent_change) > 15:\n",
    "                    print(f\"      ⚠️ Notable operational change\")\n",
    "        else:\n",
    "            print(f\"   ✅ Stable operational patterns\")\n",
    "    \n",
    "    total_features = len(operational_features) + len(pattern_changes)\n",
    "    print(f\"\\n📊 Total operational features calculated: {total_features}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for operational pattern analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Step 5: Feature Validation and Selection\n",
    "\n",
    "Now let's validate that our interpretable features actually predict DPF maintenance needs. This step is crucial for building trust with fleet managers.\n",
    "\n",
    "### What Makes a Good Predictive Feature:\n",
    "1. **Statistically Significant**: Reliably different between healthy and failing vehicles\n",
    "2. **Practically Meaningful**: Changes that fleet managers can act upon\n",
    "3. **Stable**: Consistent behavior across different vehicles and time periods\n",
    "4. **Explainable**: Clear causal relationship to DPF health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Building Comprehensive Interpretable Feature Dataset\n",
      "============================================================\n",
      "🔧 Building comprehensive feature dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns, UTC] and Timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:559\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:542\u001b[39m, in \u001b[36mDatetimeArray._check_compatible_with\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assert_tzawareness_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:788\u001b[39m, in \u001b[36mDatetimeArray._assert_tzawareness_compat\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m other_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot compare tz-naive and tz-aware datetime-like objects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    790\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Cannot compare tz-naive and tz-aware datetime-like objects",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInvalidComparison\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1006\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     other = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_comparison_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:562\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m         \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidComparison(other) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(other):\n",
      "\u001b[31mInvalidComparison\u001b[39m: 2023-05-13 00:00:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔧 Building Comprehensive Interpretable Feature Dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m feature_dataset = \u001b[43mbuild_comprehensive_feature_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaintenance_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_dataset) > \u001b[32m0\u001b[39m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Built feature dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(feature_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mbuild_comprehensive_feature_dataset\u001b[39m\u001b[34m(maintenance_df, sensor_df, window_days)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get sensor data before maintenance\u001b[39;00m\n\u001b[32m     18\u001b[39m start_date = maintenance_date - timedelta(days=window_days)\n\u001b[32m     20\u001b[39m vehicle_data = sensor_df[\n\u001b[32m     21\u001b[39m     (sensor_df[\u001b[33m'\u001b[39m\u001b[33mvin\u001b[39m\u001b[33m'\u001b[39m] == vin) &\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     (\u001b[43msensor_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m) &\n\u001b[32m     23\u001b[39m     (sensor_df[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m] < maintenance_date)\n\u001b[32m     24\u001b[39m ].copy()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vehicle_data) < \u001b[32m10\u001b[39m:  \u001b[38;5;66;03m# Need minimum data\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/series.py:6130\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6127\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6128\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:330\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLengths must match to compare\u001b[39m\u001b[33m\"\u001b[39m, lvalues.shape, rvalues.shape\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    326\u001b[39m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[32m    327\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m lvalues.dtype != \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    328\u001b[39m ):\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator.ne:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1008\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1006\u001b[39m     other = \u001b[38;5;28mself\u001b[39m._validate_comparison_value(other)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[32m   1012\u001b[39m     \u001b[38;5;66;03m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m#  comparison otherwise it would raise when comparing to None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/invalid.py:40\u001b[39m, in \u001b[36minvalid_comparison\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     typ = \u001b[38;5;28mtype\u001b[39m(right).\u001b[34m__name__\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[31mTypeError\u001b[39m: Invalid comparison between dtype=datetime64[ns, UTC] and Timestamp"
     ]
    }
   ],
   "source": [
    "def build_comprehensive_feature_dataset(maintenance_df, sensor_df, window_days=30):\n",
    "    \"\"\"\n",
    "    Build a comprehensive dataset with all our interpretable features.\n",
    "    This combines trend, threshold, and operational pattern features.\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Building comprehensive feature dataset...\")\n",
    "    \n",
    "    feature_data = []\n",
    "    \n",
    "    for _, maintenance_row in maintenance_df.iterrows():\n",
    "        vin = maintenance_row['VIN Number']\n",
    "        maintenance_date = maintenance_row['Date of Issue']\n",
    "        \n",
    "        if pd.isna(vin) or pd.isna(maintenance_date):\n",
    "            continue\n",
    "        \n",
    "        # Get sensor data before maintenance\n",
    "        start_date = maintenance_date - timedelta(days=window_days)\n",
    "        \n",
    "        vehicle_data = sensor_df[\n",
    "            (sensor_df['vin'] == vin) &\n",
    "            (sensor_df['time'] >= start_date) &\n",
    "            (sensor_df['time'] < maintenance_date)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(vehicle_data) < 10:  # Need minimum data\n",
    "            continue\n",
    "        \n",
    "        # Calculate all feature types\n",
    "        all_features = {\n",
    "            'vin': vin,\n",
    "            'vehicle_number': maintenance_row['Vehicle_Number'],\n",
    "            'maintenance_date': maintenance_date,\n",
    "            'maintenance_type': maintenance_row['lines_jobDescriptions'],\n",
    "            'data_points': len(vehicle_data),\n",
    "            'window_days': window_days\n",
    "        }\n",
    "        \n",
    "        # 1. Trend features for each DPF sensor\n",
    "        for sensor in dpf_sensors.keys():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    trend_features = calculate_trend_features(sensor_subset, sensor)\n",
    "                    all_features.update(trend_features)\n",
    "        \n",
    "        # 2. Threshold features for each DPF sensor\n",
    "        for sensor, info in dpf_sensors.items():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    threshold_features = calculate_threshold_features(\n",
    "                        sensor_subset, sensor, info['alert_thresholds']\n",
    "                    )\n",
    "                    all_features.update(threshold_features)\n",
    "        \n",
    "        # 3. Operational pattern features\n",
    "        operational_features = calculate_operational_pattern_features(vehicle_data)\n",
    "        all_features.update(operational_features)\n",
    "        \n",
    "        # 4. Operational change features\n",
    "        pattern_changes = compare_operational_periods(vehicle_data)\n",
    "        all_features.update(pattern_changes)\n",
    "        \n",
    "        feature_data.append(all_features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "# Build the comprehensive feature dataset\n",
    "print(\"🔧 Building Comprehensive Interpretable Feature Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dataset = build_comprehensive_feature_dataset(maintenance_df, sensor_df)\n",
    "\n",
    "if len(feature_dataset) > 0:\n",
    "    print(f\"✅ Built feature dataset with {len(feature_dataset)} examples\")\n",
    "    \n",
    "    # Analyze feature coverage\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"📊 Total interpretable features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation', 'excess', 'deficit'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   📈 Trend features: {len(trend_features)}\")\n",
    "    print(f\"   🎯 Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   🔄 Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    # Show maintenance type distribution\n",
    "    print(f\"\\n🔧 Maintenance Type Distribution:\")\n",
    "    for maint_type, count in feature_dataset['maintenance_type'].value_counts().items():\n",
    "        print(f\"   {maint_type}: {count} examples\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Could not build feature dataset - insufficient data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature quality and predictive power\n",
    "def validate_interpretable_features(feature_dataset):\n",
    "    \"\"\"\n",
    "    Validate the quality and predictive power of our interpretable features.\n",
    "    \"\"\"\n",
    "    if len(feature_dataset) == 0:\n",
    "        return\n",
    "    \n",
    "    print(\"🧪 Validating Interpretable Features\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"❌ No features to validate\")\n",
    "        return\n",
    "    \n",
    "    # 1. DATA QUALITY VALIDATION\n",
    "    print(\"📊 Data Quality Analysis:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_rates = feature_dataset[feature_cols].isnull().mean() * 100\n",
    "    high_missing = missing_rates[missing_rates > 50]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   ⚠️ Features with >50% missing data: {len(high_missing)}\")\n",
    "        for feature, missing_pct in high_missing.head(5).items():\n",
    "            print(f\"      - {feature}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(f\"   ✅ Good data coverage - no features with >50% missing data\")\n",
    "    \n",
    "    # 2. FEATURE DISCRIMINATIVE POWER\n",
    "    print(f\"\\n🔍 Feature Discriminative Power (by Maintenance Type):\")\n",
    "    \n",
    "    # For each maintenance type, find features that discriminate well\n",
    "    maintenance_types = feature_dataset['maintenance_type'].unique()\n",
    "    \n",
    "    if len(maintenance_types) > 1:\n",
    "        # Calculate feature means by maintenance type\n",
    "        type_means = feature_dataset.groupby('maintenance_type')[feature_cols].mean()\n",
    "        \n",
    "        # Calculate coefficient of variation across maintenance types for each feature\n",
    "        discriminative_power = {}\n",
    "        for feature in feature_cols:\n",
    "            values = type_means[feature].dropna()\n",
    "            if len(values) > 1 and values.std() > 0:\n",
    "                cv = values.std() / abs(values.mean()) if values.mean() != 0 else 0\n",
    "                discriminative_power[feature] = cv\n",
    "        \n",
    "        # Show top discriminative features\n",
    "        if discriminative_power:\n",
    "            top_discriminative = sorted(discriminative_power.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"   🏆 Top 10 Most Discriminative Features:\")\n",
    "            for i, (feature, cv) in enumerate(top_discriminative, 1):\n",
    "                # Determine feature type\n",
    "                if 'trend' in feature:\n",
    "                    feature_type = \"📈 Trend\"\n",
    "                elif any(x in feature for x in ['pct_time', 'streak', 'violation']):\n",
    "                    feature_type = \"🎯 Threshold\"\n",
    "                else:\n",
    "                    feature_type = \"🔄 Operational\"\n",
    "                \n",
    "                print(f\"      {i}. {feature_type} - {feature}: CV={cv:.3f}\")\n",
    "    \n",
    "    # 3. FEATURE INTERPRETABILITY ASSESSMENT\n",
    "    print(f\"\\n💡 Feature Interpretability Assessment:\")\n",
    "    \n",
    "    interpretability_scores = {\n",
    "        'trend_slope': {'score': 10, 'explanation': 'Clear direction and rate of change'},\n",
    "        'pct_time': {'score': 9, 'explanation': 'Percentage time outside thresholds'},\n",
    "        'avg_': {'score': 8, 'explanation': 'Simple average values'},\n",
    "        'max_streak': {'score': 8, 'explanation': 'Consecutive violations count'},\n",
    "        'change_pct': {'score': 7, 'explanation': 'Percentage change over time'},\n",
    "        'correlation': {'score': 6, 'explanation': 'Relationship between sensors'},\n",
    "        'volatility': {'score': 5, 'explanation': 'Stability measure'}\n",
    "    }\n",
    "    \n",
    "    # Score each feature\n",
    "    feature_scores = []\n",
    "    for feature in feature_cols:\n",
    "        score = 3  # Default score\n",
    "        explanation = \"General metric\"\n",
    "        \n",
    "        for pattern, info in interpretability_scores.items():\n",
    "            if pattern in feature:\n",
    "                score = info['score']\n",
    "                explanation = info['explanation']\n",
    "                break\n",
    "        \n",
    "        feature_scores.append((feature, score, explanation))\n",
    "    \n",
    "    # Show most interpretable features\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   🌟 Most Interpretable Features (Score 8-10):\")\n",
    "    high_interpretability = [f for f in feature_scores if f[1] >= 8]\n",
    "    \n",
    "    for feature, score, explanation in high_interpretability[:10]:\n",
    "        print(f\"      • {feature} (Score: {score}/10) - {explanation}\")\n",
    "    \n",
    "    # 4. FEATURE RECOMMENDATION\n",
    "    print(f\"\\n🎯 Feature Recommendations for Fleet Managers:\")\n",
    "    \n",
    "    # Combine discriminative power and interpretability\n",
    "    if discriminative_power:\n",
    "        recommended_features = []\n",
    "        \n",
    "        for feature, interp_score, explanation in feature_scores:\n",
    "            if feature in discriminative_power and interp_score >= 7:\n",
    "                combined_score = discriminative_power[feature] * (interp_score / 10)\n",
    "                recommended_features.append((feature, combined_score, interp_score, explanation))\n",
    "        \n",
    "        recommended_features.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"   🏆 Top 5 Recommended Features (High interpretability + High predictive power):\")\n",
    "        for i, (feature, combined_score, interp_score, explanation) in enumerate(recommended_features[:5], 1):\n",
    "            print(f\"      {i}. {feature}\")\n",
    "            print(f\"         Interpretability: {interp_score}/10 - {explanation}\")\n",
    "            print(f\"         Combined Score: {combined_score:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Run feature validation\n",
    "if len(feature_dataset) > 0:\n",
    "    validate_interpretable_features(feature_dataset)\n",
    "else:\n",
    "    print(\"❌ No feature dataset available for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Step 6: Feature Engineering Best Practices Summary\n",
    "\n",
    "### 🎯 Key Principles for Interpretable Features:\n",
    "\n",
    "1. **Domain-Driven**: Every feature should relate to known DPF failure modes\n",
    "2. **Actionable**: Fleet managers should know what to do when a feature triggers\n",
    "3. **Explainable**: Each feature should tell a clear story about vehicle health\n",
    "4. **Validated**: Features must actually predict maintenance needs\n",
    "\n",
    "### 📊 Feature Categories We've Built:\n",
    "\n",
    "**Trend Features** (📈):\n",
    "- Sensor slopes (increasing/decreasing patterns)\n",
    "- Trend strength (how reliable the trend is)\n",
    "- Recent vs historical comparisons\n",
    "\n",
    "**Threshold Features** (🎯):\n",
    "- Time spent outside normal ranges\n",
    "- Consecutive violation streaks\n",
    "- Severity of threshold breaches\n",
    "\n",
    "**Operational Features** (🔄):\n",
    "- Duty cycle changes\n",
    "- Drive profile shifts (city vs highway)\n",
    "- Multi-sensor pattern correlations\n",
    "\n",
    "### 🚀 Implementation Roadmap:\n",
    "\n",
    "1. **Start Simple**: Begin with top 5 most interpretable features\n",
    "2. **Validate Continuously**: Track feature performance against actual maintenance\n",
    "3. **Refine Thresholds**: Adjust based on operational experience\n",
    "4. **Add Domain Knowledge**: Incorporate fleet manager insights into features\n",
    "5. **Scale Gradually**: Add more features as confidence builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the interpretable feature dataset for use in other notebooks\n",
    "if len(feature_dataset) > 0:\n",
    "    output_file = '../data/interpretable_features_dataset.csv'\n",
    "    feature_dataset.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"💾 FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"✅ Saved interpretable features to: {output_file}\")\n",
    "    print(f\"📊 Dataset contains: {len(feature_dataset)} examples\")\n",
    "    \n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    print(f\"🔧 Total features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   📈 Trend features: {len(trend_features)}\")\n",
    "    print(f\"   🎯 Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   🔄 Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Ready for model building and deployment!\")\n",
    "    print(f\"Next step: Use these features in the explainable model notebook\")\n",
    "else:\n",
    "    print(\"❌ No interpretable features dataset created\")\n",
    "    print(\"Please check data availability and try again\")\n",
    "\n",
    "print(f\"\\n📚 What You've Learned:\")\n",
    "print(f\"• How to create trend features that show sensor degradation patterns\")\n",
    "print(f\"• How to build threshold features for actionable alerts\")\n",
    "print(f\"• How to detect operational pattern changes affecting DPF health\")\n",
    "print(f\"• How to validate feature quality and interpretability\")\n",
    "print(f\"• Best practices for explainable feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dms3noi9r0g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ready to build interpretable features!\n",
      "📚 This notebook will teach you to create features that:\n",
      "   • Fleet managers can understand\n",
      "   • Maintenance teams can act upon\n",
      "   • Actually predict DPF issues\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"🚀 Ready to build interpretable features!\")\n",
    "print(\"📚 This notebook will teach you to create features that:\")\n",
    "print(\"   • Fleet managers can understand\")\n",
    "print(\"   • Maintenance teams can act upon\")\n",
    "print(\"   • Actually predict DPF issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jai8mnjzpz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading DPF datasets...\n",
      "✅ Loaded 82 maintenance events\n",
      "✅ Loaded 4,466,272 sensor readings\n"
     ]
    }
   ],
   "source": [
    "# Load our datasets\n",
    "print(\"📁 Loading DPF datasets...\")\n",
    "\n",
    "try:\n",
    "    maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n",
    "    sensor_df = pd.read_csv('../data/dpf_vehicle_stats.csv')\n",
    "    \n",
    "    # Convert datetime columns and handle timezones\n",
    "    maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n",
    "    sensor_df['time'] = pd.to_datetime(sensor_df['time']).dt.tz_localize(None)  # Remove timezone info\n",
    "    \n",
    "    print(f\"✅ Loaded {len(maintenance_df)} maintenance events\")\n",
    "    print(f\"✅ Loaded {len(sensor_df):,} sensor readings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Please run the data munging script first: uv run python 01_data_munging.py\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75t9lyn3w9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing DPF-Relevant Sensors\n",
      "==================================================\n",
      "\n",
      "📊 Engine Load Percentage (engineLoadPercent)\n",
      "   📈 Data Availability: 36.3% (1,619,223 readings)\n",
      "   🎯 DPF Relevance: High load generates more soot, stressing DPF\n",
      "   📏 Normal Range: (20, 60) %\n",
      "   📊 Current Range: 1.0 - 125.0 %\n",
      "\n",
      "📊 Engine RPM (engineRpm)\n",
      "   📈 Data Availability: 42.3% (1,889,794 readings)\n",
      "   🎯 DPF Relevance: RPM affects DPF regeneration efficiency\n",
      "   📏 Normal Range: (800, 1800) RPM\n",
      "   📊 Current Range: 26.0 - 2618.0 RPM\n",
      "\n",
      "📊 DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent)\n",
      "   📈 Data Availability: 45.9% (2,048,969 readings)\n",
      "   🎯 DPF Relevance: Low DEF prevents proper DPF operation\n",
      "   📏 Normal Range: (60000, 95000) milli-%\n",
      "   📊 Current Range: 400.0 - 100000.0 milli-%\n",
      "\n",
      "📊 Engine Coolant Temperature (engineCoolantTemperatureMilliC)\n",
      "   📈 Data Availability: 33.5% (1,497,692 readings)\n",
      "   🎯 DPF Relevance: Temperature affects DPF regeneration cycles\n",
      "   📏 Normal Range: (75000, 90000) milli-°C\n",
      "   📊 Current Range: 1000.0 - 123000.0 milli-°C\n",
      "\n",
      "📊 Vehicle Speed (ecuSpeedMph)\n",
      "   📈 Data Availability: 69.0% (3,079,837 readings)\n",
      "   🎯 DPF Relevance: Highway speeds help DPF regeneration\n",
      "   📏 Normal Range: (0, 65) mph\n",
      "   📊 Current Range: 0.0 - 88.9 mph\n"
     ]
    }
   ],
   "source": [
    "# Identify DPF-relevant sensors and their characteristics\n",
    "print(\"🔍 Analyzing DPF-Relevant Sensors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define sensors most relevant to DPF health with their meanings\n",
    "dpf_sensors = {\n",
    "    'engineLoadPercent': {\n",
    "        'description': 'Engine Load Percentage',\n",
    "        'dpf_relevance': 'High load generates more soot, stressing DPF',\n",
    "        'normal_range': (20, 60),\n",
    "        'alert_thresholds': {'high': 80, 'low': 10},\n",
    "        'unit': '%'\n",
    "    },\n",
    "    'engineRpm': {\n",
    "        'description': 'Engine RPM',\n",
    "        'dpf_relevance': 'RPM affects DPF regeneration efficiency',\n",
    "        'normal_range': (800, 1800),\n",
    "        'alert_thresholds': {'high': 2000, 'low': 600},\n",
    "        'unit': 'RPM'\n",
    "    },\n",
    "    'defLevelMilliPercent': {\n",
    "        'description': 'DEF (Diesel Exhaust Fluid) Level',\n",
    "        'dpf_relevance': 'Low DEF prevents proper DPF operation',\n",
    "        'normal_range': (60000, 95000),  # 60-95%\n",
    "        'alert_thresholds': {'high': 95000, 'low': 50000},\n",
    "        'unit': 'milli-%'\n",
    "    },\n",
    "    'engineCoolantTemperatureMilliC': {\n",
    "        'description': 'Engine Coolant Temperature',\n",
    "        'dpf_relevance': 'Temperature affects DPF regeneration cycles',\n",
    "        'normal_range': (75000, 90000),  # 75-90°C\n",
    "        'alert_thresholds': {'high': 95000, 'low': 65000},\n",
    "        'unit': 'milli-°C'\n",
    "    },\n",
    "    'ecuSpeedMph': {\n",
    "        'description': 'Vehicle Speed',\n",
    "        'dpf_relevance': 'Highway speeds help DPF regeneration',\n",
    "        'normal_range': (0, 65),\n",
    "        'alert_thresholds': {'high': 80, 'low': 0},\n",
    "        'unit': 'mph'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze data availability for each sensor\n",
    "for sensor, info in dpf_sensors.items():\n",
    "    if sensor in sensor_df.columns:\n",
    "        non_null = sensor_df[sensor].notna().sum()\n",
    "        total = len(sensor_df)\n",
    "        availability = (non_null / total) * 100\n",
    "        \n",
    "        print(f\"\\n📊 {info['description']} ({sensor})\")\n",
    "        print(f\"   📈 Data Availability: {availability:.1f}% ({non_null:,} readings)\")\n",
    "        print(f\"   🎯 DPF Relevance: {info['dpf_relevance']}\")\n",
    "        print(f\"   📏 Normal Range: {info['normal_range']} {info['unit']}\")\n",
    "        \n",
    "        if non_null > 0:\n",
    "            values = sensor_df[sensor].dropna()\n",
    "            print(f\"   📊 Current Range: {values.min():.1f} - {values.max():.1f} {info['unit']}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {info['description']} ({sensor}) - No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1m6xief9fh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Demonstrating Trend Analysis\n",
      "========================================\n",
      "Sample Vehicle: 1XK1D40X1NJ495537\n",
      "Data Points: 269964\n",
      "Date Range: 2024-06-08 00:03:00 to 2025-05-21 20:40:00\n",
      "\n",
      "📈 Engine Load Percentage:\n",
      "   Trend: increasing by 0.013 %/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   ✅ Normal: Stable readings\n",
      "\n",
      "📈 Engine RPM:\n",
      "   Trend: increasing by 0.111 RPM/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate increasing trend\n",
      "\n",
      "📈 DEF (Diesel Exhaust Fluid) Level:\n",
      "   Trend: decreasing by 61.047 milli-%/day\n",
      "   Strength: 0.072 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate decreasing trend\n",
      "\n",
      "📈 Engine Coolant Temperature:\n",
      "   Trend: decreasing by 9.565 milli-°C/day\n",
      "   Strength: 0.005 (0=weak, 1=strong linear trend)\n",
      "   ⚠️ Caution: Moderate decreasing trend\n",
      "\n",
      "📈 Vehicle Speed:\n",
      "   Trend: decreasing by 0.003 mph/day\n",
      "   Strength: 0.000 (0=weak, 1=strong linear trend)\n",
      "   ✅ Normal: Stable readings\n",
      "\n",
      "📊 Total trend features calculated: 30\n"
     ]
    }
   ],
   "source": [
    "def calculate_trend_features(sensor_data, sensor_name, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate interpretable trend features for a sensor.\n",
    "    Returns features that fleet managers can understand and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 3:  # Need minimum points for trend\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 3:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    times = pd.to_numeric(data[time_col]) / 1e9  # Convert to seconds\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. LINEAR TREND (Most Important)\n",
    "    # Calculate slope in original units per day\n",
    "    slope_per_second, intercept = np.polyfit(times, values, 1)\n",
    "    slope_per_day = slope_per_second * 86400  # Convert to per day\n",
    "    \n",
    "    features[f'{sensor_name}_trend_slope_per_day'] = slope_per_day\n",
    "    \n",
    "    # 2. TREND STRENGTH (How reliable is the trend?)\n",
    "    # R-squared shows how well the linear trend fits\n",
    "    correlation = np.corrcoef(times, values)[0, 1]\n",
    "    r_squared = correlation ** 2 if not np.isnan(correlation) else 0\n",
    "    features[f'{sensor_name}_trend_strength'] = r_squared\n",
    "    \n",
    "    # 3. TREND SIGNIFICANCE (Is this trend meaningful?)\n",
    "    # Calculate statistical significance of the trend\n",
    "    if len(values) >= 3:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(times, values)\n",
    "        features[f'{sensor_name}_trend_p_value'] = p_value\n",
    "        features[f'{sensor_name}_trend_significant'] = 1 if p_value < 0.05 else 0\n",
    "    \n",
    "    # 4. RECENT VS HISTORICAL TREND\n",
    "    # Compare recent behavior to historical (last 25% vs first 75%)\n",
    "    split_point = int(len(values) * 0.75)\n",
    "    if split_point > 2 and split_point < len(values) - 2:\n",
    "        historical_mean = np.mean(values[:split_point])\n",
    "        recent_mean = np.mean(values[split_point:])\n",
    "        \n",
    "        if historical_mean != 0:\n",
    "            percent_change = ((recent_mean - historical_mean) / abs(historical_mean)) * 100\n",
    "            features[f'{sensor_name}_recent_vs_historical_pct'] = percent_change\n",
    "    \n",
    "    # 5. VOLATILITY TREND (Is stability changing?)\n",
    "    # Calculate if the sensor is becoming more or less stable\n",
    "    if len(values) >= 6:\n",
    "        mid_point = len(values) // 2\n",
    "        early_std = np.std(values[:mid_point])\n",
    "        late_std = np.std(values[mid_point:])\n",
    "        \n",
    "        if early_std > 0:\n",
    "            volatility_change = (late_std - early_std) / early_std\n",
    "            features[f'{sensor_name}_volatility_change'] = volatility_change\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Demonstrate trend analysis with a sample vehicle\n",
    "print(\"🔍 Demonstrating Trend Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get a vehicle with good data coverage\n",
    "vehicle_counts = sensor_df['vin'].value_counts()\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Sample Vehicle: {sample_vin}\")\n",
    "    print(f\"Data Points: {len(sample_data)}\")\n",
    "    print(f\"Date Range: {sample_data['time'].min()} to {sample_data['time'].max()}\")\n",
    "    \n",
    "    # Calculate trends for each DPF sensor\n",
    "    all_trends = {}\n",
    "    for sensor in dpf_sensors.keys():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                trends = calculate_trend_features(sensor_subset, sensor)\n",
    "                all_trends.update(trends)\n",
    "                \n",
    "                # Show human-readable interpretation\n",
    "                slope_key = f'{sensor}_trend_slope_per_day'\n",
    "                if slope_key in trends:\n",
    "                    slope = trends[slope_key]\n",
    "                    direction = \"increasing\" if slope > 0 else \"decreasing\"\n",
    "                    strength = trends.get(f'{sensor}_trend_strength', 0)\n",
    "                    \n",
    "                    print(f\"\\n📈 {dpf_sensors[sensor]['description']}:\")\n",
    "                    print(f\"   Trend: {direction} by {abs(slope):.3f} {dpf_sensors[sensor]['unit']}/day\")\n",
    "                    print(f\"   Strength: {strength:.3f} (0=weak, 1=strong linear trend)\")\n",
    "                    \n",
    "                    # Interpret for fleet manager\n",
    "                    if abs(slope) > 0.1 and strength > 0.3:\n",
    "                        print(f\"   🚨 Alert: Strong {direction} trend detected!\")\n",
    "                    elif abs(slope) > 0.05:\n",
    "                        print(f\"   ⚠️ Caution: Moderate {direction} trend\")\n",
    "                    else:\n",
    "                        print(f\"   ✅ Normal: Stable readings\")\n",
    "    \n",
    "    print(f\"\\n📊 Total trend features calculated: {len(all_trends)}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for trend analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "y8rc06quid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Demonstrating Threshold Analysis\n",
      "========================================\n",
      "Analyzing thresholds for Vehicle: 1XK1D40X1NJ495537\n",
      "\n",
      "🎯 Engine Load Percentage (engineLoadPercent):\n",
      "   ⬆️ Time above 80 %: 24.8%\n",
      "      🚨 ALERT: Excessive high readings!\n",
      "   ⬇️ Time below 10 %: 6.4%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 16 consecutive readings\n",
      "\n",
      "🎯 Engine RPM (engineRpm):\n",
      "   ⬆️ Time above 2000 RPM: 0.0%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 600 RPM: 4.8%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 1 consecutive readings\n",
      "\n",
      "🎯 DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent):\n",
      "   ⬆️ Time above 95000 milli-%: 5.4%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 50000 milli-%: 28.5%\n",
      "      🚨 ALERT: Excessive low readings!\n",
      "   📊 Longest high streak: 101 consecutive readings\n",
      "\n",
      "🎯 Engine Coolant Temperature (engineCoolantTemperatureMilliC):\n",
      "   ⬆️ Time above 95000 milli-°C: 17.6%\n",
      "      ⚠️ WARNING: Frequent high readings\n",
      "   ⬇️ Time below 65000 milli-°C: 4.2%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 35 consecutive readings\n",
      "\n",
      "🎯 Vehicle Speed (ecuSpeedMph):\n",
      "   ⬆️ Time above 80 mph: 9.7%\n",
      "      ✅ Normal high threshold behavior\n",
      "   ⬇️ Time below 0 mph: 0.0%\n",
      "      ✅ Normal low threshold behavior\n",
      "   📊 Longest high streak: 18 consecutive readings\n",
      "\n",
      "📊 Total threshold features calculated: 44\n"
     ]
    }
   ],
   "source": [
    "def calculate_threshold_features(sensor_data, sensor_name, thresholds, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate threshold-based features that are easy to interpret and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    features = {}\n",
    "    \n",
    "    # 1. PERCENTAGE OF TIME OUTSIDE THRESHOLDS\n",
    "    \n",
    "    # Time above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        high_thresh = thresholds['high']\n",
    "        pct_above_high = (values > high_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_above_high'] = pct_above_high\n",
    "    \n",
    "    # Time below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        low_thresh = thresholds['low']\n",
    "        pct_below_low = (values < low_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_below_low'] = pct_below_low\n",
    "    \n",
    "    # Total time outside normal range\n",
    "    if 'high' in thresholds and 'low' in thresholds:\n",
    "        outside_normal = ((values > thresholds['high']) | (values < thresholds['low'])).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_outside_normal'] = outside_normal\n",
    "    \n",
    "    # 2. CONSECUTIVE VIOLATIONS (Streak Analysis)\n",
    "    \n",
    "    # Longest streak above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        above_high = values > thresholds['high']\n",
    "        max_streak_high = calculate_max_consecutive(above_high)\n",
    "        features[f'{sensor_name}_max_streak_above_high'] = max_streak_high\n",
    "    \n",
    "    # Longest streak below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        below_low = values < thresholds['low']\n",
    "        max_streak_low = calculate_max_consecutive(below_low)\n",
    "        features[f'{sensor_name}_max_streak_below_low'] = max_streak_low\n",
    "    \n",
    "    # 3. FREQUENCY OF VIOLATIONS\n",
    "    \n",
    "    # How many separate incidents of threshold violations?\n",
    "    if 'high' in thresholds:\n",
    "        high_violations = count_violation_episodes(values > thresholds['high'])\n",
    "        features[f'{sensor_name}_high_violation_episodes'] = high_violations\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations = count_violation_episodes(values < thresholds['low'])\n",
    "        features[f'{sensor_name}_low_violation_episodes'] = low_violations\n",
    "    \n",
    "    # 4. SEVERITY OF VIOLATIONS\n",
    "    \n",
    "    # Average severity when violations occur\n",
    "    if 'high' in thresholds:\n",
    "        high_violations_mask = values > thresholds['high']\n",
    "        if high_violations_mask.any():\n",
    "            avg_excess = (values[high_violations_mask] - thresholds['high']).mean()\n",
    "            features[f'{sensor_name}_avg_excess_above_high'] = avg_excess\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations_mask = values < thresholds['low']\n",
    "        if low_violations_mask.any():\n",
    "            avg_deficit = (thresholds['low'] - values[low_violations_mask]).mean()\n",
    "            features[f'{sensor_name}_avg_deficit_below_low'] = avg_deficit\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_max_consecutive(boolean_array):\n",
    "    \"\"\"Calculate maximum consecutive True values in a boolean array.\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    \n",
    "    return max_streak\n",
    "\n",
    "def count_violation_episodes(boolean_array):\n",
    "    \"\"\"Count number of separate violation episodes (groups of consecutive True values).\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    episodes = 0\n",
    "    in_violation = False\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value and not in_violation:\n",
    "            episodes += 1\n",
    "            in_violation = True\n",
    "        elif not value:\n",
    "            in_violation = False\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Demonstrate threshold analysis\n",
    "print(\"🎯 Demonstrating Threshold Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing thresholds for Vehicle: {sample_vin}\")\n",
    "    \n",
    "    # Calculate threshold features for each sensor\n",
    "    all_threshold_features = {}\n",
    "    \n",
    "    for sensor, info in dpf_sensors.items():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                thresholds = info['alert_thresholds']\n",
    "                threshold_features = calculate_threshold_features(sensor_subset, sensor, thresholds)\n",
    "                all_threshold_features.update(threshold_features)\n",
    "                \n",
    "                print(f\"\\n🎯 {info['description']} ({sensor}):\")\n",
    "                \n",
    "                # Show key threshold metrics\n",
    "                pct_high_key = f'{sensor}_pct_time_above_high'\n",
    "                pct_low_key = f'{sensor}_pct_time_below_low'\n",
    "                \n",
    "                if pct_high_key in threshold_features:\n",
    "                    pct_high = threshold_features[pct_high_key]\n",
    "                    print(f\"   ⬆️ Time above {thresholds['high']} {info['unit']}: {pct_high:.1f}%\")\n",
    "                    \n",
    "                    if pct_high > 20:\n",
    "                        print(f\"      🚨 ALERT: Excessive high readings!\")\n",
    "                    elif pct_high > 10:\n",
    "                        print(f\"      ⚠️ WARNING: Frequent high readings\")\n",
    "                    else:\n",
    "                        print(f\"      ✅ Normal high threshold behavior\")\n",
    "                \n",
    "                if pct_low_key in threshold_features:\n",
    "                    pct_low = threshold_features[pct_low_key]\n",
    "                    print(f\"   ⬇️ Time below {thresholds['low']} {info['unit']}: {pct_low:.1f}%\")\n",
    "                    \n",
    "                    if pct_low > 20:\n",
    "                        print(f\"      🚨 ALERT: Excessive low readings!\")\n",
    "                    elif pct_low > 10:\n",
    "                        print(f\"      ⚠️ WARNING: Frequent low readings\")\n",
    "                    else:\n",
    "                        print(f\"      ✅ Normal low threshold behavior\")\n",
    "                \n",
    "                # Show streak information\n",
    "                streak_high_key = f'{sensor}_max_streak_above_high'\n",
    "                if streak_high_key in threshold_features:\n",
    "                    streak = threshold_features[streak_high_key]\n",
    "                    print(f\"   📊 Longest high streak: {streak} consecutive readings\")\n",
    "    \n",
    "    print(f\"\\n📊 Total threshold features calculated: {len(all_threshold_features)}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for threshold analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vcc76a24vdr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Demonstrating Operational Pattern Analysis\n",
      "==================================================\n",
      "Analyzing operational patterns for Vehicle: 1XK1D40X1NJ495537\n",
      "Data points: 269964\n",
      "\n",
      "📊 Current Operational Profile:\n",
      "   🔧 Average Engine Load: 51.1%\n",
      "      ✅ Moderate duty cycle\n",
      "   🛣️ Highway driving: 73.6% of time\n",
      "   🏙️ City driving: 17.2% of time\n",
      "      ✅ Good highway driving - helps DPF regeneration\n",
      "\n",
      "📈 Operational Changes (Recent vs Historical):\n",
      "   📊 peak_operating_hour: increased by 16.7%\n",
      "      ⚠️ Notable operational change\n",
      "   📊 operating_hour_spread: decreased by 57.4%\n",
      "      🚨 Significant operational change detected!\n",
      "\n",
      "📊 Total operational features calculated: 28\n"
     ]
    }
   ],
   "source": [
    "def calculate_operational_pattern_features(sensor_data, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate operational pattern features that help identify changes in vehicle usage.\n",
    "    These features help fleet managers understand if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 10:  # Need reasonable amount of data for patterns\n",
    "        return {}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. DUTY CYCLE ANALYSIS (How hard is the vehicle working?)\n",
    "    if 'engineLoadPercent' in sensor_data.columns:\n",
    "        load_data = sensor_data['engineLoadPercent'].dropna()\n",
    "        if len(load_data) >= 10:\n",
    "            \n",
    "            # Average duty cycle\n",
    "            avg_load = load_data.mean()\n",
    "            features['avg_engine_load'] = avg_load\n",
    "            \n",
    "            # Load distribution analysis\n",
    "            features['pct_time_high_load'] = (load_data > 70).mean() * 100  # >70% load\n",
    "            features['pct_time_medium_load'] = ((load_data >= 40) & (load_data <= 70)).mean() * 100\n",
    "            features['pct_time_low_load'] = (load_data < 40).mean() * 100  # <40% load\n",
    "            \n",
    "            # Load consistency (coefficient of variation)\n",
    "            if avg_load > 0:\n",
    "                features['load_consistency'] = load_data.std() / avg_load\n",
    "    \n",
    "    # 2. DRIVE PROFILE ANALYSIS (City vs Highway driving)\n",
    "    if 'ecuSpeedMph' in sensor_data.columns:\n",
    "        speed_data = sensor_data['ecuSpeedMph'].dropna()\n",
    "        if len(speed_data) >= 10:\n",
    "            \n",
    "            # Speed distribution (proxy for drive cycle)\n",
    "            features['pct_time_highway_speed'] = (speed_data > 45).mean() * 100  # >45 mph = highway\n",
    "            features['pct_time_city_speed'] = ((speed_data > 0) & (speed_data <= 35)).mean() * 100  # 0-35 mph = city\n",
    "            features['pct_time_stopped'] = (speed_data == 0).mean() * 100  # Stopped\n",
    "            \n",
    "            # Average operating speed\n",
    "            features['avg_operating_speed'] = speed_data[speed_data > 0].mean() if (speed_data > 0).any() else 0\n",
    "            \n",
    "            # Speed variability (smooth vs stop-and-go driving)\n",
    "            features['speed_variability'] = speed_data.std()\n",
    "    \n",
    "    # 3. OPERATIONAL TIMING PATTERNS\n",
    "    if time_col in sensor_data.columns:\n",
    "        # Add hour of day for operational pattern analysis\n",
    "        sensor_data_copy = sensor_data.copy()\n",
    "        sensor_data_copy['hour'] = pd.to_datetime(sensor_data_copy[time_col]).dt.hour\n",
    "        \n",
    "        # Operating hours distribution\n",
    "        operating_hours = sensor_data_copy['hour'].value_counts().sort_index()\n",
    "        \n",
    "        # Peak operating hours\n",
    "        if len(operating_hours) > 0:\n",
    "            features['peak_operating_hour'] = operating_hours.idxmax()\n",
    "            features['operating_hour_spread'] = operating_hours.max() - operating_hours.min()\n",
    "    \n",
    "    # 4. MULTI-SENSOR OPERATIONAL PATTERNS\n",
    "    # Correlation between load and speed (operational efficiency)\n",
    "    if 'engineLoadPercent' in sensor_data.columns and 'ecuSpeedMph' in sensor_data.columns:\n",
    "        load_speed_data = sensor_data[['engineLoadPercent', 'ecuSpeedMph']].dropna()\n",
    "        if len(load_speed_data) >= 10:\n",
    "            correlation = load_speed_data['engineLoadPercent'].corr(load_speed_data['ecuSpeedMph'])\n",
    "            features['load_speed_correlation'] = correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    # Engine efficiency patterns (RPM vs Load)\n",
    "    if 'engineRpm' in sensor_data.columns and 'engineLoadPercent' in sensor_data.columns:\n",
    "        rpm_load_data = sensor_data[['engineRpm', 'engineLoadPercent']].dropna()\n",
    "        if len(rpm_load_data) >= 10:\n",
    "            # Calculate efficiency proxy (load per RPM)\n",
    "            efficiency_data = rpm_load_data[rpm_load_data['engineRpm'] > 0].copy()\n",
    "            if len(efficiency_data) > 0:\n",
    "                efficiency_data['load_per_rpm'] = efficiency_data['engineLoadPercent'] / efficiency_data['engineRpm'] * 1000\n",
    "                features['avg_load_per_rpm'] = efficiency_data['load_per_rpm'].mean()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compare_operational_periods(sensor_data, split_ratio=0.7, time_col='time'):\n",
    "    \"\"\"\n",
    "    Compare operational patterns between historical (early) and recent periods.\n",
    "    This helps identify if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 20:  # Need enough data to split meaningfully\n",
    "        return {}\n",
    "    \n",
    "    # Sort by time and split\n",
    "    sorted_data = sensor_data.sort_values(time_col)\n",
    "    split_point = int(len(sorted_data) * split_ratio)\n",
    "    \n",
    "    historical_data = sorted_data.iloc[:split_point]\n",
    "    recent_data = sorted_data.iloc[split_point:]\n",
    "    \n",
    "    # Calculate operational features for both periods\n",
    "    historical_features = calculate_operational_pattern_features(historical_data, time_col)\n",
    "    recent_features = calculate_operational_pattern_features(recent_data, time_col)\n",
    "    \n",
    "    # Calculate changes\n",
    "    changes = {}\n",
    "    for feature in historical_features:\n",
    "        if feature in recent_features:\n",
    "            historical_value = historical_features[feature]\n",
    "            recent_value = recent_features[feature]\n",
    "            \n",
    "            if historical_value != 0:\n",
    "                percent_change = ((recent_value - historical_value) / abs(historical_value)) * 100\n",
    "                changes[f'{feature}_change_pct'] = percent_change\n",
    "    \n",
    "    return changes\n",
    "\n",
    "# Demonstrate operational pattern analysis\n",
    "print(\"🔄 Demonstrating Operational Pattern Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing operational patterns for Vehicle: {sample_vin}\")\n",
    "    print(f\"Data points: {len(sample_data)}\")\n",
    "    \n",
    "    # Calculate current operational features\n",
    "    operational_features = calculate_operational_pattern_features(sample_data)\n",
    "    \n",
    "    if operational_features:\n",
    "        print(f\"\\n📊 Current Operational Profile:\")\n",
    "        \n",
    "        # Duty cycle analysis\n",
    "        if 'avg_engine_load' in operational_features:\n",
    "            avg_load = operational_features['avg_engine_load']\n",
    "            print(f\"   🔧 Average Engine Load: {avg_load:.1f}%\")\n",
    "            \n",
    "            if avg_load > 60:\n",
    "                print(f\"      🚨 High duty cycle - may stress DPF\")\n",
    "            elif avg_load < 30:\n",
    "                print(f\"      ⚠️ Low duty cycle - may not regenerate DPF properly\")\n",
    "            else:\n",
    "                print(f\"      ✅ Moderate duty cycle\")\n",
    "        \n",
    "        # Drive profile analysis\n",
    "        if 'pct_time_highway_speed' in operational_features:\n",
    "            highway_pct = operational_features['pct_time_highway_speed']\n",
    "            city_pct = operational_features.get('pct_time_city_speed', 0)\n",
    "            \n",
    "            print(f\"   🛣️ Highway driving: {highway_pct:.1f}% of time\")\n",
    "            print(f\"   🏙️ City driving: {city_pct:.1f}% of time\")\n",
    "            \n",
    "            if highway_pct < 20:\n",
    "                print(f\"      🚨 Mostly city driving - poor for DPF regeneration\")\n",
    "            elif highway_pct > 60:\n",
    "                print(f\"      ✅ Good highway driving - helps DPF regeneration\")\n",
    "            else:\n",
    "                print(f\"      ⚠️ Mixed driving profile\")\n",
    "    \n",
    "    # Compare historical vs recent patterns\n",
    "    pattern_changes = compare_operational_periods(sample_data)\n",
    "    \n",
    "    if pattern_changes:\n",
    "        print(f\"\\n📈 Operational Changes (Recent vs Historical):\")\n",
    "        \n",
    "        significant_changes = {k: v for k, v in pattern_changes.items() if abs(v) > 10}\n",
    "        \n",
    "        if significant_changes:\n",
    "            for change_feature, percent_change in significant_changes.items():\n",
    "                base_feature = change_feature.replace('_change_pct', '')\n",
    "                direction = \"increased\" if percent_change > 0 else \"decreased\"\n",
    "                print(f\"   📊 {base_feature}: {direction} by {abs(percent_change):.1f}%\")\n",
    "                \n",
    "                # Interpret the change\n",
    "                if abs(percent_change) > 25:\n",
    "                    print(f\"      🚨 Significant operational change detected!\")\n",
    "                elif abs(percent_change) > 15:\n",
    "                    print(f\"      ⚠️ Notable operational change\")\n",
    "        else:\n",
    "            print(f\"   ✅ Stable operational patterns\")\n",
    "    \n",
    "    total_features = len(operational_features) + len(pattern_changes)\n",
    "    print(f\"\\n📊 Total operational features calculated: {total_features}\")\n",
    "else:\n",
    "    print(\"❌ No vehicle data available for operational pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1mnnzpb7wu4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Building Comprehensive Interpretable Feature Dataset\n",
      "============================================================\n",
      "🔧 Building comprehensive feature dataset...\n",
      "✅ Built feature dataset with 46 examples\n",
      "📊 Total interpretable features: 102\n",
      "   📈 Trend features: 20\n",
      "   🎯 Threshold features: 56\n",
      "   🔄 Operational features: 26\n",
      "\n",
      "🔧 Maintenance Type Distribution:\n",
      "   EXHAUST SYSTEM INSPECT DIAGNOSE: 26 examples\n",
      "   EXHAUST SYSTEM: 14 examples\n",
      "   FILTER - DIESEL PARTICULATE: 6 examples\n"
     ]
    }
   ],
   "source": [
    "def build_comprehensive_feature_dataset(maintenance_df, sensor_df, window_days=30):\n",
    "    \"\"\"\n",
    "    Build a comprehensive dataset with all our interpretable features.\n",
    "    This combines trend, threshold, and operational pattern features.\n",
    "    \"\"\"\n",
    "    print(f\"🔧 Building comprehensive feature dataset...\")\n",
    "    \n",
    "    feature_data = []\n",
    "    \n",
    "    for _, maintenance_row in maintenance_df.iterrows():\n",
    "        vin = maintenance_row['VIN Number']\n",
    "        maintenance_date = maintenance_row['Date of Issue']\n",
    "        \n",
    "        if pd.isna(vin) or pd.isna(maintenance_date):\n",
    "            continue\n",
    "        \n",
    "        # Get sensor data before maintenance\n",
    "        start_date = maintenance_date - timedelta(days=window_days)\n",
    "        \n",
    "        vehicle_data = sensor_df[\n",
    "            (sensor_df['vin'] == vin) &\n",
    "            (sensor_df['time'] >= start_date) &\n",
    "            (sensor_df['time'] < maintenance_date)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(vehicle_data) < 10:  # Need minimum data\n",
    "            continue\n",
    "        \n",
    "        # Calculate all feature types\n",
    "        all_features = {\n",
    "            'vin': vin,\n",
    "            'vehicle_number': maintenance_row['Vehicle_Number'],\n",
    "            'maintenance_date': maintenance_date,\n",
    "            'maintenance_type': maintenance_row['lines_jobDescriptions'],\n",
    "            'data_points': len(vehicle_data),\n",
    "            'window_days': window_days\n",
    "        }\n",
    "        \n",
    "        # 1. Trend features for each DPF sensor\n",
    "        for sensor in dpf_sensors.keys():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    trend_features = calculate_trend_features(sensor_subset, sensor)\n",
    "                    all_features.update(trend_features)\n",
    "        \n",
    "        # 2. Threshold features for each DPF sensor\n",
    "        for sensor, info in dpf_sensors.items():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    threshold_features = calculate_threshold_features(\n",
    "                        sensor_subset, sensor, info['alert_thresholds']\n",
    "                    )\n",
    "                    all_features.update(threshold_features)\n",
    "        \n",
    "        # 3. Operational pattern features\n",
    "        operational_features = calculate_operational_pattern_features(vehicle_data)\n",
    "        all_features.update(operational_features)\n",
    "        \n",
    "        # 4. Operational change features\n",
    "        pattern_changes = compare_operational_periods(vehicle_data)\n",
    "        all_features.update(pattern_changes)\n",
    "        \n",
    "        feature_data.append(all_features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "# Build the comprehensive feature dataset\n",
    "print(\"🔧 Building Comprehensive Interpretable Feature Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dataset = build_comprehensive_feature_dataset(maintenance_df, sensor_df)\n",
    "\n",
    "if len(feature_dataset) > 0:\n",
    "    print(f\"✅ Built feature dataset with {len(feature_dataset)} examples\")\n",
    "    \n",
    "    # Analyze feature coverage\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"📊 Total interpretable features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation', 'excess', 'deficit'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   📈 Trend features: {len(trend_features)}\")\n",
    "    print(f\"   🎯 Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   🔄 Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    # Show maintenance type distribution\n",
    "    print(f\"\\n🔧 Maintenance Type Distribution:\")\n",
    "    for maint_type, count in feature_dataset['maintenance_type'].value_counts().items():\n",
    "        print(f\"   {maint_type}: {count} examples\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Could not build feature dataset - insufficient data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "di0gmgaqohe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Validating Interpretable Features\n",
      "========================================\n",
      "📊 Data Quality Analysis:\n",
      "   ⚠️ Features with >50% missing data: 1\n",
      "      - ecuSpeedMph_avg_excess_above_high: 76.1% missing\n",
      "\n",
      "🔍 Feature Discriminative Power (by Maintenance Type):\n",
      "   🏆 Top 10 Most Discriminative Features:\n",
      "      1. 📈 Trend - engineCoolantTemperatureMilliC_trend_slope_per_day: CV=24.380\n",
      "      2. 🎯 Threshold - pct_time_city_speed_change_pct: CV=20.160\n",
      "      3. 📈 Trend - defLevelMilliPercent_trend_slope_per_day: CV=17.688\n",
      "      4. 🔄 Operational - load_speed_correlation_change_pct: CV=16.439\n",
      "      5. 🔄 Operational - avg_engine_load_change_pct: CV=13.777\n",
      "      6. 🎯 Threshold - pct_time_medium_load_change_pct: CV=13.562\n",
      "      7. 📈 Trend - engineRpm_trend_slope_per_day: CV=4.648\n",
      "      8. 🎯 Threshold - pct_time_stopped_change_pct: CV=4.272\n",
      "      9. 🔄 Operational - ecuSpeedMph_recent_vs_historical_pct: CV=3.999\n",
      "      10. 📈 Trend - ecuSpeedMph_trend_slope_per_day: CV=2.841\n",
      "\n",
      "💡 Feature Interpretability Assessment:\n",
      "   🌟 Most Interpretable Features (Score 8-10):\n",
      "      • engineLoadPercent_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      • engineRpm_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      • defLevelMilliPercent_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      • engineCoolantTemperatureMilliC_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      • ecuSpeedMph_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      • engineLoadPercent_pct_time_above_high (Score: 9/10) - Percentage time outside thresholds\n",
      "      • engineLoadPercent_pct_time_below_low (Score: 9/10) - Percentage time outside thresholds\n",
      "      • engineLoadPercent_pct_time_outside_normal (Score: 9/10) - Percentage time outside thresholds\n",
      "      • engineRpm_pct_time_above_high (Score: 9/10) - Percentage time outside thresholds\n",
      "      • engineRpm_pct_time_below_low (Score: 9/10) - Percentage time outside thresholds\n",
      "\n",
      "🎯 Feature Recommendations for Fleet Managers:\n",
      "   🏆 Top 5 Recommended Features (High interpretability + High predictive power):\n",
      "      1. engineCoolantTemperatureMilliC_trend_slope_per_day\n",
      "         Interpretability: 10/10 - Clear direction and rate of change\n",
      "         Combined Score: 24.380\n",
      "\n",
      "      2. pct_time_city_speed_change_pct\n",
      "         Interpretability: 9/10 - Percentage time outside thresholds\n",
      "         Combined Score: 18.144\n",
      "\n",
      "      3. defLevelMilliPercent_trend_slope_per_day\n",
      "         Interpretability: 10/10 - Clear direction and rate of change\n",
      "         Combined Score: 17.688\n",
      "\n",
      "      4. pct_time_medium_load_change_pct\n",
      "         Interpretability: 9/10 - Percentage time outside thresholds\n",
      "         Combined Score: 12.206\n",
      "\n",
      "      5. load_speed_correlation_change_pct\n",
      "         Interpretability: 7/10 - Percentage change over time\n",
      "         Combined Score: 11.508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate feature quality and predictive power\n",
    "def validate_interpretable_features(feature_dataset):\n",
    "    \"\"\"\n",
    "    Validate the quality and predictive power of our interpretable features.\n",
    "    \"\"\"\n",
    "    if len(feature_dataset) == 0:\n",
    "        return\n",
    "    \n",
    "    print(\"🧪 Validating Interpretable Features\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"❌ No features to validate\")\n",
    "        return\n",
    "    \n",
    "    # 1. DATA QUALITY VALIDATION\n",
    "    print(\"📊 Data Quality Analysis:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_rates = feature_dataset[feature_cols].isnull().mean() * 100\n",
    "    high_missing = missing_rates[missing_rates > 50]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   ⚠️ Features with >50% missing data: {len(high_missing)}\")\n",
    "        for feature, missing_pct in high_missing.head(5).items():\n",
    "            print(f\"      - {feature}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(f\"   ✅ Good data coverage - no features with >50% missing data\")\n",
    "    \n",
    "    # 2. FEATURE DISCRIMINATIVE POWER\n",
    "    print(f\"\\n🔍 Feature Discriminative Power (by Maintenance Type):\")\n",
    "    \n",
    "    # For each maintenance type, find features that discriminate well\n",
    "    maintenance_types = feature_dataset['maintenance_type'].unique()\n",
    "    \n",
    "    if len(maintenance_types) > 1:\n",
    "        # Calculate feature means by maintenance type\n",
    "        type_means = feature_dataset.groupby('maintenance_type')[feature_cols].mean()\n",
    "        \n",
    "        # Calculate coefficient of variation across maintenance types for each feature\n",
    "        discriminative_power = {}\n",
    "        for feature in feature_cols:\n",
    "            values = type_means[feature].dropna()\n",
    "            if len(values) > 1 and values.std() > 0:\n",
    "                cv = values.std() / abs(values.mean()) if values.mean() != 0 else 0\n",
    "                discriminative_power[feature] = cv\n",
    "        \n",
    "        # Show top discriminative features\n",
    "        if discriminative_power:\n",
    "            top_discriminative = sorted(discriminative_power.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"   🏆 Top 10 Most Discriminative Features:\")\n",
    "            for i, (feature, cv) in enumerate(top_discriminative, 1):\n",
    "                # Determine feature type\n",
    "                if 'trend' in feature:\n",
    "                    feature_type = \"📈 Trend\"\n",
    "                elif any(x in feature for x in ['pct_time', 'streak', 'violation']):\n",
    "                    feature_type = \"🎯 Threshold\"\n",
    "                else:\n",
    "                    feature_type = \"🔄 Operational\"\n",
    "                \n",
    "                print(f\"      {i}. {feature_type} - {feature}: CV={cv:.3f}\")\n",
    "    \n",
    "    # 3. FEATURE INTERPRETABILITY ASSESSMENT\n",
    "    print(f\"\\n💡 Feature Interpretability Assessment:\")\n",
    "    \n",
    "    interpretability_scores = {\n",
    "        'trend_slope': {'score': 10, 'explanation': 'Clear direction and rate of change'},\n",
    "        'pct_time': {'score': 9, 'explanation': 'Percentage time outside thresholds'},\n",
    "        'avg_': {'score': 8, 'explanation': 'Simple average values'},\n",
    "        'max_streak': {'score': 8, 'explanation': 'Consecutive violations count'},\n",
    "        'change_pct': {'score': 7, 'explanation': 'Percentage change over time'},\n",
    "        'correlation': {'score': 6, 'explanation': 'Relationship between sensors'},\n",
    "        'volatility': {'score': 5, 'explanation': 'Stability measure'}\n",
    "    }\n",
    "    \n",
    "    # Score each feature\n",
    "    feature_scores = []\n",
    "    for feature in feature_cols:\n",
    "        score = 3  # Default score\n",
    "        explanation = \"General metric\"\n",
    "        \n",
    "        for pattern, info in interpretability_scores.items():\n",
    "            if pattern in feature:\n",
    "                score = info['score']\n",
    "                explanation = info['explanation']\n",
    "                break\n",
    "        \n",
    "        feature_scores.append((feature, score, explanation))\n",
    "    \n",
    "    # Show most interpretable features\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   🌟 Most Interpretable Features (Score 8-10):\")\n",
    "    high_interpretability = [f for f in feature_scores if f[1] >= 8]\n",
    "    \n",
    "    for feature, score, explanation in high_interpretability[:10]:\n",
    "        print(f\"      • {feature} (Score: {score}/10) - {explanation}\")\n",
    "    \n",
    "    # 4. FEATURE RECOMMENDATION\n",
    "    print(f\"\\n🎯 Feature Recommendations for Fleet Managers:\")\n",
    "    \n",
    "    # Combine discriminative power and interpretability\n",
    "    if discriminative_power:\n",
    "        recommended_features = []\n",
    "        \n",
    "        for feature, interp_score, explanation in feature_scores:\n",
    "            if feature in discriminative_power and interp_score >= 7:\n",
    "                combined_score = discriminative_power[feature] * (interp_score / 10)\n",
    "                recommended_features.append((feature, combined_score, interp_score, explanation))\n",
    "        \n",
    "        recommended_features.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"   🏆 Top 5 Recommended Features (High interpretability + High predictive power):\")\n",
    "        for i, (feature, combined_score, interp_score, explanation) in enumerate(recommended_features[:5], 1):\n",
    "            print(f\"      {i}. {feature}\")\n",
    "            print(f\"         Interpretability: {interp_score}/10 - {explanation}\")\n",
    "            print(f\"         Combined Score: {combined_score:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Run feature validation\n",
    "if len(feature_dataset) > 0:\n",
    "    validate_interpretable_features(feature_dataset)\n",
    "else:\n",
    "    print(\"❌ No feature dataset available for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dqfofv8qu5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 FEATURE ENGINEERING COMPLETE!\n",
      "==================================================\n",
      "✅ Saved interpretable features to: ../data/interpretable_features_dataset.csv\n",
      "📊 Dataset contains: 46 examples\n",
      "🔧 Total features: 102\n",
      "   📈 Trend features: 20\n",
      "   🎯 Threshold features: 47\n",
      "   🔄 Operational features: 35\n",
      "\n",
      "🎉 Ready for model building and deployment!\n",
      "Next step: Use these features in the explainable model notebook\n",
      "\n",
      "📚 What You've Learned:\n",
      "• How to create trend features that show sensor degradation patterns\n",
      "• How to build threshold features for actionable alerts\n",
      "• How to detect operational pattern changes affecting DPF health\n",
      "• How to validate feature quality and interpretability\n",
      "• Best practices for explainable feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Save the interpretable feature dataset for use in other notebooks\n",
    "if len(feature_dataset) > 0:\n",
    "    output_file = '../data/interpretable_features_dataset.csv'\n",
    "    feature_dataset.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"💾 FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"✅ Saved interpretable features to: {output_file}\")\n",
    "    print(f\"📊 Dataset contains: {len(feature_dataset)} examples\")\n",
    "    \n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    print(f\"🔧 Total features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   📈 Trend features: {len(trend_features)}\")\n",
    "    print(f\"   🎯 Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   🔄 Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Ready for model building and deployment!\")\n",
    "    print(f\"Next step: Use these features in the explainable model notebook\")\n",
    "else:\n",
    "    print(\"❌ No interpretable features dataset created\")\n",
    "    print(\"Please check data availability and try again\")\n",
    "\n",
    "print(f\"\\n📚 What You've Learned:\")\n",
    "print(f\"• How to create trend features that show sensor degradation patterns\")\n",
    "print(f\"• How to build threshold features for actionable alerts\")\n",
    "print(f\"• How to detect operational pattern changes affecting DPF health\")\n",
    "print(f\"• How to validate feature quality and interpretability\")\n",
    "print(f\"• Best practices for explainable feature engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
