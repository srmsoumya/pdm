{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Feature Engineering for DPF Maintenance Prediction\n",
    "\n",
    "This notebook teaches you how to create **explainable features** from raw sensor data that fleet managers can understand and trust.\n",
    "\n",
    "## ğŸ¯ Why Interpretable Features Matter\n",
    "\n",
    "**Traditional Approach**: Complex features that work but can't be explained\n",
    "- Fourier transforms, wavelet coefficients, deep learning embeddings\n",
    "- Hard to explain why a vehicle needs maintenance\n",
    "- Fleet managers can't take actionable steps\n",
    "\n",
    "**Our Approach**: Simple, domain-driven features that tell a story\n",
    "- \"Engine load has been trending upward 5% per week\"\n",
    "- \"DEF level drops below 75% more than usual\"\n",
    "- \"Recent driving patterns 20% more aggressive than historical\"\n",
    "\n",
    "## ğŸ“š What You'll Learn\n",
    "\n",
    "1. **Trend Analysis**: How to detect meaningful changes in sensor patterns\n",
    "2. **Threshold Engineering**: Creating actionable alert boundaries\n",
    "3. **Pattern Recognition**: Identifying operational behavior changes\n",
    "4. **Domain Knowledge Integration**: Using fleet management expertise in features\n",
    "5. **Feature Validation**: Testing that features actually predict maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Ready to build interpretable features!\n",
      "ğŸ“š This notebook will teach you to create features that:\n",
      "   â€¢ Fleet managers can understand\n",
      "   â€¢ Maintenance teams can act upon\n",
      "   â€¢ Actually predict DPF issues\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ğŸš€ Ready to build interpretable features!\")\n",
    "print(\"ğŸ“š This notebook will teach you to create features that:\")\n",
    "print(\"   â€¢ Fleet managers can understand\")\n",
    "print(\"   â€¢ Maintenance teams can act upon\")\n",
    "print(\"   â€¢ Actually predict DPF issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 1: Understanding Raw Sensor Data\n",
    "\n",
    "Before we engineer features, let's understand what our raw sensor data looks like and what it tells us about DPF health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading DPF datasets...\n",
      "âœ… Loaded 82 maintenance events\n",
      "âœ… Loaded 4,466,272 sensor readings\n"
     ]
    }
   ],
   "source": [
    "# Load our datasets\n",
    "print(\"ğŸ“ Loading DPF datasets...\")\n",
    "\n",
    "try:\n",
    "    maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n",
    "    sensor_df = pd.read_csv('../data/dpf_vehicle_stats.csv')\n",
    "    \n",
    "    # Convert datetime columns\n",
    "    maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n",
    "    sensor_df['time'] = pd.to_datetime(sensor_df['time'])\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(maintenance_df)} maintenance events\")\n",
    "    print(f\"âœ… Loaded {len(sensor_df):,} sensor readings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Please run the data munging script first: uv run python 01_data_munging.py\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing DPF-Relevant Sensors\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Engine Load Percentage (engineLoadPercent)\n",
      "   ğŸ“ˆ Data Availability: 36.3% (1,619,223 readings)\n",
      "   ğŸ¯ DPF Relevance: High load generates more soot, stressing DPF\n",
      "   ğŸ“ Normal Range: (20, 60) %\n",
      "   ğŸ“Š Current Range: 1.0 - 125.0 %\n",
      "\n",
      "ğŸ“Š Engine RPM (engineRpm)\n",
      "   ğŸ“ˆ Data Availability: 42.3% (1,889,794 readings)\n",
      "   ğŸ¯ DPF Relevance: RPM affects DPF regeneration efficiency\n",
      "   ğŸ“ Normal Range: (800, 1800) RPM\n",
      "   ğŸ“Š Current Range: 26.0 - 2618.0 RPM\n",
      "\n",
      "ğŸ“Š DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent)\n",
      "   ğŸ“ˆ Data Availability: 45.9% (2,048,969 readings)\n",
      "   ğŸ¯ DPF Relevance: Low DEF prevents proper DPF operation\n",
      "   ğŸ“ Normal Range: (60000, 95000) milli-%\n",
      "   ğŸ“Š Current Range: 400.0 - 100000.0 milli-%\n",
      "\n",
      "ğŸ“Š Engine Coolant Temperature (engineCoolantTemperatureMilliC)\n",
      "   ğŸ“ˆ Data Availability: 33.5% (1,497,692 readings)\n",
      "   ğŸ¯ DPF Relevance: Temperature affects DPF regeneration cycles\n",
      "   ğŸ“ Normal Range: (75000, 90000) milli-Â°C\n",
      "   ğŸ“Š Current Range: 1000.0 - 123000.0 milli-Â°C\n",
      "\n",
      "ğŸ“Š Vehicle Speed (ecuSpeedMph)\n",
      "   ğŸ“ˆ Data Availability: 69.0% (3,079,837 readings)\n",
      "   ğŸ¯ DPF Relevance: Highway speeds help DPF regeneration\n",
      "   ğŸ“ Normal Range: (0, 65) mph\n",
      "   ğŸ“Š Current Range: 0.0 - 88.9 mph\n"
     ]
    }
   ],
   "source": [
    "# Identify DPF-relevant sensors and their characteristics\n",
    "print(\"ğŸ” Analyzing DPF-Relevant Sensors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define sensors most relevant to DPF health with their meanings\n",
    "dpf_sensors = {\n",
    "    'engineLoadPercent': {\n",
    "        'description': 'Engine Load Percentage',\n",
    "        'dpf_relevance': 'High load generates more soot, stressing DPF',\n",
    "        'normal_range': (20, 60),\n",
    "        'alert_thresholds': {'high': 80, 'low': 10},\n",
    "        'unit': '%'\n",
    "    },\n",
    "    'engineRpm': {\n",
    "        'description': 'Engine RPM',\n",
    "        'dpf_relevance': 'RPM affects DPF regeneration efficiency',\n",
    "        'normal_range': (800, 1800),\n",
    "        'alert_thresholds': {'high': 2000, 'low': 600},\n",
    "        'unit': 'RPM'\n",
    "    },\n",
    "    'defLevelMilliPercent': {\n",
    "        'description': 'DEF (Diesel Exhaust Fluid) Level',\n",
    "        'dpf_relevance': 'Low DEF prevents proper DPF operation',\n",
    "        'normal_range': (60000, 95000),  # 60-95%\n",
    "        'alert_thresholds': {'high': 95000, 'low': 50000},\n",
    "        'unit': 'milli-%'\n",
    "    },\n",
    "    'engineCoolantTemperatureMilliC': {\n",
    "        'description': 'Engine Coolant Temperature',\n",
    "        'dpf_relevance': 'Temperature affects DPF regeneration cycles',\n",
    "        'normal_range': (75000, 90000),  # 75-90Â°C\n",
    "        'alert_thresholds': {'high': 95000, 'low': 65000},\n",
    "        'unit': 'milli-Â°C'\n",
    "    },\n",
    "    'ecuSpeedMph': {\n",
    "        'description': 'Vehicle Speed',\n",
    "        'dpf_relevance': 'Highway speeds help DPF regeneration',\n",
    "        'normal_range': (0, 65),\n",
    "        'alert_thresholds': {'high': 80, 'low': 0},\n",
    "        'unit': 'mph'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze data availability for each sensor\n",
    "for sensor, info in dpf_sensors.items():\n",
    "    if sensor in sensor_df.columns:\n",
    "        non_null = sensor_df[sensor].notna().sum()\n",
    "        total = len(sensor_df)\n",
    "        availability = (non_null / total) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“Š {info['description']} ({sensor})\")\n",
    "        print(f\"   ğŸ“ˆ Data Availability: {availability:.1f}% ({non_null:,} readings)\")\n",
    "        print(f\"   ğŸ¯ DPF Relevance: {info['dpf_relevance']}\")\n",
    "        print(f\"   ğŸ“ Normal Range: {info['normal_range']} {info['unit']}\")\n",
    "        \n",
    "        if non_null > 0:\n",
    "            values = sensor_df[sensor].dropna()\n",
    "            print(f\"   ğŸ“Š Current Range: {values.min():.1f} - {values.max():.1f} {info['unit']}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ {info['description']} ({sensor}) - No data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 2: Trend Analysis Features\n",
    "\n",
    "**Trend features** help us understand if a sensor is getting worse over time. These are highly interpretable:\n",
    "- **Positive slope**: \"Engine load is increasing 2% per week\"\n",
    "- **Negative slope**: \"DEF level is dropping 5% per week\"\n",
    "- **Flat trend**: \"Temperature has been stable\"\n",
    "\n",
    "### Why This Works for Fleet Managers:\n",
    "- Easy to visualize and understand\n",
    "- Actionable: know which direction things are heading\n",
    "- Predictive: trends often continue into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Demonstrating Trend Analysis\n",
      "========================================\n",
      "Sample Vehicle: 1XK1D40X1NJ495537\n",
      "Data Points: 269964\n",
      "Date Range: 2024-06-08 00:03:00+00:00 to 2025-05-21 20:40:00+00:00\n",
      "\n",
      "ğŸ“ˆ Engine Load Percentage:\n",
      "   Trend: increasing by 0.013 %/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   âœ… Normal: Stable readings\n",
      "\n",
      "ğŸ“ˆ Engine RPM:\n",
      "   Trend: increasing by 0.111 RPM/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate increasing trend\n",
      "\n",
      "ğŸ“ˆ DEF (Diesel Exhaust Fluid) Level:\n",
      "   Trend: decreasing by 61.047 milli-%/day\n",
      "   Strength: 0.072 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate decreasing trend\n",
      "\n",
      "ğŸ“ˆ Engine Coolant Temperature:\n",
      "   Trend: decreasing by 9.565 milli-Â°C/day\n",
      "   Strength: 0.005 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate decreasing trend\n",
      "\n",
      "ğŸ“ˆ Vehicle Speed:\n",
      "   Trend: decreasing by 0.003 mph/day\n",
      "   Strength: 0.000 (0=weak, 1=strong linear trend)\n",
      "   âœ… Normal: Stable readings\n",
      "\n",
      "ğŸ“Š Total trend features calculated: 30\n"
     ]
    }
   ],
   "source": [
    "def calculate_trend_features(sensor_data, sensor_name, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate interpretable trend features for a sensor.\n",
    "    Returns features that fleet managers can understand and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 3:  # Need minimum points for trend\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 3:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    times = pd.to_numeric(data[time_col]) / 1e9  # Convert to seconds\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. LINEAR TREND (Most Important)\n",
    "    # Calculate slope in original units per day\n",
    "    slope_per_second, intercept = np.polyfit(times, values, 1)\n",
    "    slope_per_day = slope_per_second * 86400  # Convert to per day\n",
    "    \n",
    "    features[f'{sensor_name}_trend_slope_per_day'] = slope_per_day\n",
    "    \n",
    "    # 2. TREND STRENGTH (How reliable is the trend?)\n",
    "    # R-squared shows how well the linear trend fits\n",
    "    correlation = np.corrcoef(times, values)[0, 1]\n",
    "    r_squared = correlation ** 2 if not np.isnan(correlation) else 0\n",
    "    features[f'{sensor_name}_trend_strength'] = r_squared\n",
    "    \n",
    "    # 3. TREND SIGNIFICANCE (Is this trend meaningful?)\n",
    "    # Calculate statistical significance of the trend\n",
    "    if len(values) >= 3:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(times, values)\n",
    "        features[f'{sensor_name}_trend_p_value'] = p_value\n",
    "        features[f'{sensor_name}_trend_significant'] = 1 if p_value < 0.05 else 0\n",
    "    \n",
    "    # 4. RECENT VS HISTORICAL TREND\n",
    "    # Compare recent behavior to historical (last 25% vs first 75%)\n",
    "    split_point = int(len(values) * 0.75)\n",
    "    if split_point > 2 and split_point < len(values) - 2:\n",
    "        historical_mean = np.mean(values[:split_point])\n",
    "        recent_mean = np.mean(values[split_point:])\n",
    "        \n",
    "        if historical_mean != 0:\n",
    "            percent_change = ((recent_mean - historical_mean) / abs(historical_mean)) * 100\n",
    "            features[f'{sensor_name}_recent_vs_historical_pct'] = percent_change\n",
    "    \n",
    "    # 5. VOLATILITY TREND (Is stability changing?)\n",
    "    # Calculate if the sensor is becoming more or less stable\n",
    "    if len(values) >= 6:\n",
    "        mid_point = len(values) // 2\n",
    "        early_std = np.std(values[:mid_point])\n",
    "        late_std = np.std(values[mid_point:])\n",
    "        \n",
    "        if early_std > 0:\n",
    "            volatility_change = (late_std - early_std) / early_std\n",
    "            features[f'{sensor_name}_volatility_change'] = volatility_change\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Demonstrate trend analysis with a sample vehicle\n",
    "print(\"ğŸ” Demonstrating Trend Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get a vehicle with good data coverage\n",
    "vehicle_counts = sensor_df['vin'].value_counts()\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Sample Vehicle: {sample_vin}\")\n",
    "    print(f\"Data Points: {len(sample_data)}\")\n",
    "    print(f\"Date Range: {sample_data['time'].min()} to {sample_data['time'].max()}\")\n",
    "    \n",
    "    # Calculate trends for each DPF sensor\n",
    "    all_trends = {}\n",
    "    for sensor in dpf_sensors.keys():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                trends = calculate_trend_features(sensor_subset, sensor)\n",
    "                all_trends.update(trends)\n",
    "                \n",
    "                # Show human-readable interpretation\n",
    "                slope_key = f'{sensor}_trend_slope_per_day'\n",
    "                if slope_key in trends:\n",
    "                    slope = trends[slope_key]\n",
    "                    direction = \"increasing\" if slope > 0 else \"decreasing\"\n",
    "                    strength = trends.get(f'{sensor}_trend_strength', 0)\n",
    "                    \n",
    "                    print(f\"\\nğŸ“ˆ {dpf_sensors[sensor]['description']}:\")\n",
    "                    print(f\"   Trend: {direction} by {abs(slope):.3f} {dpf_sensors[sensor]['unit']}/day\")\n",
    "                    print(f\"   Strength: {strength:.3f} (0=weak, 1=strong linear trend)\")\n",
    "                    \n",
    "                    # Interpret for fleet manager\n",
    "                    if abs(slope) > 0.1 and strength > 0.3:\n",
    "                        print(f\"   ğŸš¨ Alert: Strong {direction} trend detected!\")\n",
    "                    elif abs(slope) > 0.05:\n",
    "                        print(f\"   âš ï¸ Caution: Moderate {direction} trend\")\n",
    "                    else:\n",
    "                        print(f\"   âœ… Normal: Stable readings\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total trend features calculated: {len(all_trends)}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for trend analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 3: Threshold-Based Features\n",
    "\n",
    "**Threshold features** tell us how often a sensor operates outside normal ranges. These are immediately actionable:\n",
    "- \"Engine runs above 80% load 25% of the time\" â†’ Reduce load\n",
    "- \"DEF level below 60% for 3 days\" â†’ Refill DEF\n",
    "- \"Temperature exceeds 95Â°C twice this week\" â†’ Check cooling system\n",
    "\n",
    "### Fleet Manager Benefits:\n",
    "- Clear operational boundaries\n",
    "- Direct maintenance actions\n",
    "- Easy to set up monitoring alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Demonstrating Threshold Analysis\n",
      "========================================\n",
      "Analyzing thresholds for Vehicle: 1XK1D40X1NJ495537\n",
      "\n",
      "ğŸ¯ Engine Load Percentage (engineLoadPercent):\n",
      "   â¬†ï¸ Time above 80 %: 24.8%\n",
      "      ğŸš¨ ALERT: Excessive high readings!\n",
      "   â¬‡ï¸ Time below 10 %: 6.4%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 16 consecutive readings\n",
      "\n",
      "ğŸ¯ Engine RPM (engineRpm):\n",
      "   â¬†ï¸ Time above 2000 RPM: 0.0%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 600 RPM: 4.8%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 1 consecutive readings\n",
      "\n",
      "ğŸ¯ DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent):\n",
      "   â¬†ï¸ Time above 95000 milli-%: 5.4%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 50000 milli-%: 28.5%\n",
      "      ğŸš¨ ALERT: Excessive low readings!\n",
      "   ğŸ“Š Longest high streak: 101 consecutive readings\n",
      "\n",
      "ğŸ¯ Engine Coolant Temperature (engineCoolantTemperatureMilliC):\n",
      "   â¬†ï¸ Time above 95000 milli-Â°C: 17.6%\n",
      "      âš ï¸ WARNING: Frequent high readings\n",
      "   â¬‡ï¸ Time below 65000 milli-Â°C: 4.2%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 35 consecutive readings\n",
      "\n",
      "ğŸ¯ Vehicle Speed (ecuSpeedMph):\n",
      "   â¬†ï¸ Time above 80 mph: 9.7%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 0 mph: 0.0%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 18 consecutive readings\n",
      "\n",
      "ğŸ“Š Total threshold features calculated: 44\n"
     ]
    }
   ],
   "source": [
    "def calculate_threshold_features(sensor_data, sensor_name, thresholds, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate threshold-based features that are easy to interpret and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    features = {}\n",
    "    \n",
    "    # 1. PERCENTAGE OF TIME OUTSIDE THRESHOLDS\n",
    "    \n",
    "    # Time above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        high_thresh = thresholds['high']\n",
    "        pct_above_high = (values > high_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_above_high'] = pct_above_high\n",
    "    \n",
    "    # Time below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        low_thresh = thresholds['low']\n",
    "        pct_below_low = (values < low_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_below_low'] = pct_below_low\n",
    "    \n",
    "    # Total time outside normal range\n",
    "    if 'high' in thresholds and 'low' in thresholds:\n",
    "        outside_normal = ((values > thresholds['high']) | (values < thresholds['low'])).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_outside_normal'] = outside_normal\n",
    "    \n",
    "    # 2. CONSECUTIVE VIOLATIONS (Streak Analysis)\n",
    "    \n",
    "    # Longest streak above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        above_high = values > thresholds['high']\n",
    "        max_streak_high = calculate_max_consecutive(above_high)\n",
    "        features[f'{sensor_name}_max_streak_above_high'] = max_streak_high\n",
    "    \n",
    "    # Longest streak below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        below_low = values < thresholds['low']\n",
    "        max_streak_low = calculate_max_consecutive(below_low)\n",
    "        features[f'{sensor_name}_max_streak_below_low'] = max_streak_low\n",
    "    \n",
    "    # 3. FREQUENCY OF VIOLATIONS\n",
    "    \n",
    "    # How many separate incidents of threshold violations?\n",
    "    if 'high' in thresholds:\n",
    "        high_violations = count_violation_episodes(values > thresholds['high'])\n",
    "        features[f'{sensor_name}_high_violation_episodes'] = high_violations\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations = count_violation_episodes(values < thresholds['low'])\n",
    "        features[f'{sensor_name}_low_violation_episodes'] = low_violations\n",
    "    \n",
    "    # 4. SEVERITY OF VIOLATIONS\n",
    "    \n",
    "    # Average severity when violations occur\n",
    "    if 'high' in thresholds:\n",
    "        high_violations_mask = values > thresholds['high']\n",
    "        if high_violations_mask.any():\n",
    "            avg_excess = (values[high_violations_mask] - thresholds['high']).mean()\n",
    "            features[f'{sensor_name}_avg_excess_above_high'] = avg_excess\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations_mask = values < thresholds['low']\n",
    "        if low_violations_mask.any():\n",
    "            avg_deficit = (thresholds['low'] - values[low_violations_mask]).mean()\n",
    "            features[f'{sensor_name}_avg_deficit_below_low'] = avg_deficit\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_max_consecutive(boolean_array):\n",
    "    \"\"\"Calculate maximum consecutive True values in a boolean array.\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    \n",
    "    return max_streak\n",
    "\n",
    "def count_violation_episodes(boolean_array):\n",
    "    \"\"\"Count number of separate violation episodes (groups of consecutive True values).\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    episodes = 0\n",
    "    in_violation = False\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value and not in_violation:\n",
    "            episodes += 1\n",
    "            in_violation = True\n",
    "        elif not value:\n",
    "            in_violation = False\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Demonstrate threshold analysis\n",
    "print(\"ğŸ¯ Demonstrating Threshold Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing thresholds for Vehicle: {sample_vin}\")\n",
    "    \n",
    "    # Calculate threshold features for each sensor\n",
    "    all_threshold_features = {}\n",
    "    \n",
    "    for sensor, info in dpf_sensors.items():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                thresholds = info['alert_thresholds']\n",
    "                threshold_features = calculate_threshold_features(sensor_subset, sensor, thresholds)\n",
    "                all_threshold_features.update(threshold_features)\n",
    "                \n",
    "                print(f\"\\nğŸ¯ {info['description']} ({sensor}):\")\n",
    "                \n",
    "                # Show key threshold metrics\n",
    "                pct_high_key = f'{sensor}_pct_time_above_high'\n",
    "                pct_low_key = f'{sensor}_pct_time_below_low'\n",
    "                \n",
    "                if pct_high_key in threshold_features:\n",
    "                    pct_high = threshold_features[pct_high_key]\n",
    "                    print(f\"   â¬†ï¸ Time above {thresholds['high']} {info['unit']}: {pct_high:.1f}%\")\n",
    "                    \n",
    "                    if pct_high > 20:\n",
    "                        print(f\"      ğŸš¨ ALERT: Excessive high readings!\")\n",
    "                    elif pct_high > 10:\n",
    "                        print(f\"      âš ï¸ WARNING: Frequent high readings\")\n",
    "                    else:\n",
    "                        print(f\"      âœ… Normal high threshold behavior\")\n",
    "                \n",
    "                if pct_low_key in threshold_features:\n",
    "                    pct_low = threshold_features[pct_low_key]\n",
    "                    print(f\"   â¬‡ï¸ Time below {thresholds['low']} {info['unit']}: {pct_low:.1f}%\")\n",
    "                    \n",
    "                    if pct_low > 20:\n",
    "                        print(f\"      ğŸš¨ ALERT: Excessive low readings!\")\n",
    "                    elif pct_low > 10:\n",
    "                        print(f\"      âš ï¸ WARNING: Frequent low readings\")\n",
    "                    else:\n",
    "                        print(f\"      âœ… Normal low threshold behavior\")\n",
    "                \n",
    "                # Show streak information\n",
    "                streak_high_key = f'{sensor}_max_streak_above_high'\n",
    "                if streak_high_key in threshold_features:\n",
    "                    streak = threshold_features[streak_high_key]\n",
    "                    print(f\"   ğŸ“Š Longest high streak: {streak} consecutive readings\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total threshold features calculated: {len(all_threshold_features)}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for threshold analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 4: Operational Pattern Features\n",
    "\n",
    "**Pattern features** capture changes in how the vehicle is being operated. These help identify external factors affecting DPF health:\n",
    "\n",
    "- **Duty Cycle Changes**: \"Vehicle now works 30% harder than usual\"\n",
    "- **Drive Profile Shifts**: \"More city driving, less highway (bad for DPF)\"\n",
    "- **Load Distribution**: \"Consistently heavy loads vs mixed operation\"\n",
    "\n",
    "### Why This Matters:\n",
    "- Operational changes often precede maintenance needs\n",
    "- Helps identify root causes (driver behavior, route changes)\n",
    "- Enables proactive operational adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Demonstrating Operational Pattern Analysis\n",
      "==================================================\n",
      "Analyzing operational patterns for Vehicle: 1XK1D40X1NJ495537\n",
      "Data points: 269964\n",
      "\n",
      "ğŸ“Š Current Operational Profile:\n",
      "   ğŸ”§ Average Engine Load: 51.1%\n",
      "      âœ… Moderate duty cycle\n",
      "   ğŸ›£ï¸ Highway driving: 73.6% of time\n",
      "   ğŸ™ï¸ City driving: 17.2% of time\n",
      "      âœ… Good highway driving - helps DPF regeneration\n",
      "\n",
      "ğŸ“ˆ Operational Changes (Recent vs Historical):\n",
      "   ğŸ“Š peak_operating_hour: increased by 16.7%\n",
      "      âš ï¸ Notable operational change\n",
      "   ğŸ“Š operating_hour_spread: decreased by 57.4%\n",
      "      ğŸš¨ Significant operational change detected!\n",
      "\n",
      "ğŸ“Š Total operational features calculated: 28\n"
     ]
    }
   ],
   "source": [
    "def calculate_operational_pattern_features(sensor_data, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate operational pattern features that help identify changes in vehicle usage.\n",
    "    These features help fleet managers understand if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 10:  # Need reasonable amount of data for patterns\n",
    "        return {}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. DUTY CYCLE ANALYSIS (How hard is the vehicle working?)\n",
    "    if 'engineLoadPercent' in sensor_data.columns:\n",
    "        load_data = sensor_data['engineLoadPercent'].dropna()\n",
    "        if len(load_data) >= 10:\n",
    "            \n",
    "            # Average duty cycle\n",
    "            avg_load = load_data.mean()\n",
    "            features['avg_engine_load'] = avg_load\n",
    "            \n",
    "            # Load distribution analysis\n",
    "            features['pct_time_high_load'] = (load_data > 70).mean() * 100  # >70% load\n",
    "            features['pct_time_medium_load'] = ((load_data >= 40) & (load_data <= 70)).mean() * 100\n",
    "            features['pct_time_low_load'] = (load_data < 40).mean() * 100  # <40% load\n",
    "            \n",
    "            # Load consistency (coefficient of variation)\n",
    "            if avg_load > 0:\n",
    "                features['load_consistency'] = load_data.std() / avg_load\n",
    "    \n",
    "    # 2. DRIVE PROFILE ANALYSIS (City vs Highway driving)\n",
    "    if 'ecuSpeedMph' in sensor_data.columns:\n",
    "        speed_data = sensor_data['ecuSpeedMph'].dropna()\n",
    "        if len(speed_data) >= 10:\n",
    "            \n",
    "            # Speed distribution (proxy for drive cycle)\n",
    "            features['pct_time_highway_speed'] = (speed_data > 45).mean() * 100  # >45 mph = highway\n",
    "            features['pct_time_city_speed'] = ((speed_data > 0) & (speed_data <= 35)).mean() * 100  # 0-35 mph = city\n",
    "            features['pct_time_stopped'] = (speed_data == 0).mean() * 100  # Stopped\n",
    "            \n",
    "            # Average operating speed\n",
    "            features['avg_operating_speed'] = speed_data[speed_data > 0].mean() if (speed_data > 0).any() else 0\n",
    "            \n",
    "            # Speed variability (smooth vs stop-and-go driving)\n",
    "            features['speed_variability'] = speed_data.std()\n",
    "    \n",
    "    # 3. OPERATIONAL TIMING PATTERNS\n",
    "    if time_col in sensor_data.columns:\n",
    "        # Add hour of day for operational pattern analysis\n",
    "        sensor_data_copy = sensor_data.copy()\n",
    "        sensor_data_copy['hour'] = pd.to_datetime(sensor_data_copy[time_col]).dt.hour\n",
    "        \n",
    "        # Operating hours distribution\n",
    "        operating_hours = sensor_data_copy['hour'].value_counts().sort_index()\n",
    "        \n",
    "        # Peak operating hours\n",
    "        if len(operating_hours) > 0:\n",
    "            features['peak_operating_hour'] = operating_hours.idxmax()\n",
    "            features['operating_hour_spread'] = operating_hours.max() - operating_hours.min()\n",
    "    \n",
    "    # 4. MULTI-SENSOR OPERATIONAL PATTERNS\n",
    "    # Correlation between load and speed (operational efficiency)\n",
    "    if 'engineLoadPercent' in sensor_data.columns and 'ecuSpeedMph' in sensor_data.columns:\n",
    "        load_speed_data = sensor_data[['engineLoadPercent', 'ecuSpeedMph']].dropna()\n",
    "        if len(load_speed_data) >= 10:\n",
    "            correlation = load_speed_data['engineLoadPercent'].corr(load_speed_data['ecuSpeedMph'])\n",
    "            features['load_speed_correlation'] = correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    # Engine efficiency patterns (RPM vs Load)\n",
    "    if 'engineRpm' in sensor_data.columns and 'engineLoadPercent' in sensor_data.columns:\n",
    "        rpm_load_data = sensor_data[['engineRpm', 'engineLoadPercent']].dropna()\n",
    "        if len(rpm_load_data) >= 10:\n",
    "            # Calculate efficiency proxy (load per RPM)\n",
    "            efficiency_data = rpm_load_data[rpm_load_data['engineRpm'] > 0].copy()\n",
    "            if len(efficiency_data) > 0:\n",
    "                efficiency_data['load_per_rpm'] = efficiency_data['engineLoadPercent'] / efficiency_data['engineRpm'] * 1000\n",
    "                features['avg_load_per_rpm'] = efficiency_data['load_per_rpm'].mean()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compare_operational_periods(sensor_data, split_ratio=0.7, time_col='time'):\n",
    "    \"\"\"\n",
    "    Compare operational patterns between historical (early) and recent periods.\n",
    "    This helps identify if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 20:  # Need enough data to split meaningfully\n",
    "        return {}\n",
    "    \n",
    "    # Sort by time and split\n",
    "    sorted_data = sensor_data.sort_values(time_col)\n",
    "    split_point = int(len(sorted_data) * split_ratio)\n",
    "    \n",
    "    historical_data = sorted_data.iloc[:split_point]\n",
    "    recent_data = sorted_data.iloc[split_point:]\n",
    "    \n",
    "    # Calculate operational features for both periods\n",
    "    historical_features = calculate_operational_pattern_features(historical_data, time_col)\n",
    "    recent_features = calculate_operational_pattern_features(recent_data, time_col)\n",
    "    \n",
    "    # Calculate changes\n",
    "    changes = {}\n",
    "    for feature in historical_features:\n",
    "        if feature in recent_features:\n",
    "            historical_value = historical_features[feature]\n",
    "            recent_value = recent_features[feature]\n",
    "            \n",
    "            if historical_value != 0:\n",
    "                percent_change = ((recent_value - historical_value) / abs(historical_value)) * 100\n",
    "                changes[f'{feature}_change_pct'] = percent_change\n",
    "    \n",
    "    return changes\n",
    "\n",
    "# Demonstrate operational pattern analysis\n",
    "print(\"ğŸ”„ Demonstrating Operational Pattern Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing operational patterns for Vehicle: {sample_vin}\")\n",
    "    print(f\"Data points: {len(sample_data)}\")\n",
    "    \n",
    "    # Calculate current operational features\n",
    "    operational_features = calculate_operational_pattern_features(sample_data)\n",
    "    \n",
    "    if operational_features:\n",
    "        print(f\"\\nğŸ“Š Current Operational Profile:\")\n",
    "        \n",
    "        # Duty cycle analysis\n",
    "        if 'avg_engine_load' in operational_features:\n",
    "            avg_load = operational_features['avg_engine_load']\n",
    "            print(f\"   ğŸ”§ Average Engine Load: {avg_load:.1f}%\")\n",
    "            \n",
    "            if avg_load > 60:\n",
    "                print(f\"      ğŸš¨ High duty cycle - may stress DPF\")\n",
    "            elif avg_load < 30:\n",
    "                print(f\"      âš ï¸ Low duty cycle - may not regenerate DPF properly\")\n",
    "            else:\n",
    "                print(f\"      âœ… Moderate duty cycle\")\n",
    "        \n",
    "        # Drive profile analysis\n",
    "        if 'pct_time_highway_speed' in operational_features:\n",
    "            highway_pct = operational_features['pct_time_highway_speed']\n",
    "            city_pct = operational_features.get('pct_time_city_speed', 0)\n",
    "            \n",
    "            print(f\"   ğŸ›£ï¸ Highway driving: {highway_pct:.1f}% of time\")\n",
    "            print(f\"   ğŸ™ï¸ City driving: {city_pct:.1f}% of time\")\n",
    "            \n",
    "            if highway_pct < 20:\n",
    "                print(f\"      ğŸš¨ Mostly city driving - poor for DPF regeneration\")\n",
    "            elif highway_pct > 60:\n",
    "                print(f\"      âœ… Good highway driving - helps DPF regeneration\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ Mixed driving profile\")\n",
    "    \n",
    "    # Compare historical vs recent patterns\n",
    "    pattern_changes = compare_operational_periods(sample_data)\n",
    "    \n",
    "    if pattern_changes:\n",
    "        print(f\"\\nğŸ“ˆ Operational Changes (Recent vs Historical):\")\n",
    "        \n",
    "        significant_changes = {k: v for k, v in pattern_changes.items() if abs(v) > 10}\n",
    "        \n",
    "        if significant_changes:\n",
    "            for change_feature, percent_change in significant_changes.items():\n",
    "                base_feature = change_feature.replace('_change_pct', '')\n",
    "                direction = \"increased\" if percent_change > 0 else \"decreased\"\n",
    "                print(f\"   ğŸ“Š {base_feature}: {direction} by {abs(percent_change):.1f}%\")\n",
    "                \n",
    "                # Interpret the change\n",
    "                if abs(percent_change) > 25:\n",
    "                    print(f\"      ğŸš¨ Significant operational change detected!\")\n",
    "                elif abs(percent_change) > 15:\n",
    "                    print(f\"      âš ï¸ Notable operational change\")\n",
    "        else:\n",
    "            print(f\"   âœ… Stable operational patterns\")\n",
    "    \n",
    "    total_features = len(operational_features) + len(pattern_changes)\n",
    "    print(f\"\\nğŸ“Š Total operational features calculated: {total_features}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for operational pattern analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 5: Feature Validation and Selection\n",
    "\n",
    "Now let's validate that our interpretable features actually predict DPF maintenance needs. This step is crucial for building trust with fleet managers.\n",
    "\n",
    "### What Makes a Good Predictive Feature:\n",
    "1. **Statistically Significant**: Reliably different between healthy and failing vehicles\n",
    "2. **Practically Meaningful**: Changes that fleet managers can act upon\n",
    "3. **Stable**: Consistent behavior across different vehicles and time periods\n",
    "4. **Explainable**: Clear causal relationship to DPF health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Building Comprehensive Interpretable Feature Dataset\n",
      "============================================================\n",
      "ğŸ”§ Building comprehensive feature dataset...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns, UTC] and Timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:559\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:542\u001b[39m, in \u001b[36mDatetimeArray._check_compatible_with\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assert_tzawareness_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:788\u001b[39m, in \u001b[36mDatetimeArray._assert_tzawareness_compat\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m other_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot compare tz-naive and tz-aware datetime-like objects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    790\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Cannot compare tz-naive and tz-aware datetime-like objects",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInvalidComparison\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1006\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     other = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_comparison_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:562\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m         \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidComparison(other) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(other):\n",
      "\u001b[31mInvalidComparison\u001b[39m: 2023-05-13 00:00:00",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ”§ Building Comprehensive Interpretable Feature Dataset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m feature_dataset = \u001b[43mbuild_comprehensive_feature_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaintenance_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensor_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_dataset) > \u001b[32m0\u001b[39m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Built feature dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(feature_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mbuild_comprehensive_feature_dataset\u001b[39m\u001b[34m(maintenance_df, sensor_df, window_days)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get sensor data before maintenance\u001b[39;00m\n\u001b[32m     18\u001b[39m start_date = maintenance_date - timedelta(days=window_days)\n\u001b[32m     20\u001b[39m vehicle_data = sensor_df[\n\u001b[32m     21\u001b[39m     (sensor_df[\u001b[33m'\u001b[39m\u001b[33mvin\u001b[39m\u001b[33m'\u001b[39m] == vin) &\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     (\u001b[43msensor_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m) &\n\u001b[32m     23\u001b[39m     (sensor_df[\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m] < maintenance_date)\n\u001b[32m     24\u001b[39m ].copy()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vehicle_data) < \u001b[32m10\u001b[39m:  \u001b[38;5;66;03m# Need minimum data\u001b[39;00m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/series.py:6130\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6127\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6128\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6130\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:330\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLengths must match to compare\u001b[39m\u001b[33m\"\u001b[39m, lvalues.shape, rvalues.shape\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    326\u001b[39m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[32m    327\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m lvalues.dtype != \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    328\u001b[39m ):\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator.ne:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/arrays/datetimelike.py:1008\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1006\u001b[39m     other = \u001b[38;5;28mself\u001b[39m._validate_comparison_value(other)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[32m   1012\u001b[39m     \u001b[38;5;66;03m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m#  comparison otherwise it would raise when comparing to None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/ops/invalid.py:40\u001b[39m, in \u001b[36minvalid_comparison\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     typ = \u001b[38;5;28mtype\u001b[39m(right).\u001b[34m__name__\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[31mTypeError\u001b[39m: Invalid comparison between dtype=datetime64[ns, UTC] and Timestamp"
     ]
    }
   ],
   "source": [
    "def build_comprehensive_feature_dataset(maintenance_df, sensor_df, window_days=30):\n",
    "    \"\"\"\n",
    "    Build a comprehensive dataset with all our interpretable features.\n",
    "    This combines trend, threshold, and operational pattern features.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”§ Building comprehensive feature dataset...\")\n",
    "    \n",
    "    feature_data = []\n",
    "    \n",
    "    for _, maintenance_row in maintenance_df.iterrows():\n",
    "        vin = maintenance_row['VIN Number']\n",
    "        maintenance_date = maintenance_row['Date of Issue']\n",
    "        \n",
    "        if pd.isna(vin) or pd.isna(maintenance_date):\n",
    "            continue\n",
    "        \n",
    "        # Get sensor data before maintenance\n",
    "        start_date = maintenance_date - timedelta(days=window_days)\n",
    "        \n",
    "        vehicle_data = sensor_df[\n",
    "            (sensor_df['vin'] == vin) &\n",
    "            (sensor_df['time'] >= start_date) &\n",
    "            (sensor_df['time'] < maintenance_date)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(vehicle_data) < 10:  # Need minimum data\n",
    "            continue\n",
    "        \n",
    "        # Calculate all feature types\n",
    "        all_features = {\n",
    "            'vin': vin,\n",
    "            'vehicle_number': maintenance_row['Vehicle_Number'],\n",
    "            'maintenance_date': maintenance_date,\n",
    "            'maintenance_type': maintenance_row['lines_jobDescriptions'],\n",
    "            'data_points': len(vehicle_data),\n",
    "            'window_days': window_days\n",
    "        }\n",
    "        \n",
    "        # 1. Trend features for each DPF sensor\n",
    "        for sensor in dpf_sensors.keys():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    trend_features = calculate_trend_features(sensor_subset, sensor)\n",
    "                    all_features.update(trend_features)\n",
    "        \n",
    "        # 2. Threshold features for each DPF sensor\n",
    "        for sensor, info in dpf_sensors.items():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    threshold_features = calculate_threshold_features(\n",
    "                        sensor_subset, sensor, info['alert_thresholds']\n",
    "                    )\n",
    "                    all_features.update(threshold_features)\n",
    "        \n",
    "        # 3. Operational pattern features\n",
    "        operational_features = calculate_operational_pattern_features(vehicle_data)\n",
    "        all_features.update(operational_features)\n",
    "        \n",
    "        # 4. Operational change features\n",
    "        pattern_changes = compare_operational_periods(vehicle_data)\n",
    "        all_features.update(pattern_changes)\n",
    "        \n",
    "        feature_data.append(all_features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "# Build the comprehensive feature dataset\n",
    "print(\"ğŸ”§ Building Comprehensive Interpretable Feature Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dataset = build_comprehensive_feature_dataset(maintenance_df, sensor_df)\n",
    "\n",
    "if len(feature_dataset) > 0:\n",
    "    print(f\"âœ… Built feature dataset with {len(feature_dataset)} examples\")\n",
    "    \n",
    "    # Analyze feature coverage\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"ğŸ“Š Total interpretable features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation', 'excess', 'deficit'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Trend features: {len(trend_features)}\")\n",
    "    print(f\"   ğŸ¯ Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   ğŸ”„ Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    # Show maintenance type distribution\n",
    "    print(f\"\\nğŸ”§ Maintenance Type Distribution:\")\n",
    "    for maint_type, count in feature_dataset['maintenance_type'].value_counts().items():\n",
    "        print(f\"   {maint_type}: {count} examples\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Could not build feature dataset - insufficient data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate feature quality and predictive power\n",
    "def validate_interpretable_features(feature_dataset):\n",
    "    \"\"\"\n",
    "    Validate the quality and predictive power of our interpretable features.\n",
    "    \"\"\"\n",
    "    if len(feature_dataset) == 0:\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ§ª Validating Interpretable Features\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"âŒ No features to validate\")\n",
    "        return\n",
    "    \n",
    "    # 1. DATA QUALITY VALIDATION\n",
    "    print(\"ğŸ“Š Data Quality Analysis:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_rates = feature_dataset[feature_cols].isnull().mean() * 100\n",
    "    high_missing = missing_rates[missing_rates > 50]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   âš ï¸ Features with >50% missing data: {len(high_missing)}\")\n",
    "        for feature, missing_pct in high_missing.head(5).items():\n",
    "            print(f\"      - {feature}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(f\"   âœ… Good data coverage - no features with >50% missing data\")\n",
    "    \n",
    "    # 2. FEATURE DISCRIMINATIVE POWER\n",
    "    print(f\"\\nğŸ” Feature Discriminative Power (by Maintenance Type):\")\n",
    "    \n",
    "    # For each maintenance type, find features that discriminate well\n",
    "    maintenance_types = feature_dataset['maintenance_type'].unique()\n",
    "    \n",
    "    if len(maintenance_types) > 1:\n",
    "        # Calculate feature means by maintenance type\n",
    "        type_means = feature_dataset.groupby('maintenance_type')[feature_cols].mean()\n",
    "        \n",
    "        # Calculate coefficient of variation across maintenance types for each feature\n",
    "        discriminative_power = {}\n",
    "        for feature in feature_cols:\n",
    "            values = type_means[feature].dropna()\n",
    "            if len(values) > 1 and values.std() > 0:\n",
    "                cv = values.std() / abs(values.mean()) if values.mean() != 0 else 0\n",
    "                discriminative_power[feature] = cv\n",
    "        \n",
    "        # Show top discriminative features\n",
    "        if discriminative_power:\n",
    "            top_discriminative = sorted(discriminative_power.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"   ğŸ† Top 10 Most Discriminative Features:\")\n",
    "            for i, (feature, cv) in enumerate(top_discriminative, 1):\n",
    "                # Determine feature type\n",
    "                if 'trend' in feature:\n",
    "                    feature_type = \"ğŸ“ˆ Trend\"\n",
    "                elif any(x in feature for x in ['pct_time', 'streak', 'violation']):\n",
    "                    feature_type = \"ğŸ¯ Threshold\"\n",
    "                else:\n",
    "                    feature_type = \"ğŸ”„ Operational\"\n",
    "                \n",
    "                print(f\"      {i}. {feature_type} - {feature}: CV={cv:.3f}\")\n",
    "    \n",
    "    # 3. FEATURE INTERPRETABILITY ASSESSMENT\n",
    "    print(f\"\\nğŸ’¡ Feature Interpretability Assessment:\")\n",
    "    \n",
    "    interpretability_scores = {\n",
    "        'trend_slope': {'score': 10, 'explanation': 'Clear direction and rate of change'},\n",
    "        'pct_time': {'score': 9, 'explanation': 'Percentage time outside thresholds'},\n",
    "        'avg_': {'score': 8, 'explanation': 'Simple average values'},\n",
    "        'max_streak': {'score': 8, 'explanation': 'Consecutive violations count'},\n",
    "        'change_pct': {'score': 7, 'explanation': 'Percentage change over time'},\n",
    "        'correlation': {'score': 6, 'explanation': 'Relationship between sensors'},\n",
    "        'volatility': {'score': 5, 'explanation': 'Stability measure'}\n",
    "    }\n",
    "    \n",
    "    # Score each feature\n",
    "    feature_scores = []\n",
    "    for feature in feature_cols:\n",
    "        score = 3  # Default score\n",
    "        explanation = \"General metric\"\n",
    "        \n",
    "        for pattern, info in interpretability_scores.items():\n",
    "            if pattern in feature:\n",
    "                score = info['score']\n",
    "                explanation = info['explanation']\n",
    "                break\n",
    "        \n",
    "        feature_scores.append((feature, score, explanation))\n",
    "    \n",
    "    # Show most interpretable features\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   ğŸŒŸ Most Interpretable Features (Score 8-10):\")\n",
    "    high_interpretability = [f for f in feature_scores if f[1] >= 8]\n",
    "    \n",
    "    for feature, score, explanation in high_interpretability[:10]:\n",
    "        print(f\"      â€¢ {feature} (Score: {score}/10) - {explanation}\")\n",
    "    \n",
    "    # 4. FEATURE RECOMMENDATION\n",
    "    print(f\"\\nğŸ¯ Feature Recommendations for Fleet Managers:\")\n",
    "    \n",
    "    # Combine discriminative power and interpretability\n",
    "    if discriminative_power:\n",
    "        recommended_features = []\n",
    "        \n",
    "        for feature, interp_score, explanation in feature_scores:\n",
    "            if feature in discriminative_power and interp_score >= 7:\n",
    "                combined_score = discriminative_power[feature] * (interp_score / 10)\n",
    "                recommended_features.append((feature, combined_score, interp_score, explanation))\n",
    "        \n",
    "        recommended_features.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"   ğŸ† Top 5 Recommended Features (High interpretability + High predictive power):\")\n",
    "        for i, (feature, combined_score, interp_score, explanation) in enumerate(recommended_features[:5], 1):\n",
    "            print(f\"      {i}. {feature}\")\n",
    "            print(f\"         Interpretability: {interp_score}/10 - {explanation}\")\n",
    "            print(f\"         Combined Score: {combined_score:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Run feature validation\n",
    "if len(feature_dataset) > 0:\n",
    "    validate_interpretable_features(feature_dataset)\n",
    "else:\n",
    "    print(\"âŒ No feature dataset available for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Step 6: Feature Engineering Best Practices Summary\n",
    "\n",
    "### ğŸ¯ Key Principles for Interpretable Features:\n",
    "\n",
    "1. **Domain-Driven**: Every feature should relate to known DPF failure modes\n",
    "2. **Actionable**: Fleet managers should know what to do when a feature triggers\n",
    "3. **Explainable**: Each feature should tell a clear story about vehicle health\n",
    "4. **Validated**: Features must actually predict maintenance needs\n",
    "\n",
    "### ğŸ“Š Feature Categories We've Built:\n",
    "\n",
    "**Trend Features** (ğŸ“ˆ):\n",
    "- Sensor slopes (increasing/decreasing patterns)\n",
    "- Trend strength (how reliable the trend is)\n",
    "- Recent vs historical comparisons\n",
    "\n",
    "**Threshold Features** (ğŸ¯):\n",
    "- Time spent outside normal ranges\n",
    "- Consecutive violation streaks\n",
    "- Severity of threshold breaches\n",
    "\n",
    "**Operational Features** (ğŸ”„):\n",
    "- Duty cycle changes\n",
    "- Drive profile shifts (city vs highway)\n",
    "- Multi-sensor pattern correlations\n",
    "\n",
    "### ğŸš€ Implementation Roadmap:\n",
    "\n",
    "1. **Start Simple**: Begin with top 5 most interpretable features\n",
    "2. **Validate Continuously**: Track feature performance against actual maintenance\n",
    "3. **Refine Thresholds**: Adjust based on operational experience\n",
    "4. **Add Domain Knowledge**: Incorporate fleet manager insights into features\n",
    "5. **Scale Gradually**: Add more features as confidence builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the interpretable feature dataset for use in other notebooks\n",
    "if len(feature_dataset) > 0:\n",
    "    output_file = '../data/interpretable_features_dataset.csv'\n",
    "    feature_dataset.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"ğŸ’¾ FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"âœ… Saved interpretable features to: {output_file}\")\n",
    "    print(f\"ğŸ“Š Dataset contains: {len(feature_dataset)} examples\")\n",
    "    \n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    print(f\"ğŸ”§ Total features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Trend features: {len(trend_features)}\")\n",
    "    print(f\"   ğŸ¯ Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   ğŸ”„ Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Ready for model building and deployment!\")\n",
    "    print(f\"Next step: Use these features in the explainable model notebook\")\n",
    "else:\n",
    "    print(\"âŒ No interpretable features dataset created\")\n",
    "    print(\"Please check data availability and try again\")\n",
    "\n",
    "print(f\"\\nğŸ“š What You've Learned:\")\n",
    "print(f\"â€¢ How to create trend features that show sensor degradation patterns\")\n",
    "print(f\"â€¢ How to build threshold features for actionable alerts\")\n",
    "print(f\"â€¢ How to detect operational pattern changes affecting DPF health\")\n",
    "print(f\"â€¢ How to validate feature quality and interpretability\")\n",
    "print(f\"â€¢ Best practices for explainable feature engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dms3noi9r0g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Ready to build interpretable features!\n",
      "ğŸ“š This notebook will teach you to create features that:\n",
      "   â€¢ Fleet managers can understand\n",
      "   â€¢ Maintenance teams can act upon\n",
      "   â€¢ Actually predict DPF issues\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ğŸš€ Ready to build interpretable features!\")\n",
    "print(\"ğŸ“š This notebook will teach you to create features that:\")\n",
    "print(\"   â€¢ Fleet managers can understand\")\n",
    "print(\"   â€¢ Maintenance teams can act upon\")\n",
    "print(\"   â€¢ Actually predict DPF issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jai8mnjzpz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Loading DPF datasets...\n",
      "âœ… Loaded 82 maintenance events\n",
      "âœ… Loaded 4,466,272 sensor readings\n"
     ]
    }
   ],
   "source": [
    "# Load our datasets\n",
    "print(\"ğŸ“ Loading DPF datasets...\")\n",
    "\n",
    "try:\n",
    "    maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n",
    "    sensor_df = pd.read_csv('../data/dpf_vehicle_stats.csv')\n",
    "    \n",
    "    # Convert datetime columns and handle timezones\n",
    "    maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n",
    "    sensor_df['time'] = pd.to_datetime(sensor_df['time']).dt.tz_localize(None)  # Remove timezone info\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(maintenance_df)} maintenance events\")\n",
    "    print(f\"âœ… Loaded {len(sensor_df):,} sensor readings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Please run the data munging script first: uv run python 01_data_munging.py\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75t9lyn3w9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Analyzing DPF-Relevant Sensors\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Engine Load Percentage (engineLoadPercent)\n",
      "   ğŸ“ˆ Data Availability: 36.3% (1,619,223 readings)\n",
      "   ğŸ¯ DPF Relevance: High load generates more soot, stressing DPF\n",
      "   ğŸ“ Normal Range: (20, 60) %\n",
      "   ğŸ“Š Current Range: 1.0 - 125.0 %\n",
      "\n",
      "ğŸ“Š Engine RPM (engineRpm)\n",
      "   ğŸ“ˆ Data Availability: 42.3% (1,889,794 readings)\n",
      "   ğŸ¯ DPF Relevance: RPM affects DPF regeneration efficiency\n",
      "   ğŸ“ Normal Range: (800, 1800) RPM\n",
      "   ğŸ“Š Current Range: 26.0 - 2618.0 RPM\n",
      "\n",
      "ğŸ“Š DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent)\n",
      "   ğŸ“ˆ Data Availability: 45.9% (2,048,969 readings)\n",
      "   ğŸ¯ DPF Relevance: Low DEF prevents proper DPF operation\n",
      "   ğŸ“ Normal Range: (60000, 95000) milli-%\n",
      "   ğŸ“Š Current Range: 400.0 - 100000.0 milli-%\n",
      "\n",
      "ğŸ“Š Engine Coolant Temperature (engineCoolantTemperatureMilliC)\n",
      "   ğŸ“ˆ Data Availability: 33.5% (1,497,692 readings)\n",
      "   ğŸ¯ DPF Relevance: Temperature affects DPF regeneration cycles\n",
      "   ğŸ“ Normal Range: (75000, 90000) milli-Â°C\n",
      "   ğŸ“Š Current Range: 1000.0 - 123000.0 milli-Â°C\n",
      "\n",
      "ğŸ“Š Vehicle Speed (ecuSpeedMph)\n",
      "   ğŸ“ˆ Data Availability: 69.0% (3,079,837 readings)\n",
      "   ğŸ¯ DPF Relevance: Highway speeds help DPF regeneration\n",
      "   ğŸ“ Normal Range: (0, 65) mph\n",
      "   ğŸ“Š Current Range: 0.0 - 88.9 mph\n"
     ]
    }
   ],
   "source": [
    "# Identify DPF-relevant sensors and their characteristics\n",
    "print(\"ğŸ” Analyzing DPF-Relevant Sensors\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define sensors most relevant to DPF health with their meanings\n",
    "dpf_sensors = {\n",
    "    'engineLoadPercent': {\n",
    "        'description': 'Engine Load Percentage',\n",
    "        'dpf_relevance': 'High load generates more soot, stressing DPF',\n",
    "        'normal_range': (20, 60),\n",
    "        'alert_thresholds': {'high': 80, 'low': 10},\n",
    "        'unit': '%'\n",
    "    },\n",
    "    'engineRpm': {\n",
    "        'description': 'Engine RPM',\n",
    "        'dpf_relevance': 'RPM affects DPF regeneration efficiency',\n",
    "        'normal_range': (800, 1800),\n",
    "        'alert_thresholds': {'high': 2000, 'low': 600},\n",
    "        'unit': 'RPM'\n",
    "    },\n",
    "    'defLevelMilliPercent': {\n",
    "        'description': 'DEF (Diesel Exhaust Fluid) Level',\n",
    "        'dpf_relevance': 'Low DEF prevents proper DPF operation',\n",
    "        'normal_range': (60000, 95000),  # 60-95%\n",
    "        'alert_thresholds': {'high': 95000, 'low': 50000},\n",
    "        'unit': 'milli-%'\n",
    "    },\n",
    "    'engineCoolantTemperatureMilliC': {\n",
    "        'description': 'Engine Coolant Temperature',\n",
    "        'dpf_relevance': 'Temperature affects DPF regeneration cycles',\n",
    "        'normal_range': (75000, 90000),  # 75-90Â°C\n",
    "        'alert_thresholds': {'high': 95000, 'low': 65000},\n",
    "        'unit': 'milli-Â°C'\n",
    "    },\n",
    "    'ecuSpeedMph': {\n",
    "        'description': 'Vehicle Speed',\n",
    "        'dpf_relevance': 'Highway speeds help DPF regeneration',\n",
    "        'normal_range': (0, 65),\n",
    "        'alert_thresholds': {'high': 80, 'low': 0},\n",
    "        'unit': 'mph'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze data availability for each sensor\n",
    "for sensor, info in dpf_sensors.items():\n",
    "    if sensor in sensor_df.columns:\n",
    "        non_null = sensor_df[sensor].notna().sum()\n",
    "        total = len(sensor_df)\n",
    "        availability = (non_null / total) * 100\n",
    "        \n",
    "        print(f\"\\nğŸ“Š {info['description']} ({sensor})\")\n",
    "        print(f\"   ğŸ“ˆ Data Availability: {availability:.1f}% ({non_null:,} readings)\")\n",
    "        print(f\"   ğŸ¯ DPF Relevance: {info['dpf_relevance']}\")\n",
    "        print(f\"   ğŸ“ Normal Range: {info['normal_range']} {info['unit']}\")\n",
    "        \n",
    "        if non_null > 0:\n",
    "            values = sensor_df[sensor].dropna()\n",
    "            print(f\"   ğŸ“Š Current Range: {values.min():.1f} - {values.max():.1f} {info['unit']}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ {info['description']} ({sensor}) - No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1m6xief9fh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Demonstrating Trend Analysis\n",
      "========================================\n",
      "Sample Vehicle: 1XK1D40X1NJ495537\n",
      "Data Points: 269964\n",
      "Date Range: 2024-06-08 00:03:00 to 2025-05-21 20:40:00\n",
      "\n",
      "ğŸ“ˆ Engine Load Percentage:\n",
      "   Trend: increasing by 0.013 %/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   âœ… Normal: Stable readings\n",
      "\n",
      "ğŸ“ˆ Engine RPM:\n",
      "   Trend: increasing by 0.111 RPM/day\n",
      "   Strength: 0.001 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate increasing trend\n",
      "\n",
      "ğŸ“ˆ DEF (Diesel Exhaust Fluid) Level:\n",
      "   Trend: decreasing by 61.047 milli-%/day\n",
      "   Strength: 0.072 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate decreasing trend\n",
      "\n",
      "ğŸ“ˆ Engine Coolant Temperature:\n",
      "   Trend: decreasing by 9.565 milli-Â°C/day\n",
      "   Strength: 0.005 (0=weak, 1=strong linear trend)\n",
      "   âš ï¸ Caution: Moderate decreasing trend\n",
      "\n",
      "ğŸ“ˆ Vehicle Speed:\n",
      "   Trend: decreasing by 0.003 mph/day\n",
      "   Strength: 0.000 (0=weak, 1=strong linear trend)\n",
      "   âœ… Normal: Stable readings\n",
      "\n",
      "ğŸ“Š Total trend features calculated: 30\n"
     ]
    }
   ],
   "source": [
    "def calculate_trend_features(sensor_data, sensor_name, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate interpretable trend features for a sensor.\n",
    "    Returns features that fleet managers can understand and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 3:  # Need minimum points for trend\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 3:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    times = pd.to_numeric(data[time_col]) / 1e9  # Convert to seconds\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. LINEAR TREND (Most Important)\n",
    "    # Calculate slope in original units per day\n",
    "    slope_per_second, intercept = np.polyfit(times, values, 1)\n",
    "    slope_per_day = slope_per_second * 86400  # Convert to per day\n",
    "    \n",
    "    features[f'{sensor_name}_trend_slope_per_day'] = slope_per_day\n",
    "    \n",
    "    # 2. TREND STRENGTH (How reliable is the trend?)\n",
    "    # R-squared shows how well the linear trend fits\n",
    "    correlation = np.corrcoef(times, values)[0, 1]\n",
    "    r_squared = correlation ** 2 if not np.isnan(correlation) else 0\n",
    "    features[f'{sensor_name}_trend_strength'] = r_squared\n",
    "    \n",
    "    # 3. TREND SIGNIFICANCE (Is this trend meaningful?)\n",
    "    # Calculate statistical significance of the trend\n",
    "    if len(values) >= 3:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(times, values)\n",
    "        features[f'{sensor_name}_trend_p_value'] = p_value\n",
    "        features[f'{sensor_name}_trend_significant'] = 1 if p_value < 0.05 else 0\n",
    "    \n",
    "    # 4. RECENT VS HISTORICAL TREND\n",
    "    # Compare recent behavior to historical (last 25% vs first 75%)\n",
    "    split_point = int(len(values) * 0.75)\n",
    "    if split_point > 2 and split_point < len(values) - 2:\n",
    "        historical_mean = np.mean(values[:split_point])\n",
    "        recent_mean = np.mean(values[split_point:])\n",
    "        \n",
    "        if historical_mean != 0:\n",
    "            percent_change = ((recent_mean - historical_mean) / abs(historical_mean)) * 100\n",
    "            features[f'{sensor_name}_recent_vs_historical_pct'] = percent_change\n",
    "    \n",
    "    # 5. VOLATILITY TREND (Is stability changing?)\n",
    "    # Calculate if the sensor is becoming more or less stable\n",
    "    if len(values) >= 6:\n",
    "        mid_point = len(values) // 2\n",
    "        early_std = np.std(values[:mid_point])\n",
    "        late_std = np.std(values[mid_point:])\n",
    "        \n",
    "        if early_std > 0:\n",
    "            volatility_change = (late_std - early_std) / early_std\n",
    "            features[f'{sensor_name}_volatility_change'] = volatility_change\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Demonstrate trend analysis with a sample vehicle\n",
    "print(\"ğŸ” Demonstrating Trend Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get a vehicle with good data coverage\n",
    "vehicle_counts = sensor_df['vin'].value_counts()\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Sample Vehicle: {sample_vin}\")\n",
    "    print(f\"Data Points: {len(sample_data)}\")\n",
    "    print(f\"Date Range: {sample_data['time'].min()} to {sample_data['time'].max()}\")\n",
    "    \n",
    "    # Calculate trends for each DPF sensor\n",
    "    all_trends = {}\n",
    "    for sensor in dpf_sensors.keys():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                trends = calculate_trend_features(sensor_subset, sensor)\n",
    "                all_trends.update(trends)\n",
    "                \n",
    "                # Show human-readable interpretation\n",
    "                slope_key = f'{sensor}_trend_slope_per_day'\n",
    "                if slope_key in trends:\n",
    "                    slope = trends[slope_key]\n",
    "                    direction = \"increasing\" if slope > 0 else \"decreasing\"\n",
    "                    strength = trends.get(f'{sensor}_trend_strength', 0)\n",
    "                    \n",
    "                    print(f\"\\nğŸ“ˆ {dpf_sensors[sensor]['description']}:\")\n",
    "                    print(f\"   Trend: {direction} by {abs(slope):.3f} {dpf_sensors[sensor]['unit']}/day\")\n",
    "                    print(f\"   Strength: {strength:.3f} (0=weak, 1=strong linear trend)\")\n",
    "                    \n",
    "                    # Interpret for fleet manager\n",
    "                    if abs(slope) > 0.1 and strength > 0.3:\n",
    "                        print(f\"   ğŸš¨ Alert: Strong {direction} trend detected!\")\n",
    "                    elif abs(slope) > 0.05:\n",
    "                        print(f\"   âš ï¸ Caution: Moderate {direction} trend\")\n",
    "                    else:\n",
    "                        print(f\"   âœ… Normal: Stable readings\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total trend features calculated: {len(all_trends)}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for trend analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "y8rc06quid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Demonstrating Threshold Analysis\n",
      "========================================\n",
      "Analyzing thresholds for Vehicle: 1XK1D40X1NJ495537\n",
      "\n",
      "ğŸ¯ Engine Load Percentage (engineLoadPercent):\n",
      "   â¬†ï¸ Time above 80 %: 24.8%\n",
      "      ğŸš¨ ALERT: Excessive high readings!\n",
      "   â¬‡ï¸ Time below 10 %: 6.4%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 16 consecutive readings\n",
      "\n",
      "ğŸ¯ Engine RPM (engineRpm):\n",
      "   â¬†ï¸ Time above 2000 RPM: 0.0%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 600 RPM: 4.8%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 1 consecutive readings\n",
      "\n",
      "ğŸ¯ DEF (Diesel Exhaust Fluid) Level (defLevelMilliPercent):\n",
      "   â¬†ï¸ Time above 95000 milli-%: 5.4%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 50000 milli-%: 28.5%\n",
      "      ğŸš¨ ALERT: Excessive low readings!\n",
      "   ğŸ“Š Longest high streak: 101 consecutive readings\n",
      "\n",
      "ğŸ¯ Engine Coolant Temperature (engineCoolantTemperatureMilliC):\n",
      "   â¬†ï¸ Time above 95000 milli-Â°C: 17.6%\n",
      "      âš ï¸ WARNING: Frequent high readings\n",
      "   â¬‡ï¸ Time below 65000 milli-Â°C: 4.2%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 35 consecutive readings\n",
      "\n",
      "ğŸ¯ Vehicle Speed (ecuSpeedMph):\n",
      "   â¬†ï¸ Time above 80 mph: 9.7%\n",
      "      âœ… Normal high threshold behavior\n",
      "   â¬‡ï¸ Time below 0 mph: 0.0%\n",
      "      âœ… Normal low threshold behavior\n",
      "   ğŸ“Š Longest high streak: 18 consecutive readings\n",
      "\n",
      "ğŸ“Š Total threshold features calculated: 44\n"
     ]
    }
   ],
   "source": [
    "def calculate_threshold_features(sensor_data, sensor_name, thresholds, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate threshold-based features that are easy to interpret and act upon.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    # Prepare data\n",
    "    data = sensor_data.dropna().sort_values(time_col)\n",
    "    if len(data) < 2:\n",
    "        return {}\n",
    "    \n",
    "    values = data[sensor_name].values\n",
    "    features = {}\n",
    "    \n",
    "    # 1. PERCENTAGE OF TIME OUTSIDE THRESHOLDS\n",
    "    \n",
    "    # Time above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        high_thresh = thresholds['high']\n",
    "        pct_above_high = (values > high_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_above_high'] = pct_above_high\n",
    "    \n",
    "    # Time below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        low_thresh = thresholds['low']\n",
    "        pct_below_low = (values < low_thresh).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_below_low'] = pct_below_low\n",
    "    \n",
    "    # Total time outside normal range\n",
    "    if 'high' in thresholds and 'low' in thresholds:\n",
    "        outside_normal = ((values > thresholds['high']) | (values < thresholds['low'])).mean() * 100\n",
    "        features[f'{sensor_name}_pct_time_outside_normal'] = outside_normal\n",
    "    \n",
    "    # 2. CONSECUTIVE VIOLATIONS (Streak Analysis)\n",
    "    \n",
    "    # Longest streak above high threshold\n",
    "    if 'high' in thresholds:\n",
    "        above_high = values > thresholds['high']\n",
    "        max_streak_high = calculate_max_consecutive(above_high)\n",
    "        features[f'{sensor_name}_max_streak_above_high'] = max_streak_high\n",
    "    \n",
    "    # Longest streak below low threshold\n",
    "    if 'low' in thresholds:\n",
    "        below_low = values < thresholds['low']\n",
    "        max_streak_low = calculate_max_consecutive(below_low)\n",
    "        features[f'{sensor_name}_max_streak_below_low'] = max_streak_low\n",
    "    \n",
    "    # 3. FREQUENCY OF VIOLATIONS\n",
    "    \n",
    "    # How many separate incidents of threshold violations?\n",
    "    if 'high' in thresholds:\n",
    "        high_violations = count_violation_episodes(values > thresholds['high'])\n",
    "        features[f'{sensor_name}_high_violation_episodes'] = high_violations\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations = count_violation_episodes(values < thresholds['low'])\n",
    "        features[f'{sensor_name}_low_violation_episodes'] = low_violations\n",
    "    \n",
    "    # 4. SEVERITY OF VIOLATIONS\n",
    "    \n",
    "    # Average severity when violations occur\n",
    "    if 'high' in thresholds:\n",
    "        high_violations_mask = values > thresholds['high']\n",
    "        if high_violations_mask.any():\n",
    "            avg_excess = (values[high_violations_mask] - thresholds['high']).mean()\n",
    "            features[f'{sensor_name}_avg_excess_above_high'] = avg_excess\n",
    "    \n",
    "    if 'low' in thresholds:\n",
    "        low_violations_mask = values < thresholds['low']\n",
    "        if low_violations_mask.any():\n",
    "            avg_deficit = (thresholds['low'] - values[low_violations_mask]).mean()\n",
    "            features[f'{sensor_name}_avg_deficit_below_low'] = avg_deficit\n",
    "    \n",
    "    return features\n",
    "\n",
    "def calculate_max_consecutive(boolean_array):\n",
    "    \"\"\"Calculate maximum consecutive True values in a boolean array.\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    max_streak = 0\n",
    "    current_streak = 0\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value:\n",
    "            current_streak += 1\n",
    "            max_streak = max(max_streak, current_streak)\n",
    "        else:\n",
    "            current_streak = 0\n",
    "    \n",
    "    return max_streak\n",
    "\n",
    "def count_violation_episodes(boolean_array):\n",
    "    \"\"\"Count number of separate violation episodes (groups of consecutive True values).\"\"\"\n",
    "    if len(boolean_array) == 0:\n",
    "        return 0\n",
    "    \n",
    "    episodes = 0\n",
    "    in_violation = False\n",
    "    \n",
    "    for value in boolean_array:\n",
    "        if value and not in_violation:\n",
    "            episodes += 1\n",
    "            in_violation = True\n",
    "        elif not value:\n",
    "            in_violation = False\n",
    "    \n",
    "    return episodes\n",
    "\n",
    "# Demonstrate threshold analysis\n",
    "print(\"ğŸ¯ Demonstrating Threshold Analysis\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing thresholds for Vehicle: {sample_vin}\")\n",
    "    \n",
    "    # Calculate threshold features for each sensor\n",
    "    all_threshold_features = {}\n",
    "    \n",
    "    for sensor, info in dpf_sensors.items():\n",
    "        if sensor in sample_data.columns:\n",
    "            sensor_subset = sample_data[['time', sensor]].dropna()\n",
    "            if len(sensor_subset) >= 5:\n",
    "                thresholds = info['alert_thresholds']\n",
    "                threshold_features = calculate_threshold_features(sensor_subset, sensor, thresholds)\n",
    "                all_threshold_features.update(threshold_features)\n",
    "                \n",
    "                print(f\"\\nğŸ¯ {info['description']} ({sensor}):\")\n",
    "                \n",
    "                # Show key threshold metrics\n",
    "                pct_high_key = f'{sensor}_pct_time_above_high'\n",
    "                pct_low_key = f'{sensor}_pct_time_below_low'\n",
    "                \n",
    "                if pct_high_key in threshold_features:\n",
    "                    pct_high = threshold_features[pct_high_key]\n",
    "                    print(f\"   â¬†ï¸ Time above {thresholds['high']} {info['unit']}: {pct_high:.1f}%\")\n",
    "                    \n",
    "                    if pct_high > 20:\n",
    "                        print(f\"      ğŸš¨ ALERT: Excessive high readings!\")\n",
    "                    elif pct_high > 10:\n",
    "                        print(f\"      âš ï¸ WARNING: Frequent high readings\")\n",
    "                    else:\n",
    "                        print(f\"      âœ… Normal high threshold behavior\")\n",
    "                \n",
    "                if pct_low_key in threshold_features:\n",
    "                    pct_low = threshold_features[pct_low_key]\n",
    "                    print(f\"   â¬‡ï¸ Time below {thresholds['low']} {info['unit']}: {pct_low:.1f}%\")\n",
    "                    \n",
    "                    if pct_low > 20:\n",
    "                        print(f\"      ğŸš¨ ALERT: Excessive low readings!\")\n",
    "                    elif pct_low > 10:\n",
    "                        print(f\"      âš ï¸ WARNING: Frequent low readings\")\n",
    "                    else:\n",
    "                        print(f\"      âœ… Normal low threshold behavior\")\n",
    "                \n",
    "                # Show streak information\n",
    "                streak_high_key = f'{sensor}_max_streak_above_high'\n",
    "                if streak_high_key in threshold_features:\n",
    "                    streak = threshold_features[streak_high_key]\n",
    "                    print(f\"   ğŸ“Š Longest high streak: {streak} consecutive readings\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total threshold features calculated: {len(all_threshold_features)}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for threshold analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vcc76a24vdr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Demonstrating Operational Pattern Analysis\n",
      "==================================================\n",
      "Analyzing operational patterns for Vehicle: 1XK1D40X1NJ495537\n",
      "Data points: 269964\n",
      "\n",
      "ğŸ“Š Current Operational Profile:\n",
      "   ğŸ”§ Average Engine Load: 51.1%\n",
      "      âœ… Moderate duty cycle\n",
      "   ğŸ›£ï¸ Highway driving: 73.6% of time\n",
      "   ğŸ™ï¸ City driving: 17.2% of time\n",
      "      âœ… Good highway driving - helps DPF regeneration\n",
      "\n",
      "ğŸ“ˆ Operational Changes (Recent vs Historical):\n",
      "   ğŸ“Š peak_operating_hour: increased by 16.7%\n",
      "      âš ï¸ Notable operational change\n",
      "   ğŸ“Š operating_hour_spread: decreased by 57.4%\n",
      "      ğŸš¨ Significant operational change detected!\n",
      "\n",
      "ğŸ“Š Total operational features calculated: 28\n"
     ]
    }
   ],
   "source": [
    "def calculate_operational_pattern_features(sensor_data, time_col='time'):\n",
    "    \"\"\"\n",
    "    Calculate operational pattern features that help identify changes in vehicle usage.\n",
    "    These features help fleet managers understand if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 10:  # Need reasonable amount of data for patterns\n",
    "        return {}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 1. DUTY CYCLE ANALYSIS (How hard is the vehicle working?)\n",
    "    if 'engineLoadPercent' in sensor_data.columns:\n",
    "        load_data = sensor_data['engineLoadPercent'].dropna()\n",
    "        if len(load_data) >= 10:\n",
    "            \n",
    "            # Average duty cycle\n",
    "            avg_load = load_data.mean()\n",
    "            features['avg_engine_load'] = avg_load\n",
    "            \n",
    "            # Load distribution analysis\n",
    "            features['pct_time_high_load'] = (load_data > 70).mean() * 100  # >70% load\n",
    "            features['pct_time_medium_load'] = ((load_data >= 40) & (load_data <= 70)).mean() * 100\n",
    "            features['pct_time_low_load'] = (load_data < 40).mean() * 100  # <40% load\n",
    "            \n",
    "            # Load consistency (coefficient of variation)\n",
    "            if avg_load > 0:\n",
    "                features['load_consistency'] = load_data.std() / avg_load\n",
    "    \n",
    "    # 2. DRIVE PROFILE ANALYSIS (City vs Highway driving)\n",
    "    if 'ecuSpeedMph' in sensor_data.columns:\n",
    "        speed_data = sensor_data['ecuSpeedMph'].dropna()\n",
    "        if len(speed_data) >= 10:\n",
    "            \n",
    "            # Speed distribution (proxy for drive cycle)\n",
    "            features['pct_time_highway_speed'] = (speed_data > 45).mean() * 100  # >45 mph = highway\n",
    "            features['pct_time_city_speed'] = ((speed_data > 0) & (speed_data <= 35)).mean() * 100  # 0-35 mph = city\n",
    "            features['pct_time_stopped'] = (speed_data == 0).mean() * 100  # Stopped\n",
    "            \n",
    "            # Average operating speed\n",
    "            features['avg_operating_speed'] = speed_data[speed_data > 0].mean() if (speed_data > 0).any() else 0\n",
    "            \n",
    "            # Speed variability (smooth vs stop-and-go driving)\n",
    "            features['speed_variability'] = speed_data.std()\n",
    "    \n",
    "    # 3. OPERATIONAL TIMING PATTERNS\n",
    "    if time_col in sensor_data.columns:\n",
    "        # Add hour of day for operational pattern analysis\n",
    "        sensor_data_copy = sensor_data.copy()\n",
    "        sensor_data_copy['hour'] = pd.to_datetime(sensor_data_copy[time_col]).dt.hour\n",
    "        \n",
    "        # Operating hours distribution\n",
    "        operating_hours = sensor_data_copy['hour'].value_counts().sort_index()\n",
    "        \n",
    "        # Peak operating hours\n",
    "        if len(operating_hours) > 0:\n",
    "            features['peak_operating_hour'] = operating_hours.idxmax()\n",
    "            features['operating_hour_spread'] = operating_hours.max() - operating_hours.min()\n",
    "    \n",
    "    # 4. MULTI-SENSOR OPERATIONAL PATTERNS\n",
    "    # Correlation between load and speed (operational efficiency)\n",
    "    if 'engineLoadPercent' in sensor_data.columns and 'ecuSpeedMph' in sensor_data.columns:\n",
    "        load_speed_data = sensor_data[['engineLoadPercent', 'ecuSpeedMph']].dropna()\n",
    "        if len(load_speed_data) >= 10:\n",
    "            correlation = load_speed_data['engineLoadPercent'].corr(load_speed_data['ecuSpeedMph'])\n",
    "            features['load_speed_correlation'] = correlation if not np.isnan(correlation) else 0\n",
    "    \n",
    "    # Engine efficiency patterns (RPM vs Load)\n",
    "    if 'engineRpm' in sensor_data.columns and 'engineLoadPercent' in sensor_data.columns:\n",
    "        rpm_load_data = sensor_data[['engineRpm', 'engineLoadPercent']].dropna()\n",
    "        if len(rpm_load_data) >= 10:\n",
    "            # Calculate efficiency proxy (load per RPM)\n",
    "            efficiency_data = rpm_load_data[rpm_load_data['engineRpm'] > 0].copy()\n",
    "            if len(efficiency_data) > 0:\n",
    "                efficiency_data['load_per_rpm'] = efficiency_data['engineLoadPercent'] / efficiency_data['engineRpm'] * 1000\n",
    "                features['avg_load_per_rpm'] = efficiency_data['load_per_rpm'].mean()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compare_operational_periods(sensor_data, split_ratio=0.7, time_col='time'):\n",
    "    \"\"\"\n",
    "    Compare operational patterns between historical (early) and recent periods.\n",
    "    This helps identify if operational changes are affecting DPF health.\n",
    "    \"\"\"\n",
    "    if len(sensor_data) < 20:  # Need enough data to split meaningfully\n",
    "        return {}\n",
    "    \n",
    "    # Sort by time and split\n",
    "    sorted_data = sensor_data.sort_values(time_col)\n",
    "    split_point = int(len(sorted_data) * split_ratio)\n",
    "    \n",
    "    historical_data = sorted_data.iloc[:split_point]\n",
    "    recent_data = sorted_data.iloc[split_point:]\n",
    "    \n",
    "    # Calculate operational features for both periods\n",
    "    historical_features = calculate_operational_pattern_features(historical_data, time_col)\n",
    "    recent_features = calculate_operational_pattern_features(recent_data, time_col)\n",
    "    \n",
    "    # Calculate changes\n",
    "    changes = {}\n",
    "    for feature in historical_features:\n",
    "        if feature in recent_features:\n",
    "            historical_value = historical_features[feature]\n",
    "            recent_value = recent_features[feature]\n",
    "            \n",
    "            if historical_value != 0:\n",
    "                percent_change = ((recent_value - historical_value) / abs(historical_value)) * 100\n",
    "                changes[f'{feature}_change_pct'] = percent_change\n",
    "    \n",
    "    return changes\n",
    "\n",
    "# Demonstrate operational pattern analysis\n",
    "print(\"ğŸ”„ Demonstrating Operational Pattern Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(vehicle_counts) > 0:\n",
    "    sample_vin = vehicle_counts.index[0]\n",
    "    sample_data = sensor_df[sensor_df['vin'] == sample_vin].copy()\n",
    "    \n",
    "    print(f\"Analyzing operational patterns for Vehicle: {sample_vin}\")\n",
    "    print(f\"Data points: {len(sample_data)}\")\n",
    "    \n",
    "    # Calculate current operational features\n",
    "    operational_features = calculate_operational_pattern_features(sample_data)\n",
    "    \n",
    "    if operational_features:\n",
    "        print(f\"\\nğŸ“Š Current Operational Profile:\")\n",
    "        \n",
    "        # Duty cycle analysis\n",
    "        if 'avg_engine_load' in operational_features:\n",
    "            avg_load = operational_features['avg_engine_load']\n",
    "            print(f\"   ğŸ”§ Average Engine Load: {avg_load:.1f}%\")\n",
    "            \n",
    "            if avg_load > 60:\n",
    "                print(f\"      ğŸš¨ High duty cycle - may stress DPF\")\n",
    "            elif avg_load < 30:\n",
    "                print(f\"      âš ï¸ Low duty cycle - may not regenerate DPF properly\")\n",
    "            else:\n",
    "                print(f\"      âœ… Moderate duty cycle\")\n",
    "        \n",
    "        # Drive profile analysis\n",
    "        if 'pct_time_highway_speed' in operational_features:\n",
    "            highway_pct = operational_features['pct_time_highway_speed']\n",
    "            city_pct = operational_features.get('pct_time_city_speed', 0)\n",
    "            \n",
    "            print(f\"   ğŸ›£ï¸ Highway driving: {highway_pct:.1f}% of time\")\n",
    "            print(f\"   ğŸ™ï¸ City driving: {city_pct:.1f}% of time\")\n",
    "            \n",
    "            if highway_pct < 20:\n",
    "                print(f\"      ğŸš¨ Mostly city driving - poor for DPF regeneration\")\n",
    "            elif highway_pct > 60:\n",
    "                print(f\"      âœ… Good highway driving - helps DPF regeneration\")\n",
    "            else:\n",
    "                print(f\"      âš ï¸ Mixed driving profile\")\n",
    "    \n",
    "    # Compare historical vs recent patterns\n",
    "    pattern_changes = compare_operational_periods(sample_data)\n",
    "    \n",
    "    if pattern_changes:\n",
    "        print(f\"\\nğŸ“ˆ Operational Changes (Recent vs Historical):\")\n",
    "        \n",
    "        significant_changes = {k: v for k, v in pattern_changes.items() if abs(v) > 10}\n",
    "        \n",
    "        if significant_changes:\n",
    "            for change_feature, percent_change in significant_changes.items():\n",
    "                base_feature = change_feature.replace('_change_pct', '')\n",
    "                direction = \"increased\" if percent_change > 0 else \"decreased\"\n",
    "                print(f\"   ğŸ“Š {base_feature}: {direction} by {abs(percent_change):.1f}%\")\n",
    "                \n",
    "                # Interpret the change\n",
    "                if abs(percent_change) > 25:\n",
    "                    print(f\"      ğŸš¨ Significant operational change detected!\")\n",
    "                elif abs(percent_change) > 15:\n",
    "                    print(f\"      âš ï¸ Notable operational change\")\n",
    "        else:\n",
    "            print(f\"   âœ… Stable operational patterns\")\n",
    "    \n",
    "    total_features = len(operational_features) + len(pattern_changes)\n",
    "    print(f\"\\nğŸ“Š Total operational features calculated: {total_features}\")\n",
    "else:\n",
    "    print(\"âŒ No vehicle data available for operational pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1mnnzpb7wu4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Building Comprehensive Interpretable Feature Dataset\n",
      "============================================================\n",
      "ğŸ”§ Building comprehensive feature dataset...\n",
      "âœ… Built feature dataset with 46 examples\n",
      "ğŸ“Š Total interpretable features: 102\n",
      "   ğŸ“ˆ Trend features: 20\n",
      "   ğŸ¯ Threshold features: 56\n",
      "   ğŸ”„ Operational features: 26\n",
      "\n",
      "ğŸ”§ Maintenance Type Distribution:\n",
      "   EXHAUST SYSTEM INSPECT DIAGNOSE: 26 examples\n",
      "   EXHAUST SYSTEM: 14 examples\n",
      "   FILTER - DIESEL PARTICULATE: 6 examples\n"
     ]
    }
   ],
   "source": [
    "def build_comprehensive_feature_dataset(maintenance_df, sensor_df, window_days=30):\n",
    "    \"\"\"\n",
    "    Build a comprehensive dataset with all our interpretable features.\n",
    "    This combines trend, threshold, and operational pattern features.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”§ Building comprehensive feature dataset...\")\n",
    "    \n",
    "    feature_data = []\n",
    "    \n",
    "    for _, maintenance_row in maintenance_df.iterrows():\n",
    "        vin = maintenance_row['VIN Number']\n",
    "        maintenance_date = maintenance_row['Date of Issue']\n",
    "        \n",
    "        if pd.isna(vin) or pd.isna(maintenance_date):\n",
    "            continue\n",
    "        \n",
    "        # Get sensor data before maintenance\n",
    "        start_date = maintenance_date - timedelta(days=window_days)\n",
    "        \n",
    "        vehicle_data = sensor_df[\n",
    "            (sensor_df['vin'] == vin) &\n",
    "            (sensor_df['time'] >= start_date) &\n",
    "            (sensor_df['time'] < maintenance_date)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(vehicle_data) < 10:  # Need minimum data\n",
    "            continue\n",
    "        \n",
    "        # Calculate all feature types\n",
    "        all_features = {\n",
    "            'vin': vin,\n",
    "            'vehicle_number': maintenance_row['Vehicle_Number'],\n",
    "            'maintenance_date': maintenance_date,\n",
    "            'maintenance_type': maintenance_row['lines_jobDescriptions'],\n",
    "            'data_points': len(vehicle_data),\n",
    "            'window_days': window_days\n",
    "        }\n",
    "        \n",
    "        # 1. Trend features for each DPF sensor\n",
    "        for sensor in dpf_sensors.keys():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    trend_features = calculate_trend_features(sensor_subset, sensor)\n",
    "                    all_features.update(trend_features)\n",
    "        \n",
    "        # 2. Threshold features for each DPF sensor\n",
    "        for sensor, info in dpf_sensors.items():\n",
    "            if sensor in vehicle_data.columns:\n",
    "                sensor_subset = vehicle_data[['time', sensor]].dropna()\n",
    "                if len(sensor_subset) >= 5:\n",
    "                    threshold_features = calculate_threshold_features(\n",
    "                        sensor_subset, sensor, info['alert_thresholds']\n",
    "                    )\n",
    "                    all_features.update(threshold_features)\n",
    "        \n",
    "        # 3. Operational pattern features\n",
    "        operational_features = calculate_operational_pattern_features(vehicle_data)\n",
    "        all_features.update(operational_features)\n",
    "        \n",
    "        # 4. Operational change features\n",
    "        pattern_changes = compare_operational_periods(vehicle_data)\n",
    "        all_features.update(pattern_changes)\n",
    "        \n",
    "        feature_data.append(all_features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "# Build the comprehensive feature dataset\n",
    "print(\"ğŸ”§ Building Comprehensive Interpretable Feature Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dataset = build_comprehensive_feature_dataset(maintenance_df, sensor_df)\n",
    "\n",
    "if len(feature_dataset) > 0:\n",
    "    print(f\"âœ… Built feature dataset with {len(feature_dataset)} examples\")\n",
    "    \n",
    "    # Analyze feature coverage\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"ğŸ“Š Total interpretable features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Categorize features by type\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation', 'excess', 'deficit'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Trend features: {len(trend_features)}\")\n",
    "    print(f\"   ğŸ¯ Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   ğŸ”„ Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    # Show maintenance type distribution\n",
    "    print(f\"\\nğŸ”§ Maintenance Type Distribution:\")\n",
    "    for maint_type, count in feature_dataset['maintenance_type'].value_counts().items():\n",
    "        print(f\"   {maint_type}: {count} examples\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ Could not build feature dataset - insufficient data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "di0gmgaqohe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Validating Interpretable Features\n",
      "========================================\n",
      "ğŸ“Š Data Quality Analysis:\n",
      "   âš ï¸ Features with >50% missing data: 1\n",
      "      - ecuSpeedMph_avg_excess_above_high: 76.1% missing\n",
      "\n",
      "ğŸ” Feature Discriminative Power (by Maintenance Type):\n",
      "   ğŸ† Top 10 Most Discriminative Features:\n",
      "      1. ğŸ“ˆ Trend - engineCoolantTemperatureMilliC_trend_slope_per_day: CV=24.380\n",
      "      2. ğŸ¯ Threshold - pct_time_city_speed_change_pct: CV=20.160\n",
      "      3. ğŸ“ˆ Trend - defLevelMilliPercent_trend_slope_per_day: CV=17.688\n",
      "      4. ğŸ”„ Operational - load_speed_correlation_change_pct: CV=16.439\n",
      "      5. ğŸ”„ Operational - avg_engine_load_change_pct: CV=13.777\n",
      "      6. ğŸ¯ Threshold - pct_time_medium_load_change_pct: CV=13.562\n",
      "      7. ğŸ“ˆ Trend - engineRpm_trend_slope_per_day: CV=4.648\n",
      "      8. ğŸ¯ Threshold - pct_time_stopped_change_pct: CV=4.272\n",
      "      9. ğŸ”„ Operational - ecuSpeedMph_recent_vs_historical_pct: CV=3.999\n",
      "      10. ğŸ“ˆ Trend - ecuSpeedMph_trend_slope_per_day: CV=2.841\n",
      "\n",
      "ğŸ’¡ Feature Interpretability Assessment:\n",
      "   ğŸŒŸ Most Interpretable Features (Score 8-10):\n",
      "      â€¢ engineLoadPercent_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      â€¢ engineRpm_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      â€¢ defLevelMilliPercent_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      â€¢ engineCoolantTemperatureMilliC_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      â€¢ ecuSpeedMph_trend_slope_per_day (Score: 10/10) - Clear direction and rate of change\n",
      "      â€¢ engineLoadPercent_pct_time_above_high (Score: 9/10) - Percentage time outside thresholds\n",
      "      â€¢ engineLoadPercent_pct_time_below_low (Score: 9/10) - Percentage time outside thresholds\n",
      "      â€¢ engineLoadPercent_pct_time_outside_normal (Score: 9/10) - Percentage time outside thresholds\n",
      "      â€¢ engineRpm_pct_time_above_high (Score: 9/10) - Percentage time outside thresholds\n",
      "      â€¢ engineRpm_pct_time_below_low (Score: 9/10) - Percentage time outside thresholds\n",
      "\n",
      "ğŸ¯ Feature Recommendations for Fleet Managers:\n",
      "   ğŸ† Top 5 Recommended Features (High interpretability + High predictive power):\n",
      "      1. engineCoolantTemperatureMilliC_trend_slope_per_day\n",
      "         Interpretability: 10/10 - Clear direction and rate of change\n",
      "         Combined Score: 24.380\n",
      "\n",
      "      2. pct_time_city_speed_change_pct\n",
      "         Interpretability: 9/10 - Percentage time outside thresholds\n",
      "         Combined Score: 18.144\n",
      "\n",
      "      3. defLevelMilliPercent_trend_slope_per_day\n",
      "         Interpretability: 10/10 - Clear direction and rate of change\n",
      "         Combined Score: 17.688\n",
      "\n",
      "      4. pct_time_medium_load_change_pct\n",
      "         Interpretability: 9/10 - Percentage time outside thresholds\n",
      "         Combined Score: 12.206\n",
      "\n",
      "      5. load_speed_correlation_change_pct\n",
      "         Interpretability: 7/10 - Percentage change over time\n",
      "         Combined Score: 11.508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate feature quality and predictive power\n",
    "def validate_interpretable_features(feature_dataset):\n",
    "    \"\"\"\n",
    "    Validate the quality and predictive power of our interpretable features.\n",
    "    \"\"\"\n",
    "    if len(feature_dataset) == 0:\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ§ª Validating Interpretable Features\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"âŒ No features to validate\")\n",
    "        return\n",
    "    \n",
    "    # 1. DATA QUALITY VALIDATION\n",
    "    print(\"ğŸ“Š Data Quality Analysis:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_rates = feature_dataset[feature_cols].isnull().mean() * 100\n",
    "    high_missing = missing_rates[missing_rates > 50]\n",
    "    \n",
    "    if len(high_missing) > 0:\n",
    "        print(f\"   âš ï¸ Features with >50% missing data: {len(high_missing)}\")\n",
    "        for feature, missing_pct in high_missing.head(5).items():\n",
    "            print(f\"      - {feature}: {missing_pct:.1f}% missing\")\n",
    "    else:\n",
    "        print(f\"   âœ… Good data coverage - no features with >50% missing data\")\n",
    "    \n",
    "    # 2. FEATURE DISCRIMINATIVE POWER\n",
    "    print(f\"\\nğŸ” Feature Discriminative Power (by Maintenance Type):\")\n",
    "    \n",
    "    # For each maintenance type, find features that discriminate well\n",
    "    maintenance_types = feature_dataset['maintenance_type'].unique()\n",
    "    \n",
    "    if len(maintenance_types) > 1:\n",
    "        # Calculate feature means by maintenance type\n",
    "        type_means = feature_dataset.groupby('maintenance_type')[feature_cols].mean()\n",
    "        \n",
    "        # Calculate coefficient of variation across maintenance types for each feature\n",
    "        discriminative_power = {}\n",
    "        for feature in feature_cols:\n",
    "            values = type_means[feature].dropna()\n",
    "            if len(values) > 1 and values.std() > 0:\n",
    "                cv = values.std() / abs(values.mean()) if values.mean() != 0 else 0\n",
    "                discriminative_power[feature] = cv\n",
    "        \n",
    "        # Show top discriminative features\n",
    "        if discriminative_power:\n",
    "            top_discriminative = sorted(discriminative_power.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "            \n",
    "            print(f\"   ğŸ† Top 10 Most Discriminative Features:\")\n",
    "            for i, (feature, cv) in enumerate(top_discriminative, 1):\n",
    "                # Determine feature type\n",
    "                if 'trend' in feature:\n",
    "                    feature_type = \"ğŸ“ˆ Trend\"\n",
    "                elif any(x in feature for x in ['pct_time', 'streak', 'violation']):\n",
    "                    feature_type = \"ğŸ¯ Threshold\"\n",
    "                else:\n",
    "                    feature_type = \"ğŸ”„ Operational\"\n",
    "                \n",
    "                print(f\"      {i}. {feature_type} - {feature}: CV={cv:.3f}\")\n",
    "    \n",
    "    # 3. FEATURE INTERPRETABILITY ASSESSMENT\n",
    "    print(f\"\\nğŸ’¡ Feature Interpretability Assessment:\")\n",
    "    \n",
    "    interpretability_scores = {\n",
    "        'trend_slope': {'score': 10, 'explanation': 'Clear direction and rate of change'},\n",
    "        'pct_time': {'score': 9, 'explanation': 'Percentage time outside thresholds'},\n",
    "        'avg_': {'score': 8, 'explanation': 'Simple average values'},\n",
    "        'max_streak': {'score': 8, 'explanation': 'Consecutive violations count'},\n",
    "        'change_pct': {'score': 7, 'explanation': 'Percentage change over time'},\n",
    "        'correlation': {'score': 6, 'explanation': 'Relationship between sensors'},\n",
    "        'volatility': {'score': 5, 'explanation': 'Stability measure'}\n",
    "    }\n",
    "    \n",
    "    # Score each feature\n",
    "    feature_scores = []\n",
    "    for feature in feature_cols:\n",
    "        score = 3  # Default score\n",
    "        explanation = \"General metric\"\n",
    "        \n",
    "        for pattern, info in interpretability_scores.items():\n",
    "            if pattern in feature:\n",
    "                score = info['score']\n",
    "                explanation = info['explanation']\n",
    "                break\n",
    "        \n",
    "        feature_scores.append((feature, score, explanation))\n",
    "    \n",
    "    # Show most interpretable features\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"   ğŸŒŸ Most Interpretable Features (Score 8-10):\")\n",
    "    high_interpretability = [f for f in feature_scores if f[1] >= 8]\n",
    "    \n",
    "    for feature, score, explanation in high_interpretability[:10]:\n",
    "        print(f\"      â€¢ {feature} (Score: {score}/10) - {explanation}\")\n",
    "    \n",
    "    # 4. FEATURE RECOMMENDATION\n",
    "    print(f\"\\nğŸ¯ Feature Recommendations for Fleet Managers:\")\n",
    "    \n",
    "    # Combine discriminative power and interpretability\n",
    "    if discriminative_power:\n",
    "        recommended_features = []\n",
    "        \n",
    "        for feature, interp_score, explanation in feature_scores:\n",
    "            if feature in discriminative_power and interp_score >= 7:\n",
    "                combined_score = discriminative_power[feature] * (interp_score / 10)\n",
    "                recommended_features.append((feature, combined_score, interp_score, explanation))\n",
    "        \n",
    "        recommended_features.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"   ğŸ† Top 5 Recommended Features (High interpretability + High predictive power):\")\n",
    "        for i, (feature, combined_score, interp_score, explanation) in enumerate(recommended_features[:5], 1):\n",
    "            print(f\"      {i}. {feature}\")\n",
    "            print(f\"         Interpretability: {interp_score}/10 - {explanation}\")\n",
    "            print(f\"         Combined Score: {combined_score:.3f}\")\n",
    "            print()\n",
    "\n",
    "# Run feature validation\n",
    "if len(feature_dataset) > 0:\n",
    "    validate_interpretable_features(feature_dataset)\n",
    "else:\n",
    "    print(\"âŒ No feature dataset available for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dqfofv8qu5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ FEATURE ENGINEERING COMPLETE!\n",
      "==================================================\n",
      "âœ… Saved interpretable features to: ../data/interpretable_features_dataset.csv\n",
      "ğŸ“Š Dataset contains: 46 examples\n",
      "ğŸ”§ Total features: 102\n",
      "   ğŸ“ˆ Trend features: 20\n",
      "   ğŸ¯ Threshold features: 47\n",
      "   ğŸ”„ Operational features: 35\n",
      "\n",
      "ğŸ‰ Ready for model building and deployment!\n",
      "Next step: Use these features in the explainable model notebook\n",
      "\n",
      "ğŸ“š What You've Learned:\n",
      "â€¢ How to create trend features that show sensor degradation patterns\n",
      "â€¢ How to build threshold features for actionable alerts\n",
      "â€¢ How to detect operational pattern changes affecting DPF health\n",
      "â€¢ How to validate feature quality and interpretability\n",
      "â€¢ Best practices for explainable feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Save the interpretable feature dataset for use in other notebooks\n",
    "if len(feature_dataset) > 0:\n",
    "    output_file = '../data/interpretable_features_dataset.csv'\n",
    "    feature_dataset.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(\"ğŸ’¾ FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"âœ… Saved interpretable features to: {output_file}\")\n",
    "    print(f\"ğŸ“Š Dataset contains: {len(feature_dataset)} examples\")\n",
    "    \n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in [\n",
    "        'vin', 'vehicle_number', 'maintenance_date', 'maintenance_type', 'data_points', 'window_days'\n",
    "    ]]\n",
    "    print(f\"ğŸ”§ Total features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Feature breakdown\n",
    "    trend_features = [col for col in feature_cols if 'trend' in col]\n",
    "    threshold_features = [col for col in feature_cols if any(x in col for x in ['pct_time', 'streak', 'violation'])]\n",
    "    operational_features = [col for col in feature_cols if col not in trend_features + threshold_features]\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ Trend features: {len(trend_features)}\")\n",
    "    print(f\"   ğŸ¯ Threshold features: {len(threshold_features)}\")\n",
    "    print(f\"   ğŸ”„ Operational features: {len(operational_features)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Ready for model building and deployment!\")\n",
    "    print(f\"Next step: Use these features in the explainable model notebook\")\n",
    "else:\n",
    "    print(\"âŒ No interpretable features dataset created\")\n",
    "    print(\"Please check data availability and try again\")\n",
    "\n",
    "print(f\"\\nğŸ“š What You've Learned:\")\n",
    "print(f\"â€¢ How to create trend features that show sensor degradation patterns\")\n",
    "print(f\"â€¢ How to build threshold features for actionable alerts\")\n",
    "print(f\"â€¢ How to detect operational pattern changes affecting DPF health\")\n",
    "print(f\"â€¢ How to validate feature quality and interpretability\")\n",
    "print(f\"â€¢ Best practices for explainable feature engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
