{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable RUL Prediction Models for Fleet Managers\n",
    "\n",
    "This notebook teaches you how to build **transparent, trustworthy models** that fleet managers can understand and confidently use for DPF maintenance decisions.\n",
    "\n",
    "## üéØ Why Explainable Models Matter\n",
    "\n",
    "**Traditional \"Black Box\" Approach**:\n",
    "- Complex neural networks, ensemble methods\n",
    "- High accuracy but no explanations\n",
    "- \"Trust the algorithm\" - hard to validate\n",
    "- Difficult to debug when predictions fail\n",
    "\n",
    "**Our Explainable Approach**:\n",
    "- Simple, transparent models\n",
    "- Every prediction comes with reasons\n",
    "- Fleet managers can validate logic\n",
    "- Easy to improve and maintain\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **Model Selection**: Choosing algorithms that naturally provide explanations\n",
    "2. **Feature Importance**: Understanding which sensors matter most\n",
    "3. **Prediction Explanations**: Generating human-readable reasons for each prediction\n",
    "4. **Model Validation**: Ensuring predictions make operational sense\n",
    "5. **Deployment Strategy**: Rolling out explainable models in production\n",
    "\n",
    "## üèÜ Success Criteria\n",
    "\n",
    "A successful explainable model should:\n",
    "- ‚úÖ Predict RUL within ¬±10 days accuracy\n",
    "- ‚úÖ Provide clear explanations for every prediction\n",
    "- ‚úÖ Use features that fleet managers understand\n",
    "- ‚úÖ Be simple enough to validate manually\n",
    "- ‚úÖ Enable actionable maintenance decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Explainable Model Tutorial Ready!\n",
      "üéØ Goal: Build RUL models that fleet managers can trust and understand\n",
      "üìä Using interpretable features from previous notebook\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Explainable ML libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap  # For model explanations\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üöÄ Explainable Model Tutorial Ready!\")\n",
    "print(\"üéØ Goal: Build RUL models that fleet managers can trust and understand\")\n",
    "print(\"üìä Using interpretable features from previous notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Load and Prepare Interpretable Features\n",
    "\n",
    "We'll use the interpretable features created in the previous notebook. These features were specifically designed to be explainable to fleet managers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load interpretable features dataset\nprint(\"üìÅ Loading Interpretable Features Dataset...\")\n\ntry:\n    # First try to load the interpretable features dataset\n    feature_dataset = pd.read_csv('../data/interpretable_features_dataset.csv')\n    print(f\"‚úÖ Loaded interpretable features: {len(feature_dataset)} examples\")\n    \n    # Check if we have the necessary columns\n    if 'rul_days' not in feature_dataset.columns:\n        print(\"‚ö†Ô∏è No RUL column found. Checking for maintenance data to create RUL...\")\n        \n        # If no RUL column, we need to create it from maintenance data\n        maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n        maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n        \n        # Create RUL by finding time between consecutive maintenance events\n        rul_data = []\n        for vehicle_num in maintenance_df['Vehicle_Number'].unique():\n            vehicle_maintenance = maintenance_df[\n                maintenance_df['Vehicle_Number'] == vehicle_num\n            ].sort_values('Date of Issue')\n            \n            # For each maintenance event except the last, calculate RUL\n            for i in range(len(vehicle_maintenance) - 1):\n                current_event = vehicle_maintenance.iloc[i]\n                next_event = vehicle_maintenance.iloc[i + 1]\n                \n                rul_days = (next_event['Date of Issue'] - current_event['Date of Issue']).days\n                \n                # Find corresponding entry in feature dataset\n                matching_entries = feature_dataset[\n                    (feature_dataset['vehicle_number'] == vehicle_num) &\n                    (pd.to_datetime(feature_dataset['maintenance_date']).dt.date == current_event['Date of Issue'].date())\n                ]\n                \n                if len(matching_entries) > 0:\n                    rul_data.append({\n                        'index': matching_entries.index[0],\n                        'rul_days': rul_days\n                    })\n        \n        # Add RUL column to feature dataset\n        feature_dataset['rul_days'] = np.nan\n        for rul_entry in rul_data:\n            feature_dataset.loc[rul_entry['index'], 'rul_days'] = rul_entry['rul_days']\n        \n        # Remove rows without RUL data\n        feature_dataset = feature_dataset.dropna(subset=['rul_days'])\n        print(f\"‚úÖ Created RUL column: {len(feature_dataset)} examples with RUL data\")\n    \nexcept FileNotFoundError:\n    print(\"‚ö†Ô∏è Interpretable features not found. Creating from raw data...\")\n    \n    # Load raw data and create features quickly\n    maintenance_df = pd.read_csv('../data/dpf_maintenance_records.csv')\n    sensor_df = pd.read_csv('../data/dpf_vehicle_stats.csv')\n    \n    # Convert datetime columns and handle timezones\n    maintenance_df['Date of Issue'] = pd.to_datetime(maintenance_df['Date of Issue'])\n    sensor_df['time'] = pd.to_datetime(sensor_df['time']).dt.tz_localize(None)\n    \n    # Create simple RUL dataset for demonstration\n    print(\"üîß Creating simplified feature dataset...\")\n    \n    feature_data = []\n    \n    # Create RUL labels (time between maintenance events)\n    for vehicle_num in maintenance_df['Vehicle_Number'].unique():\n        vehicle_maintenance = maintenance_df[\n            maintenance_df['Vehicle_Number'] == vehicle_num\n        ].sort_values('Date of Issue')\n        \n        # For each maintenance event except the last, calculate RUL\n        for i in range(len(vehicle_maintenance) - 1):\n            current_event = vehicle_maintenance.iloc[i]\n            next_event = vehicle_maintenance.iloc[i + 1]\n            \n            rul_days = (next_event['Date of Issue'] - current_event['Date of Issue']).days\n            \n            # Get sensor data for 30 days before current maintenance\n            vin = current_event['VIN Number']\n            start_date = current_event['Date of Issue'] - timedelta(days=30)\n            end_date = current_event['Date of Issue']\n            \n            vehicle_sensors = sensor_df[\n                (sensor_df['vin'] == vin) &\n                (sensor_df['time'] >= start_date) &\n                (sensor_df['time'] < end_date)\n            ]\n            \n            if len(vehicle_sensors) >= 5:  # Need minimum data\n                # Calculate simple interpretable features\n                features = {\n                    'vehicle_number': vehicle_num,\n                    'vin': vin,\n                    'rul_days': rul_days,\n                    'maintenance_type': current_event['lines_jobDescriptions'],\n                    'data_points': len(vehicle_sensors)\n                }\n                \n                # Add simple sensor statistics\n                for sensor in ['engineLoadPercent', 'engineRpm', 'ecuSpeedMph', 'defLevelMilliPercent']:\n                    if sensor in vehicle_sensors.columns:\n                        values = vehicle_sensors[sensor].dropna()\n                        if len(values) > 0:\n                            features[f'{sensor}_mean'] = values.mean()\n                            features[f'{sensor}_std'] = values.std()\n                            features[f'{sensor}_trend'] = np.polyfit(range(len(values)), values, 1)[0] if len(values) > 1 else 0\n                \n                feature_data.append(features)\n    \n    feature_dataset = pd.DataFrame(feature_data)\n    print(f\"‚úÖ Created simplified feature dataset: {len(feature_dataset)} examples\")\n\nif len(feature_dataset) == 0:\n    print(\"‚ùå No feature data available. Please run the feature engineering notebook first.\")\n    raise ValueError(\"No feature data available\")\n\n# Display dataset overview\nprint(f\"\\nüìä Dataset Overview:\")\nprint(f\"   Examples: {len(feature_dataset)}\")\n\n# Check for RUL column\nif 'rul_days' not in feature_dataset.columns:\n    print(\"‚ùå No RUL column found in dataset\")\n    raise ValueError(\"RUL column missing\")\n\nfeature_cols_count = len([col for col in feature_dataset.columns if col not in ['vehicle_number', 'vin', 'rul_days', 'maintenance_type', 'data_points', 'maintenance_date', 'window_days']])\nprint(f\"   Features: {feature_cols_count}\")\nprint(f\"   Target: RUL (days until next maintenance)\")\n\n# Show RUL distribution\nprint(f\"\\nüéØ RUL Distribution:\")\nprint(f\"   Mean: {feature_dataset['rul_days'].mean():.1f} days\")\nprint(f\"   Median: {feature_dataset['rul_days'].median():.1f} days\")\nprint(f\"   Range: {feature_dataset['rul_days'].min()}-{feature_dataset['rul_days'].max()} days\")\n\n# Filter out extreme outliers for modeling (>365 days likely data quality issues)\nfeature_dataset = feature_dataset[feature_dataset['rul_days'] <= 365]\nprint(f\"   After filtering outliers: {len(feature_dataset)} examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 2: Model Selection for Explainability\n",
    "\n",
    "We'll compare different model types based on their explainability and performance:\n",
    "\n",
    "### 1. Linear Regression (Most Explainable)\n",
    "**Pros**: Crystal clear coefficients, easy to validate\n",
    "**Cons**: Assumes linear relationships\n",
    "**Best for**: When you need maximum transparency\n",
    "\n",
    "### 2. Decision Tree (Rule-Based Explainability)\n",
    "**Pros**: Clear if-then rules, handles non-linear patterns\n",
    "**Cons**: Can overfit, unstable\n",
    "**Best for**: When you want rule-based explanations\n",
    "\n",
    "### 3. Random Forest (Balanced Approach)\n",
    "**Pros**: Good performance, feature importance, stable\n",
    "**Cons**: Less transparent than single tree\n",
    "**Best for**: When you need good performance with some explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rul_days'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'rul_days'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, feature_cols\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m X, y, feature_cols = \u001b[43mprepare_modeling_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå Could not prepare modeling data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mprepare_modeling_data\u001b[39m\u001b[34m(feature_dataset)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Prepare feature matrix\u001b[39;00m\n\u001b[32m     15\u001b[39m X = feature_dataset[feature_cols].fillna(\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# Fill missing with 0 (neutral)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m y = \u001b[43mfeature_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrul_days\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Remove infinite values\u001b[39;00m\n\u001b[32m     19\u001b[39m X = X.replace([np.inf, -np.inf], \u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workdir/planet/pdmv2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'rul_days'"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_modeling_data(feature_dataset):\n",
    "    \"\"\"\n",
    "    Prepare data for explainable modeling.\n",
    "    \"\"\"\n",
    "    # Identify feature columns (exclude metadata)\n",
    "    exclude_cols = ['vehicle_number', 'vin', 'rul_days', 'maintenance_type', 'data_points', 'maintenance_date', 'window_days']\n",
    "    feature_cols = [col for col in feature_dataset.columns if col not in exclude_cols]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"‚ùå No feature columns found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X = feature_dataset[feature_cols].fillna(0)  # Fill missing with 0 (neutral)\n",
    "    y = feature_dataset['rul_days']\n",
    "    \n",
    "    # Remove infinite values\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    print(f\"üìä Prepared modeling data:\")\n",
    "    print(f\"   Samples: {len(X)}\")\n",
    "    print(f\"   Features: {len(feature_cols)}\")\n",
    "    print(f\"   Target range: {y.min():.0f} - {y.max():.0f} days\")\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare the data\n",
    "X, y, feature_cols = prepare_modeling_data(feature_dataset)\n",
    "\n",
    "if X is None:\n",
    "    print(\"‚ùå Could not prepare modeling data\")\n",
    "else:\n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Data Split:\")\n",
    "    print(f\"   Training: {len(X_train)} samples\")\n",
    "    print(f\"   Testing: {len(X_test)} samples\")\n",
    "    \n",
    "    # Show feature preview\n",
    "    print(f\"\\nüîß Feature Preview (first 10):\")\n",
    "    for i, feature in enumerate(feature_cols[:10]):\n",
    "        print(f\"   {i+1}. {feature}\")\n",
    "    \n",
    "    if len(feature_cols) > 10:\n",
    "        print(f\"   ... and {len(feature_cols) - 10} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m linear_model, scaler, feature_importance\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Build linear model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     linear_model, linear_scaler, linear_importance = build_linear_rul_model(\n\u001b[32m     52\u001b[39m         X_train, X_test, y_train, y_test, feature_cols\n\u001b[32m     53\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 1: Linear Regression (Maximum Explainability)\n",
    "def build_linear_rul_model(X_train, X_test, y_train, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Build and evaluate a linear regression model for RUL prediction.\n",
    "    This model provides maximum explainability through coefficients.\n",
    "    \"\"\"\n",
    "    print(\"üìà Building Linear Regression Model (Maximum Explainability)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Scale features for interpretability\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train linear model\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = linear_model.predict(X_train_scaled)\n",
    "    y_pred_test = linear_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"üìä Model Performance:\")\n",
    "    print(f\"   Training MAE: {train_mae:.1f} days\")\n",
    "    print(f\"   Testing MAE: {test_mae:.1f} days\")\n",
    "    print(f\"   Training R¬≤: {train_r2:.3f}\")\n",
    "    print(f\"   Testing R¬≤: {test_r2:.3f}\")\n",
    "    \n",
    "    # Feature importance (coefficients)\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'coefficient': linear_model.coef_,\n",
    "        'abs_coefficient': np.abs(linear_model.coef_)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 10 Most Important Features (Linear Coefficients):\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        direction = \"‚ÜóÔ∏è Increases\" if row['coefficient'] > 0 else \"‚ÜòÔ∏è Decreases\"\n",
    "        print(f\"   {row['feature']}: {direction} RUL by {abs(row['coefficient']):.2f} days per unit\")\n",
    "    \n",
    "    return linear_model, scaler, feature_importance\n",
    "\n",
    "# Build linear model\n",
    "if X is not None:\n",
    "    linear_model, linear_scaler, linear_importance = build_linear_rul_model(\n",
    "        X_train, X_test, y_train, y_test, feature_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tree_model, feature_importance\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Build decision tree model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     55\u001b[39m     tree_model, tree_importance = build_decision_tree_rul_model(\n\u001b[32m     56\u001b[39m         X_train, X_test, y_train, y_test, feature_cols\n\u001b[32m     57\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 2: Decision Tree (Rule-Based Explainability)\n",
    "def build_decision_tree_rul_model(X_train, X_test, y_train, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Build and evaluate a decision tree model for RUL prediction.\n",
    "    This model provides rule-based explainability.\n",
    "    \"\"\"\n",
    "    print(\"\\nüå≥ Building Decision Tree Model (Rule-Based Explainability)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train decision tree (limit depth for interpretability)\n",
    "    tree_model = DecisionTreeRegressor(\n",
    "        max_depth=5,  # Limit depth for explainability\n",
    "        min_samples_split=10,  # Prevent overfitting\n",
    "        min_samples_leaf=5,    # Ensure meaningful leaves\n",
    "        random_state=42\n",
    "    )\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = tree_model.predict(X_train)\n",
    "    y_pred_test = tree_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"üìä Model Performance:\")\n",
    "    print(f\"   Training MAE: {train_mae:.1f} days\")\n",
    "    print(f\"   Testing MAE: {test_mae:.1f} days\")\n",
    "    print(f\"   Training R¬≤: {train_r2:.3f}\")\n",
    "    print(f\"   Testing R¬≤: {test_r2:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': tree_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 10 Most Important Features (Decision Tree):\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.3f} importance\")\n",
    "    \n",
    "    # Extract simple rules\n",
    "    print(f\"\\nüìã Sample Decision Rules (Simplified):\")\n",
    "    print(f\"   Tree depth: {tree_model.get_depth()}\")\n",
    "    print(f\"   Number of leaves: {tree_model.get_n_leaves()}\")\n",
    "    print(f\"   Most important feature: {feature_importance.iloc[0]['feature']}\")\n",
    "    \n",
    "    return tree_model, feature_importance\n",
    "\n",
    "# Build decision tree model\n",
    "if X is not None:\n",
    "    tree_model, tree_importance = build_decision_tree_rul_model(\n",
    "        X_train, X_test, y_train, y_test, feature_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rf_model, feature_importance\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Build random forest model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     50\u001b[39m     rf_model, rf_importance = build_random_forest_rul_model(\n\u001b[32m     51\u001b[39m         X_train, X_test, y_train, y_test, feature_cols\n\u001b[32m     52\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Model 3: Random Forest (Balanced Performance/Explainability)\n",
    "def build_random_forest_rul_model(X_train, X_test, y_train, y_test, feature_cols):\n",
    "    \"\"\"\n",
    "    Build and evaluate a random forest model for RUL prediction.\n",
    "    This model balances performance and explainability.\n",
    "    \"\"\"\n",
    "    print(\"\\nüå≤ Building Random Forest Model (Balanced Approach)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Train random forest\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,      # Enough trees for stability\n",
    "        max_depth=6,           # Limit depth for interpretability\n",
    "        min_samples_split=10,  # Prevent overfitting\n",
    "        min_samples_leaf=5,    # Ensure meaningful leaves\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"üìä Model Performance:\")\n",
    "    print(f\"   Training MAE: {train_mae:.1f} days\")\n",
    "    print(f\"   Testing MAE: {test_mae:.1f} days\")\n",
    "    print(f\"   Training R¬≤: {train_r2:.3f}\")\n",
    "    print(f\"   Testing R¬≤: {test_r2:.3f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nüîç Top 10 Most Important Features (Random Forest):\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']}: {row['importance']:.3f} importance\")\n",
    "    \n",
    "    return rf_model, feature_importance\n",
    "\n",
    "# Build random forest model\n",
    "if X is not None:\n",
    "    rf_model, rf_importance = build_random_forest_rul_model(\n",
    "        X_train, X_test, y_train, y_test, feature_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Model Comparison and Selection\n",
    "\n",
    "Let's compare our three models on multiple criteria that matter for fleet management deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "def compare_models():\n",
    "    \"\"\"\n",
    "    Compare all three models on performance and explainability criteria.\n",
    "    \"\"\"\n",
    "    print(\"üèÜ MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if X is None:\n",
    "        print(\"‚ùå No models to compare\")\n",
    "        return\n",
    "    \n",
    "    # Calculate performance metrics for all models\n",
    "    models_performance = []\n",
    "    \n",
    "    # Linear Regression\n",
    "    if 'linear_model' in globals():\n",
    "        X_test_scaled = linear_scaler.transform(X_test)\n",
    "        linear_pred = linear_model.predict(X_test_scaled)\n",
    "        linear_mae = mean_absolute_error(y_test, linear_pred)\n",
    "        linear_r2 = r2_score(y_test, linear_pred)\n",
    "        \n",
    "        models_performance.append({\n",
    "            'Model': 'Linear Regression',\n",
    "            'MAE (days)': linear_mae,\n",
    "            'R¬≤ Score': linear_r2,\n",
    "            'Explainability': '‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "            'Complexity': '‚≠ê',\n",
    "            'Best For': 'Maximum transparency'\n",
    "        })\n",
    "    \n",
    "    # Decision Tree\n",
    "    if 'tree_model' in globals():\n",
    "        tree_pred = tree_model.predict(X_test)\n",
    "        tree_mae = mean_absolute_error(y_test, tree_pred)\n",
    "        tree_r2 = r2_score(y_test, tree_pred)\n",
    "        \n",
    "        models_performance.append({\n",
    "            'Model': 'Decision Tree',\n",
    "            'MAE (days)': tree_mae,\n",
    "            'R¬≤ Score': tree_r2,\n",
    "            'Explainability': '‚≠ê‚≠ê‚≠ê‚≠ê',\n",
    "            'Complexity': '‚≠ê‚≠ê',\n",
    "            'Best For': 'Rule-based decisions'\n",
    "        })\n",
    "    \n",
    "    # Random Forest\n",
    "    if 'rf_model' in globals():\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "        rf_r2 = r2_score(y_test, rf_pred)\n",
    "        \n",
    "        models_performance.append({\n",
    "            'Model': 'Random Forest',\n",
    "            'MAE (days)': rf_mae,\n",
    "            'R¬≤ Score': rf_r2,\n",
    "            'Explainability': '‚≠ê‚≠ê‚≠ê',\n",
    "            'Complexity': '‚≠ê‚≠ê‚≠ê',\n",
    "            'Best For': 'Performance + some explainability'\n",
    "        })\n",
    "    \n",
    "    # Display comparison\n",
    "    if models_performance:\n",
    "        comparison_df = pd.DataFrame(models_performance)\n",
    "        print(comparison_df.to_string(index=False, float_format='%.2f'))\n",
    "        \n",
    "        # Recommendations\n",
    "        print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
    "        \n",
    "        best_performance = comparison_df.loc[comparison_df['MAE (days)'].idxmin()]\n",
    "        print(f\"   üèÜ Best Performance: {best_performance['Model']} (MAE: {best_performance['MAE (days)']:.1f} days)\")\n",
    "        \n",
    "        print(f\"   üîç Most Explainable: Linear Regression (crystal clear coefficients)\")\n",
    "        print(f\"   ‚öñÔ∏è Best Balance: Random Forest (good performance + feature importance)\")\n",
    "        \n",
    "        print(f\"\\nüí° SELECTION GUIDE:\")\n",
    "        print(f\"   ‚Ä¢ Choose Linear Regression if: Transparency is most important\")\n",
    "        print(f\"   ‚Ä¢ Choose Decision Tree if: You want simple if-then rules\")\n",
    "        print(f\"   ‚Ä¢ Choose Random Forest if: You need best predictive performance\")\n",
    "    \n",
    "    return models_performance\n",
    "\n",
    "# Compare models\n",
    "model_comparison = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Generate Explanations for Predictions\n",
    "\n",
    "The most important part of explainable models is providing clear explanations for each prediction. Let's create explanation systems for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation system for Linear Regression\n",
    "def explain_linear_prediction(model, scaler, feature_cols, feature_importance, sample_features, sample_rul=None):\n",
    "    \"\"\"\n",
    "    Generate human-readable explanations for linear regression predictions.\n",
    "    \"\"\"\n",
    "    # Scale the sample\n",
    "    sample_scaled = scaler.transform([sample_features])\n",
    "    \n",
    "    # Make prediction\n",
    "    predicted_rul = model.predict(sample_scaled)[0]\n",
    "    \n",
    "    # Calculate feature contributions\n",
    "    contributions = []\n",
    "    intercept_contribution = model.intercept_\n",
    "    \n",
    "    for i, (feature, value, scaled_value) in enumerate(zip(feature_cols, sample_features, sample_scaled[0])):\n",
    "        coefficient = model.coef_[i]\n",
    "        contribution = coefficient * scaled_value\n",
    "        \n",
    "        if abs(contribution) > 1:  # Only show significant contributions\n",
    "            contributions.append({\n",
    "                'feature': feature,\n",
    "                'value': value,\n",
    "                'coefficient': coefficient,\n",
    "                'contribution': contribution\n",
    "            })\n",
    "    \n",
    "    # Sort by absolute contribution\n",
    "    contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)\n",
    "    \n",
    "    # Generate explanation\n",
    "    explanation = {\n",
    "        'predicted_rul': predicted_rul,\n",
    "        'actual_rul': sample_rul,\n",
    "        'baseline_rul': intercept_contribution,\n",
    "        'top_contributors': contributions[:5],\n",
    "        'explanation_text': []\n",
    "    }\n",
    "    \n",
    "    # Generate human-readable text\n",
    "    explanation['explanation_text'].append(f\"üéØ Predicted RUL: {predicted_rul:.0f} days\")\n",
    "    if sample_rul is not None:\n",
    "        error = abs(predicted_rul - sample_rul)\n",
    "        explanation['explanation_text'].append(f\"üìä Actual RUL: {sample_rul:.0f} days (Error: {error:.0f} days)\")\n",
    "    \n",
    "    explanation['explanation_text'].append(f\"\\nüìà Key Contributing Factors:\")\n",
    "    \n",
    "    for i, contrib in enumerate(contributions[:3], 1):\n",
    "        direction = \"increases\" if contrib['contribution'] > 0 else \"decreases\"\n",
    "        impact = abs(contrib['contribution'])\n",
    "        \n",
    "        explanation['explanation_text'].append(\n",
    "            f\"   {i}. {contrib['feature']} ({contrib['value']:.2f}) {direction} RUL by {impact:.1f} days\"\n",
    "        )\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Explanation system for Decision Tree\n",
    "def explain_tree_prediction(model, feature_cols, sample_features, sample_rul=None):\n",
    "    \"\"\"\n",
    "    Generate human-readable explanations for decision tree predictions.\n",
    "    \"\"\"\n",
    "    # Make prediction\n",
    "    predicted_rul = model.predict([sample_features])[0]\n",
    "    \n",
    "    # Get the leaf node and path\n",
    "    leaf_id = model.decision_path([sample_features]).indices\n",
    "    \n",
    "    # Find which features were used in the decision path\n",
    "    feature_used = []\n",
    "    threshold_used = []\n",
    "    \n",
    "    for node_id in leaf_id:\n",
    "        if model.tree_.feature[node_id] != -2:  # Not a leaf\n",
    "            feature_idx = model.tree_.feature[node_id]\n",
    "            threshold = model.tree_.threshold[node_id]\n",
    "            feature_name = feature_cols[feature_idx]\n",
    "            feature_value = sample_features[feature_idx]\n",
    "            \n",
    "            direction = \"‚â§\" if feature_value <= threshold else \">\"\n",
    "            feature_used.append(f\"{feature_name} {direction} {threshold:.2f}\")\n",
    "    \n",
    "    explanation = {\n",
    "        'predicted_rul': predicted_rul,\n",
    "        'actual_rul': sample_rul,\n",
    "        'decision_path': feature_used,\n",
    "        'explanation_text': []\n",
    "    }\n",
    "    \n",
    "    # Generate human-readable text\n",
    "    explanation['explanation_text'].append(f\"üéØ Predicted RUL: {predicted_rul:.0f} days\")\n",
    "    if sample_rul is not None:\n",
    "        error = abs(predicted_rul - sample_rul)\n",
    "        explanation['explanation_text'].append(f\"üìä Actual RUL: {sample_rul:.0f} days (Error: {error:.0f} days)\")\n",
    "    \n",
    "    explanation['explanation_text'].append(f\"\\nüå≥ Decision Path:\")\n",
    "    for i, condition in enumerate(feature_used[:5], 1):\n",
    "        explanation['explanation_text'].append(f\"   {i}. {condition}\")\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "# Demonstrate explanations with a sample\n",
    "def demonstrate_explanations():\n",
    "    \"\"\"\n",
    "    Demonstrate explanation systems with sample predictions.\n",
    "    \"\"\"\n",
    "    if X is None or len(X_test) == 0:\n",
    "        print(\"‚ùå No test data available for explanation demonstration\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç EXPLANATION DEMONSTRATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get a sample from test set\n",
    "    sample_idx = 0\n",
    "    sample_features = X_test.iloc[sample_idx].values\n",
    "    sample_rul = y_test.iloc[sample_idx]\n",
    "    \n",
    "    print(f\"Sample Vehicle Analysis:\")\n",
    "    print(f\"Vehicle ID: {feature_dataset.iloc[X_test.index[sample_idx]].get('vehicle_number', 'Unknown')}\")\n",
    "    \n",
    "    # Linear Regression Explanation\n",
    "    if 'linear_model' in globals():\n",
    "        print(f\"\\nüìà LINEAR REGRESSION EXPLANATION:\")\n",
    "        linear_explanation = explain_linear_prediction(\n",
    "            linear_model, linear_scaler, feature_cols, linear_importance, \n",
    "            sample_features, sample_rul\n",
    "        )\n",
    "        \n",
    "        for line in linear_explanation['explanation_text']:\n",
    "            print(line)\n",
    "    \n",
    "    # Decision Tree Explanation\n",
    "    if 'tree_model' in globals():\n",
    "        print(f\"\\nüå≥ DECISION TREE EXPLANATION:\")\n",
    "        tree_explanation = explain_tree_prediction(\n",
    "            tree_model, feature_cols, sample_features, sample_rul\n",
    "        )\n",
    "        \n",
    "        for line in tree_explanation['explanation_text']:\n",
    "            print(line)\n",
    "    \n",
    "    print(f\"\\nüí° How Fleet Managers Should Use These Explanations:\")\n",
    "    print(f\"   ‚Ä¢ Linear Regression: Focus on top contributing factors\")\n",
    "    print(f\"   ‚Ä¢ Decision Tree: Follow the if-then rule logic\")\n",
    "    print(f\"   ‚Ä¢ Both models: Look for actionable maintenance triggers\")\n",
    "\n",
    "# Demonstrate explanations\n",
    "demonstrate_explanations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Model Validation for Fleet Management\n",
    "\n",
    "Beyond statistical metrics, we need to validate that our models make operational sense for fleet managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fleet management validation\n",
    "def validate_for_fleet_management():\n",
    "    \"\"\"\n",
    "    Validate models from a fleet management perspective.\n",
    "    \"\"\"\n",
    "    if X is None:\n",
    "        print(\"‚ùå No models to validate\")\n",
    "        return\n",
    "    \n",
    "    print(\"üöõ FLEET MANAGEMENT VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test different scenarios\n",
    "    validation_scenarios = [\n",
    "        {\n",
    "            'name': 'Early Warning Capability',\n",
    "            'description': 'Can the model predict issues 30+ days in advance?',\n",
    "            'test': lambda pred, actual: (pred <= 30 and actual <= 45) or (pred > 30 and actual > 30)\n",
    "        },\n",
    "        {\n",
    "            'name': 'Emergency Alert Accuracy',\n",
    "            'description': 'Does the model correctly identify urgent cases (<15 days)?',\n",
    "            'test': lambda pred, actual: (pred <= 15 and actual <= 20) or (pred > 15 and actual > 10)\n",
    "        },\n",
    "        {\n",
    "            'name': 'False Alarm Rate',\n",
    "            'description': 'How often does the model predict urgent when not needed?',\n",
    "            'test': lambda pred, actual: not (pred <= 30 and actual > 60)\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test each model\n",
    "    model_results = {}\n",
    "    \n",
    "    if 'linear_model' in globals():\n",
    "        X_test_scaled = linear_scaler.transform(X_test)\n",
    "        linear_predictions = linear_model.predict(X_test_scaled)\n",
    "        model_results['Linear Regression'] = linear_predictions\n",
    "    \n",
    "    if 'tree_model' in globals():\n",
    "        tree_predictions = tree_model.predict(X_test)\n",
    "        model_results['Decision Tree'] = tree_predictions\n",
    "    \n",
    "    if 'rf_model' in globals():\n",
    "        rf_predictions = rf_model.predict(X_test)\n",
    "        model_results['Random Forest'] = rf_predictions\n",
    "    \n",
    "    # Evaluate scenarios for each model\n",
    "    print(f\"üìã Scenario-Based Validation Results:\")\n",
    "    print()\n",
    "    \n",
    "    for scenario in validation_scenarios:\n",
    "        print(f\"üéØ {scenario['name']}:\")\n",
    "        print(f\"   {scenario['description']}\")\n",
    "        \n",
    "        for model_name, predictions in model_results.items():\n",
    "            # Calculate success rate for this scenario\n",
    "            successes = 0\n",
    "            total = len(predictions)\n",
    "            \n",
    "            for pred, actual in zip(predictions, y_test):\n",
    "                if scenario['test'](pred, actual):\n",
    "                    successes += 1\n",
    "            \n",
    "            success_rate = (successes / total) * 100\n",
    "            print(f\"   {model_name}: {success_rate:.1f}% success rate\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Operational recommendations\n",
    "    print(f\"üéØ OPERATIONAL RECOMMENDATIONS:\")\n",
    "    \n",
    "    # Find best model for early warning\n",
    "    best_early_warning = None\n",
    "    best_early_warning_rate = 0\n",
    "    \n",
    "    for model_name, predictions in model_results.items():\n",
    "        early_warning_successes = sum(1 for pred, actual in zip(predictions, y_test) \n",
    "                                    if (pred <= 30 and actual <= 45) or (pred > 30 and actual > 30))\n",
    "        rate = early_warning_successes / len(predictions)\n",
    "        \n",
    "        if rate > best_early_warning_rate:\n",
    "            best_early_warning_rate = rate\n",
    "            best_early_warning = model_name\n",
    "    \n",
    "    if best_early_warning:\n",
    "        print(f\"   üèÜ Best for Early Warning: {best_early_warning} ({best_early_warning_rate:.1%} success)\")\n",
    "    \n",
    "    print(f\"   üìÖ Recommended Check Frequency: Weekly for vehicles with <60 days predicted RUL\")\n",
    "    print(f\"   üö® Urgent Alert Threshold: <30 days predicted RUL\")\n",
    "    print(f\"   ‚ö†Ô∏è Caution Alert Threshold: 30-60 days predicted RUL\")\n",
    "    print(f\"   ‚úÖ Normal Monitoring: >60 days predicted RUL\")\n",
    "\n",
    "# Run fleet management validation\n",
    "validate_for_fleet_management()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 6: Deployment Strategy for Explainable Models\n",
    "\n",
    "Now let's create a practical deployment strategy that fleet managers can implement."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create deployment package\ndef create_deployment_strategy():\n    \"\"\"\n    Create a practical deployment strategy for fleet managers.\n    \"\"\"\n    print(\"üöÄ EXPLAINABLE MODEL DEPLOYMENT STRATEGY\")\n    print(\"=\"*60)\n    \n    print(f\"üìã PHASE 1: PILOT DEPLOYMENT (Weeks 1-4)\")\n    print(f\"   üéØ Objective: Validate model performance with small fleet subset\")\n    print(f\"   üìä Scope: 5-10 highest-risk vehicles\")\n    print(f\"   üìà Success Criteria:\")\n    print(f\"      ‚Ä¢ Predictions within ¬±15 days of actual maintenance\")\n    print(f\"      ‚Ä¢ Fleet managers understand all explanations\")\n    print(f\"      ‚Ä¢ At least 2 successful proactive maintenance actions\")\n    print(f\"   üîß Actions:\")\n    print(f\"      ‚Ä¢ Daily model runs and prediction reviews\")\n    print(f\"      ‚Ä¢ Weekly explanation validation meetings\")\n    print(f\"      ‚Ä¢ Track prediction accuracy vs actual maintenance\")\n    \n    print(f\"\\nüìã PHASE 2: SCALED DEPLOYMENT (Weeks 5-12)\")\n    print(f\"   üéØ Objective: Expand to full fleet with automated alerts\")\n    print(f\"   üìä Scope: All vehicles with sufficient data coverage\")\n    print(f\"   üìà Success Criteria:\")\n    print(f\"      ‚Ä¢ 20% reduction in emergency DPF repairs\")\n    print(f\"      ‚Ä¢ 15% improvement in maintenance planning efficiency\")\n    print(f\"      ‚Ä¢ Fleet manager confidence score >8/10\")\n    print(f\"   üîß Actions:\")\n    print(f\"      ‚Ä¢ Automated daily reports with explanations\")\n    print(f\"      ‚Ä¢ Integration with existing maintenance systems\")\n    print(f\"      ‚Ä¢ Staff training on model interpretation\")\n    \n    print(f\"\\nüìã PHASE 3: OPTIMIZATION (Weeks 13+)\")\n    print(f\"   üéØ Objective: Continuous improvement and model refinement\")\n    print(f\"   üìä Scope: Full fleet + model enhancements\")\n    print(f\"   üîß Actions:\")\n    print(f\"      ‚Ä¢ Monthly model retraining with new data\")\n    print(f\"      ‚Ä¢ Feature threshold adjustments based on experience\")\n    print(f\"      ‚Ä¢ Expansion to other maintenance categories\")\n    \n    print(f\"\\nüõ†Ô∏è TECHNICAL IMPLEMENTATION:\")\n    \n    # Model selection recommendation\n    if 'model_comparison' in globals() and model_comparison:\n        best_model = min(model_comparison, key=lambda x: x['MAE (days)'])\n        print(f\"   üèÜ Recommended Model: {best_model['Model']}\")\n        print(f\"      Accuracy: ¬±{best_model['MAE (days)']:.1f} days\")\n        print(f\"      Explainability: {best_model['Explainability']}\")\n        print(f\"      Best For: {best_model['Best For']}\")\n    else:\n        print(f\"   üèÜ Recommended Model: Linear Regression (Most Explainable)\")\n        print(f\"      ‚Ä¢ Crystal clear coefficient interpretations\")\n        print(f\"      ‚Ä¢ Easy to validate predictions manually\")\n        print(f\"      ‚Ä¢ Maximum transparency for fleet managers\")\n    \n    print(f\"\\n   üìä Alert System Configuration:\")\n    print(f\"      üö® URGENT (Immediate Action): RUL ‚â§ 15 days\")\n    print(f\"         ‚Üí Schedule maintenance within 1 week\")\n    print(f\"         ‚Üí Daily monitoring of key sensors\")\n    print(f\"      ‚ö†Ô∏è WARNING (Plan Soon): RUL 16-30 days\")\n    print(f\"         ‚Üí Schedule maintenance within 2-3 weeks\")\n    print(f\"         ‚Üí Weekly sensor review\")\n    print(f\"      ‚ö° CAUTION (Monitor): RUL 31-60 days\")\n    print(f\"         ‚Üí Plan maintenance within 1-2 months\")\n    print(f\"         ‚Üí Bi-weekly trend analysis\")\n    print(f\"      ‚úÖ NORMAL (Routine): RUL > 60 days\")\n    print(f\"         ‚Üí Standard maintenance schedule\")\n    print(f\"         ‚Üí Monthly monitoring\")\n    \n    print(f\"\\nüéì TRAINING REQUIREMENTS:\")\n    print(f\"   üë• Fleet Managers (4 hours):\")\n    print(f\"      ‚Ä¢ Understanding model predictions and explanations\")\n    print(f\"      ‚Ä¢ Interpreting feature importance and trends\")\n    print(f\"      ‚Ä¢ Making data-driven maintenance decisions\")\n    print(f\"   üîß Maintenance Staff (2 hours):\")\n    print(f\"      ‚Ä¢ Reading alert reports and explanations\")\n    print(f\"      ‚Ä¢ Validating predictions against actual findings\")\n    print(f\"      ‚Ä¢ Providing feedback for model improvement\")\n    \n    print(f\"\\nüìà SUCCESS METRICS:\")\n    print(f\"   üìä Operational Metrics:\")\n    print(f\"      ‚Ä¢ Prediction accuracy (target: ¬±10 days)\")\n    print(f\"      ‚Ä¢ Emergency repair reduction (target: 25%)\")\n    print(f\"      ‚Ä¢ Maintenance cost savings (target: 15%)\")\n    print(f\"      ‚Ä¢ Vehicle downtime reduction (target: 20%)\")\n    print(f\"   üë• User Adoption Metrics:\")\n    print(f\"      ‚Ä¢ Fleet manager explanation comprehension (target: >90%)\")\n    print(f\"      ‚Ä¢ Daily system usage rate (target: >80%)\")\n    print(f\"      ‚Ä¢ User confidence in predictions (target: >8/10)\")\n\n# Create deployment strategy\ncreate_deployment_strategy()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Explainable Models for Production\n",
    "\n",
    "Let's save our best performing explainable model for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for production\n",
    "def save_production_models():\n",
    "    \"\"\"\n",
    "    Save the best explainable models for production deployment.\n",
    "    \"\"\"\n",
    "    print(\"üíæ SAVING EXPLAINABLE MODELS FOR PRODUCTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    import joblib\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create model metadata\n",
    "    model_metadata = {\n",
    "        'created_date': datetime.now().isoformat(),\n",
    "        'model_purpose': 'DPF RUL Prediction',\n",
    "        'target_variable': 'Days until next DPF maintenance',\n",
    "        'feature_count': len(feature_cols) if feature_cols else 0,\n",
    "        'training_samples': len(X_train) if X is not None else 0,\n",
    "        'features': feature_cols if feature_cols else [],\n",
    "        'model_versions': []\n",
    "    }\n",
    "    \n",
    "    saved_models = []\n",
    "    \n",
    "    # Save Linear Regression (Most Explainable)\n",
    "    if 'linear_model' in globals():\n",
    "        try:\n",
    "            joblib.dump(linear_model, '../models/linear_rul_model.pkl')\n",
    "            joblib.dump(linear_scaler, '../models/linear_rul_scaler.pkl')\n",
    "            linear_importance.to_csv('../models/linear_feature_importance.csv', index=False)\n",
    "            \n",
    "            # Test performance\n",
    "            X_test_scaled = linear_scaler.transform(X_test)\n",
    "            linear_pred = linear_model.predict(X_test_scaled)\n",
    "            linear_mae = mean_absolute_error(y_test, linear_pred)\n",
    "            \n",
    "            model_metadata['model_versions'].append({\n",
    "                'name': 'Linear Regression',\n",
    "                'file': 'linear_rul_model.pkl',\n",
    "                'scaler': 'linear_rul_scaler.pkl',\n",
    "                'feature_importance': 'linear_feature_importance.csv',\n",
    "                'mae_days': linear_mae,\n",
    "                'explainability_level': 'Maximum',\n",
    "                'explanation_type': 'Coefficient-based'\n",
    "            })\n",
    "            \n",
    "            saved_models.append('Linear Regression')\n",
    "            print(f\"‚úÖ Saved Linear Regression model (MAE: {linear_mae:.1f} days)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save Linear Regression: {e}\")\n",
    "    \n",
    "    # Save Decision Tree (Rule-Based)\n",
    "    if 'tree_model' in globals():\n",
    "        try:\n",
    "            joblib.dump(tree_model, '../models/tree_rul_model.pkl')\n",
    "            tree_importance.to_csv('../models/tree_feature_importance.csv', index=False)\n",
    "            \n",
    "            # Test performance\n",
    "            tree_pred = tree_model.predict(X_test)\n",
    "            tree_mae = mean_absolute_error(y_test, tree_pred)\n",
    "            \n",
    "            model_metadata['model_versions'].append({\n",
    "                'name': 'Decision Tree',\n",
    "                'file': 'tree_rul_model.pkl',\n",
    "                'scaler': None,\n",
    "                'feature_importance': 'tree_feature_importance.csv',\n",
    "                'mae_days': tree_mae,\n",
    "                'explainability_level': 'High',\n",
    "                'explanation_type': 'Rule-based'\n",
    "            })\n",
    "            \n",
    "            saved_models.append('Decision Tree')\n",
    "            print(f\"‚úÖ Saved Decision Tree model (MAE: {tree_mae:.1f} days)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save Decision Tree: {e}\")\n",
    "    \n",
    "    # Save Random Forest (Balanced)\n",
    "    if 'rf_model' in globals():\n",
    "        try:\n",
    "            joblib.dump(rf_model, '../models/rf_rul_model.pkl')\n",
    "            rf_importance.to_csv('../models/rf_feature_importance.csv', index=False)\n",
    "            \n",
    "            # Test performance\n",
    "            rf_pred = rf_model.predict(X_test)\n",
    "            rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "            \n",
    "            model_metadata['model_versions'].append({\n",
    "                'name': 'Random Forest',\n",
    "                'file': 'rf_rul_model.pkl',\n",
    "                'scaler': None,\n",
    "                'feature_importance': 'rf_feature_importance.csv',\n",
    "                'mae_days': rf_mae,\n",
    "                'explainability_level': 'Medium',\n",
    "                'explanation_type': 'Feature importance'\n",
    "            })\n",
    "            \n",
    "            saved_models.append('Random Forest')\n",
    "            print(f\"‚úÖ Saved Random Forest model (MAE: {rf_mae:.1f} days)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save Random Forest: {e}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    try:\n",
    "        with open('../models/model_metadata.json', 'w') as f:\n",
    "            json.dump(model_metadata, f, indent=2)\n",
    "        print(f\"‚úÖ Saved model metadata\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save metadata: {e}\")\n",
    "    \n",
    "    # Create usage example\n",
    "    usage_example = f'''\n",
    "# PRODUCTION USAGE EXAMPLE\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the best explainable model\n",
    "model = joblib.load('models/linear_rul_model.pkl')  # Most explainable\n",
    "scaler = joblib.load('models/linear_rul_scaler.pkl')\n",
    "feature_importance = pd.read_csv('models/linear_feature_importance.csv')\n",
    "\n",
    "# Prepare new vehicle data (same features as training)\n",
    "new_vehicle_features = [...]  # Your feature values here\n",
    "\n",
    "# Make prediction\n",
    "scaled_features = scaler.transform([new_vehicle_features])\n",
    "predicted_rul = model.predict(scaled_features)[0]\n",
    "\n",
    "# Generate explanation\n",
    "contributions = model.coef_ * scaled_features[0]\n",
    "top_contributors = feature_importance.head(5)\n",
    "\n",
    "print(f\"Predicted RUL: {{predicted_rul:.0f}} days\")\n",
    "print(f\"Top contributing factors:\")\n",
    "for _, row in top_contributors.iterrows():\n",
    "    direction = \"increases\" if row['coefficient'] > 0 else \"decreases\"\n",
    "    print(f\"  - {{row['feature']}}: {{direction}} RUL\")\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        with open('../models/usage_example.py', 'w') as f:\n",
    "            f.write(usage_example)\n",
    "        print(f\"‚úÖ Created usage example\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create usage example: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ PRODUCTION READY!\")\n",
    "    print(f\"   üìÅ Models saved in: ../models/\")\n",
    "    print(f\"   ü§ñ Models available: {', '.join(saved_models)}\")\n",
    "    print(f\"   üìä Metadata file: model_metadata.json\")\n",
    "    print(f\"   üí° Usage example: usage_example.py\")\n",
    "    \n",
    "    return model_metadata\n",
    "\n",
    "# Create models directory and save\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "if X is not None:\n",
    "    production_metadata = save_production_models()\n",
    "else:\n",
    "    print(\"‚ùå No models available to save\")\n",
    "\n",
    "print(f\"\\nüìö TUTORIAL COMPLETE!\")\n",
    "print(f\"üéØ You've learned how to:\")\n",
    "print(f\"   ‚Ä¢ Build explainable RUL prediction models\")\n",
    "print(f\"   ‚Ä¢ Generate human-readable explanations for predictions\")\n",
    "print(f\"   ‚Ä¢ Validate models for fleet management use\")\n",
    "print(f\"   ‚Ä¢ Create deployment strategies for production\")\n",
    "print(f\"   ‚Ä¢ Save models for operational use\")\n",
    "print(f\"\\nüöÄ Ready for deployment in your fleet management system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}